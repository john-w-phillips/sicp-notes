* Chapter 1
** 1.1
*** 1.1.5 The Substitution Model for Procedure Application
    To evaluate a combination whose operator names a compound
    procedure, the interpreter follows much the same process as for
    combinations whose operators name primitive procedures, which we
    described in section 1.1.3. That is, the interpreter evaluates the
    elements of the combination and applies the procedure (which is
    the value of the operator of the combination) to the arguments
    (which are the values of the operands of the combination). 

    We can assume that the mechanism for applying primitive procedures
    to arguments is built into the interpreter. For compound
    procedures, the application process is as follows:

    - To apply a compound procedure to arguments, evaluate the body of
      the procedure with each formal parameter replaced by the
      corresponding argument.

    To illustrate the process, let's evaluate the combination

    #+BEGIN_EXAMPLE
    (f 5)
    #+END_EXAMPLE

    where f is the procedure defined in section 1.1.4. We begin by
    retrieving the body of f:

    #+BEGIN_EXAMPLE
    (sum-of-squares (+ a 1) (* a 2))
    #+END_EXAMPLE

    Then we replace the formal parameter a by the argument 5:

    #+BEGIN_EXAMPLE
    (sum-of-squares (+ 5 1) (* 5 2))
    #+END_EXAMPLE

    Thus the problem reduces to the evaluation of a combination with
    two operands and an operator sum-of-squares. Evaluating this
    combination involves three subproblems. We must evaluate the
    operator to get the procedure to be applied, and we must evaluate
    the operands to get the arguments. Now (+ 5 1) produces 6 and (*
    5 2) produces 10, so we must apply the sum-of-squares procedure to
    6 and 10. These values are substituted for the formal parameters x
    and y in the body of sum-of-squares, reducing the expression to

    #+BEGIN_EXAMPLE
    (+ (square 6) (square 10))
    #+END_EXAMPLE

    If we use the definition of square, this reduces to

    #+BEGIN_EXAMPLE
    (+ (* 6 6) (* 10 10))
    #+END_EXAMPLE

    which reduces by multiplication to

    #+BEGIN_EXAMPLE
    (+ 36 100)
    #+END_EXAMPLE

    and finally to

    136

    The process we have just described is called the _substitution
    model_ for procedure application. It can be taken as a model that
    deterimines the 'meaning' of procedure application, insofar as the
    procedures in this chapter are concerned. However, there are two
    points that should be stressed:

    - The purpose of the substitution is to help us think about
      procedure application, not to provide a description of how the
      interpreter really works. Typical interpreters do not evaluate
      procedure applications by manipulating the text of a procedure
      to substitute values for the formal parameters. In practice, the
      "substitution" is accomplished by using a local environment for
      the formal parameters. We will discuss this more fully in
      chapters 3 and 4 when we examine the implementation of an
      interpreter in detail.
    - Over the course of this book, we will present a sequence of
      increasingly elaborate models of how interpreters work,
      culminating with a complete implementation of an interpreter and
      compiler in chapter 5. The substitution model is only the first
      of these models -- a way to get started thinking formally about
      the evaluation process. In general when modeling phenomena in
      science and engineering, we begin with simplified, incomplete
      models. As we examine things in creater detail, these simple
      models become inadequate and must be replaced by more refined
      models. The substitution model is no exception. In particular,
      when we address in chapter 3 the use of procedures with "mutable
      data", we will see that the substitution model breaks down and
      must be replaced by a more complicated model of procedure
      application.

**** Applicative order versus normal order
     According to the description of evaluation given in section
     1.1.3, the interpreter first evaluates the operator and operands
     and then applies the resulting procedure to the resulting
     arguments. This is not the only way to perform evaluation. An
     alternative evaluation model would not evaluate the operands
     until their values were needed. Instead it would first substitute
     operand expressions for parameters until it obtained an
     expression involving only primitive operators, and would then
     perform the evaluation. If we used this method, the evaluation of

     #+BEGIN_EXAMPLE
     (f 5)
     #+END_EXAMPLE

     would then proceed according to the sequence of expansions 

     (sum-of-squares (+ 5 1) (* 5 2))
     (+  (square (+ 5 1))    (square (* 5 2)) )
     (+  (* (+ 5 1) (+ 5 1)) (* (* 5 2) (* 5 2)))
     
     followed by the reductions
     
     (+   (* 6 6)            (* 10 10))
     (+    36                100)
     136

     This gives the same answer as our previous evaluation model, but
     the process is different. In particular, the evaluations of (+
     5 1) and (* 5 2) are each performed twice here, corresponding to
     the reduction of the expression

     (* x x)

     with x replaced respectively by (+ 5 1) and (* 5 2).

     This alternative "fully expand and then reduce" evaluation method
     is known as _normal-order evaluation_, in contrast to the
     "evaluate the arguments and then apply" method that the
     interpreter actually uses, which is called _applicative-order
     evaluation_. It can be show that, for procedure applications that
     can be modeled using substitution (including all the procedures
     in the first two chapters of this book) and yield legitimate
     values, normal-order and applicative-order evaluation produce the
     same value. 

     Lisp uses applicative-order evaluation, partly because of the
     additional efficiency obtained from avoiding multiple evaluations
     of expressions such as those illustrated with (+ 5 1) and (* 5 2)
     above and, more significantly, because normal-order evaluation
     becomes much more complicated to deal with when we leave the
     realm of procedures that can be modeled by substitution. On the
     other hand, normal-order evaluation can be extremely valuable
     tool, and we will investigate some of its implications in chapter
     3 and 4.

*** 1.1.6 Conditional expressions and Predicates
    The expressive power of the class of procedures that we can define
    at this point is very limited, because we have no way to make
    tests and to perform different operations depending on the result
    of a test. For instance, we cannot define a procedure that
    computes the absolute value of a number by testing whether the
    number is positive, negative, or zero and taking different actions
    in the different cases according to the rule:

    #+BEGIN_EXAMPLE
           -
           | x if x > 0
    |x| =  | 0 if x = 0
           |-x if x < 0
           -
    #+END_EXAMPLE
    
    This construct is called a _case analysis_, and there is a special
    form in Lisp for notating such a case analysis. It is called cond
    (which stands for conditional), and it is used as follows:
    
    #+BEGIN_EXAMPLE
    (define (abs x)
      (cond  ((> x 0) x)
             ((= x 0) 0)
             ((< x 0) (- x))))
    #+END_EXAMPLE

    The general form of a conditional expression is

    (cond (<p1> <e1>)
          (<p2> <e2>)
          .
          .
          .
          (<pn> <en>))

    consisting of the symbol cond followed by parenthesized pairs of
    expressions (<p> <e>) called _clauses_. The first expression in
    each pair is a _predicate_ -- that is, an expression whose value
    is interpreted as either true or false.

    Conditional expressions are evaluated as follows. The predicate
    <p1> is evaluated first. If its value is false, then <p2> is
    evaluated. If <p2>'s value is also false, then <p3> is
    evaluated. This process continues until a predicate is found whose
    value is true, in which case the interpreter returns the value of
    the corresponding _consequent expression_ <e> of the clause as the
    value of the conditional expression. If none of the <p>'s is found
    to be true, the value of the cond is undefined.

    The word _predicate_ is used for procedures that return true or
    false, as well as for expressions that evaluate to true or
    false. The absolute-value procedure abs makes use of the primitive
    predicates >, <, and =. These take two numbers as arguments and
    test whether the first number is, respectively, greater than, less
    than, or equal to the second number, returning true or false
    accordingly.

    Another way to write the absolute-value procedure is

    #+BEGIN_EXAMPLE
    (define (abs x)
      (if (< x 0)
          (- x)
          x)
    #+END_EXAMPLE

    


    

    
     
*** 1.1.7 Example: Square roots by Newton's Method
    Procedures, as introduced above, are much like ordinary
    mathematical functions. They specify a value that is determined by
    one or more parameters. But there is an important difference
    between mathematical functions and computer procedures. Procedures
    must be effective.
    
    As a case in point, consider the problem of computing square
    roots. We can define the square-root function as

    #+BEGIN_EXAMPLE
    âˆšx = the y such that y >= 0 and y^2 = x
    #+END_EXAMPLE

    This describes a perfectly legitmate mathematical function. We
    could use it to recognize whether one number is the square root of
    another, or to derive facts about square roots in general. On the
    other hand, the definition does not describe a procedure. Indeed,
    it tells us almost nothing about _how_ to actually find the square
    root of a given number. It will not help matters to rephrase this
    definition in pseudo lisp:

    #+BEGIN_SRC scheme
    (define (sqrt x)
      (the y (and (>= y 0)
                  (= (square y) x))))
    #+END_SRC

    This only begs the question.

    The contrast between function and procedure is a reflection of the
    general distinction between describing properties of things and
    describing how to do things, or, as it is sometimes referred to,
    the distinction between declarative knowledge and imperative
    knowledge. In mathematics we are usually concerned with
    declarative (what is) descriptions, whereas in computer science we
    are usually concerned with imperative (how to) descriptions.

    How does one compute square roots? The most common way is to use
    Newton's method of successive approximations, which says that
    whenever we have a guess _y_ for the value of the square root of a
    number _x_, we can perform a simple manimpulation to get a better
    guess (one closer to the actual square root) by averaging y with
    x, _x/y_. For example, we can compute the square root of 2 as
    follows. Suppose our initial guess is 1:

    Guess      Quotient                      Average
    1          (2/1) = 2                     ((2 + 1)/2) = 1.5
    1.5        (2/1.5) = 1.333               ((1.333 + 1.5)/2) = 1.4167
    1.4167     (2/1.4167) = 1.4118           ((1.4167 + 1.4118)/2) = 1.4142
    1.4142     ...                           ...

    Continuing this process, we obtain better and better
    approximations to the square root.

    Now let's formalize the process in terms of procedures. We start
    with a value for the radicand (the number whose square root we are
    trying to compute) and a value for the guess. If the guess is good
    enough for our purposes, we are done; if not, we must repeat the
    process with an improved guess. We write this basic strategy as a
    procedure:

    #+BEGIN_SRC scheme
    (define (sqrt-iter guess x)
      (if (good-enough? guess x)
          guess
          (squrt-iter (improve guess x)
                      x)))
    #+END_SRC

    A guess is improved by averaging it with the quotient of the
    radicand and the old guess:

    #+BEGIN_SRC scheme
    (define (improve guess x)
      (average guess (/ x guess)))
    #+END_SRC

    where

    #+BEGIN_SRC scheme
    (define (average x y)
      (/ (+ x y) 2))
    #+END_SRC

    We also have to say what we mean by "good enough." The following
    will do for illustration, but it is not really a very good
    test. The idea is to improve the answer until it is close enough
    so that its square differs from the radicand by less than a
    predetermined tolerance (here 0.001):

    #+BEGIN_SRC scheme 
    (define (good-enough? guess x)
      (< abs (- (square guess) x) 0.001))
    #+END_SRC

    Finally, we need a way to get started. For instance, we can always
    guess that the square root of any number is 1:

    #+BEGIN_SRC scheme
    (define (sqrt x)
      (sqrt-iter 1.0 x))
    #+END_SRC

    If we type these definitions to the interpreter we can use sqrt
    just as we can use any procedure:

    #+BEGIN_SRC scheme
    (sqrt 9)
    3.00009
    (sqrt (+ 100 37))
    11.7046
    #+END_SRC

    The sqrt program also illustrates that the simple procedural
    language we have introduced so far is sufficient for writing any
    purely numerical program that one we could write in, say, C or
    Pascal. This might seem surprising, since we have not included in
    our language any iterative (looping) constructs that direct the
    computer to do something over and over again. Sqrt-iter, on the
    other hand, demonstrates, how iteration can be accomplished using
    no special construct other than the ordinary ability to call a
    procedure.

    - Exercise 1.6 Alyssa P. Hacker doesn't see why if needs to be
      provided as a special form. "Why can't I just define it as an
      ordinary procedure in terms of cond?" she asks. Alyssa's friend
      Eva Lu Ator claims this can indeed be done, and she defines a
      new version of if:

      #+BEGIN_SRC scheme
      (define (new-if predicate then-clause else-clause)
        (cond (predicate then-clause)
              (else else-clause)))
      #+END_SRC

      Eva demonstrates the program for Alyssa:

      #+BEGIN_SRC scheme
      (new-if (= 2 3) 0 5)
      5
      (new-if (= 1 1) 0 5)
      0 
      #+END_SRC

      Delighted, Alyssa uses new-if to rewrite the square-root
      program:

      #+BEGIN_SRC scheme
      (define (sqrt-iter guess x)
        (new-if (good-enough? guess x)
                guess
                (sqrt-iter (improve guess x)
                           x)))
      #+END_SRC

      What happens when Alyssa attempts to use this to compute square
      roots? Explain.

      What happens is that evaluation never stops. Because we are
      using applicative-order evaluation, that is, evaluating all
      arguments before passing them to the function, we recursively
      evaluate sqrt-iter and it never terminates because we always
      evaluate it before going into the new-if function.

    - Exercise 1.7 The good-enough? test used in computing square
      roots will not be very effective for finding the square roots of
      very small numbers. Also, in real computers, arithmetic
      operations are almost always performed with limited
      precision. This makes our test inadequate for very large
      numbers. Explain these statements, with examples showing how the
      test fails for small and large numbers. An alternative strategy
      for implementing good-enough? is to watch how guess changes from
      one iteration to the next and to stop when the change is a very
      small fraction of the guess. Design a square-root procedure that
      uses this kind of end test. Does this work better for small and
      large numbers?

      For the case of a very small number, because we have defined
      good-enough? with a number that isn't, really, all that terribly
      small, if we try to find the square root of a number even
      smaller, we will get an incorrect result.

      #+BEGIN_EXAMPLE
      > (sqrt 1e-6)
      3.126e-2
      > (* (sqrt 1e-6) (sqrt 1e-6))
      9.7722e-4
      #+END_EXAMPLE

      The answer 9.7722e-4 is way off. We can divide the answer by the
      radicand and get 977.22, which is very far from being 1, which
      it should be.

      #+BEGIN_EXAMPLE
      > (/ 9.7722e-4 1e-6)
      977.22
      #+END_EXAMPLE

      The problem is that if we subtract the square of a small but
      still wildly off guess from 1e-6, the answer will be less than
      .001. 

      For extremely large numbers there is a different problem. On
      this machine typing

      #+BEGIN_EXAMPLE
      > (sqrt 1e13)
      #+END_EXAMPLE
      
      never ends and results in an infinite loop. The problem is
      actually that our value is too small in this case rather than
      too large, basically. That is, when we are representing
      extremely large numbers we can only have so much precision in
      the 'small end' of the number, especially the fractional
      part. So, you can get as close as you're going to get to the
      approximation, but still be more than .001 away from the actual
      number.

      So if we run the improve procedure for 1e13 manually, and
      continue using the results (e.g. the improved guess) and feed
      them back into improve, eventually we get exactly the same
      number out of improve over and over again. This is

      3162277.6601683795 --

      #+BEGIN_EXAMPLE
      > (improve 3162277.6601683795 1e13)
      3162277.6601683795
      #+END_EXAMPLE

      But if we square that number:

      #+BEGIN_EXAMPLE
      > (square 3162277.6601683795)
      10000000000000.002
      #+END_EXAMPLE
      
      SInce the threshold is .001, the value of the good-enough?
      procedure is false, subtracting the two results in exactly .001
      which is not less than .001. Since improve will return the exact
      same result again, we will continue to evaluate the sqrt-iter
      procedure indefinitely.

      If we dive in deeper, we can look at what the improve procedure
      does:

      #+BEGIN_EXAMPLE
      > (average 3162277.6601683795 (/ 1e13 3162277.6601683795))
       3162277.6601683795
      #+END_EXAMPLE

      This must be the fault, it should be converging.

      (/ 1e13 3162277.6601683795)

      3162277.660168379

      The average of 3162277.660168379 and 3162277.6601683795 we can
      tell by manual inspection should not be the value
      3162277.6601683795, which is the exact same as the first
      parameter. Averaging two unequal numbers should never be exactly
      equal to one of the numbers. This is therefore an incorrect
      result from our procedure. We can break it down further by
      looking at the value of average:

      #+BEGIN_EXAMPLE
      > (+ 3162277.660168379 3162277.6601683795)
      6324555.320336759
      #+END_EXAMPLE

      If we check this by hand-evaluation we realize this isn't the
      correct value. When we add by hand we get:

      6324555.3203367585

      The computer, however, rounds this up by .5, since presumably it
      can't represent that many fractional points as well as a larger
      number, introducing the error! The rounded up result happens to
      be exactly the same as the result of doubling the first
      parameter, 3162277.660168379. So, we will continually get the
      same answer from improve, which will always be just a little
      off. So there needs to be a way to have a 'relative'
      measurement. We could do this by dividing the square of the
      guess with the number itself, which should give us a relative
      value.

      #+BEGIN_SRC scheme
      (define (good-enough? guess x)
        (< (abs (- 1 (/ x (square guess)))) .001))
      #+END_SRC

      running the same procedures that before were inaccurate or bad
      results in actual termination of algorithms at the expense (on
      the large number side) of less accuracy.  I decreased the
      threshold to .000001 right away.

      #+BEGIN_SRC scheme
      1 ]=> (square (sqrt 1e-16))
      ;Value: 1.0000000018865009e-16
      1 ]=> (square (sqrt 1e13))
      ;Value: 10000000024299.582
      #+END_SRC

      If we decrease threshold to 1e-12:

      #+BEGIN_SRC scheme
      1 ]=>  (square (sqrt 1e13))
      ;Value: 10000000000000.002
      1 ]=> (square (sqrt 1e-16))
      ;Value: 1.0000000000000001e-16
      #+END_SRC

      This is very good.

    - Exercise 1.8 Newton's method for cube roots is based on the fact
      that if _y_ is an approximation to the cube root of _x_, then a
      better approximation is given by the value:

      (x / y^2 + 2y) / 3

      Use this formula to implement a cube-root procedure analogous to
      the square-root procedure. (In section 1.3.4 we will see how to
      implement Newton's method in general as an abstraction of these
      square-root and cube-root procedures).
      
      #+BEGIN_SRC scheme
      (define THRESHOLD 1e-10)
      (define (cube-root-iter guess x)
        (if (good-enough-cube-root? guess x)
             guess
            (cube-root-iter (improve-cube-root-guess guess x) x)))
      (define (good-enough-cube-root? guess x)
        (< (abs (- 1 (/ x (* guess guess guess)))) THRESHOLD))
      (define (improve-cube-root-guess guess x)
        (/ (+ (/ x (square guess)) (* 2 guess)) 3))
      #+END_SRC
    
      #+BEGIN_SRC
      1 ]=> (cube-root-iter 1.0 27)
      
      ;Value: 3.0000000000000977
      
      1 ]=> (cube-root-iter 1.0 27.0)
      
      ;Value: 3.0000000000000977
      
      1 ]=> (* 4 4 4) 
      
      ;Value: 64
      
      1 ]=> (cube-root-iter 1.0 64)
      
      ;Value: 4.000000000076121
      
      1 ]=> (cube-root-iter 1.0 1e15)
      
      ;Value: 100000.0000002152
      
      1 ]=> (*  100000.0000002152  100000.0000002152  100000.0000002152)
      
      ;Value: 1000000000006455.9
      
      1 ]=>
      #+END_SRC

*** 1.1.8 Procedures as Black-Box Abstractions
    Sqrt is our first example of a process defined by a set of
    mutually defined procedures. Notice that the definition of
    sqrt-iter is _recursive_; that is, the procedure is defined in
    terms of itself. The idea of being able to define a procedure in
    terms of itself may be disturbing; it may seem unclear how such a
    "circular" definition could make sense at all, much less specify a
    well-defined process to be carried out by a computer. This will be
    addressed more carefully in section 1.2. But first let's consider
    some other important points illustrated by the sqrt example.

    Observe that the problem of computing square roots breaks up
    naturally into a number of subproblems: how to tell whether a
    guess is good enough, how to improve a guess, and so on. Each of
    these tasks is accomplished by a separate procedure. The entire
    sqrt program can be viewed as a cluster of procedures (shown in
    figure 1.2) that mirrors the decomposition of the problem into
    subproblems.

    The importance of this decomposition strategy is not simply that
    one is dividing the program into parts. After all, we could take
    any large program and divide it into parts -- the first ten lines,
    the next ten lines, the next ten lines, and so on. Rather, it is
    crucial that each procedure accomplishes an identifiable task that
    can be used as a module in defining other procedures. For
    example, when we define the good-enough? procedure in terms of
    square, we are able to regard the square procedure as a "black
    box". We are not at that moment concerned with _how_ the procedure
    computes its result, only with the fact that it computes the
    square. The details of how the square is computed can be
    suppressed, to be considered at a later time. Indeed, as far as
    the good-enough? procedure is concerned, square is not quite a
    procedure but rather an abstraction of a procedure, a so-called
    _procedural abstraction_. At this level of abstraction, any
    procedure that computes the square is equally good.

    Thus, considering only the values they return, the following two
    procedures for squaring a number should be indistinguishable. Each
    takes a numerical argument and produces the square of that number
    as the value:

    #+BEGIN_SRC scheme
    (define (square x) (* x x))
    (define (square x)
      (exp (double (log x))))
    (define (double x) (+ x x))
    #+END_SRC

    So a procedure definition should be able to suppress detail. The
    users of the procedure may not have written the procedure
    themselves, but may have obtained it from another programmer as a
    black box. A user should not need to know how the procedure is
    implemented in order to use it.

***** Local names
      One detail of a procedure's implementation that should not
      matter to a user of the procedure is the implementer's choice of
      names for the procedure's formal parameters. Thus, the following
      procedures should not be distinguishable:

      #+BEGIN_SRC scheme
      (define (square x) (* x x))
      (define (square y) (* y y))
      #+END_SRC

      This principle -- that the meaning of a procedure should be
      independent of the parameter names used by its author -- seems
      on the surface to be self-evident, but its consequences are
      profound. The simplest consequence is that the parameter names
      of a procedure must be local to the body of the procedure. For
      example, we used square in the definition of good-enough? in our
      square-root procedure:

      #+BEGIN_SRC scheme
      (define (good-enough? guess x)
        (< (abs (- (square guess) x)) .001))
      #+END_SRC

      The intention of the author of good-enough? is to determine if
      the square of the first argument is within a given tolerance of
      the second argument. We see that the author of good-enough? used
      the name guess to refer to the first argument and x to refer to
      the second argument. The argument of square is guess. If the
      author of square used x (as above) to refer to that argument, we
      see that the x in good-enough? must be a different x than the
      one in square. Running the procedure square must not affect the
      value of x that is used by good-enough?, because that value of x
      may be needed by good-enough? after square is done computing.

      If the parameters were not local to the bodies of their
      respective procedures, then the parameter x in square could be
      confused with the parameter x in good-enough?, and the behavior
      of good-enough? would depend upon which version of square we
      used. Thus, square would not be the black box we desired.

      A formal parameter of a procedure has a very special role in the
      procedure definition, in that it doesn't matter what name the
      formal parameter has. Such a name is called a _bound variable_,
      and we say that the procedure definition _binds_ its formal
      parameters. The meaning of a procedure definition is unchanged
      if a bound variable is consistently renamed throughout the
      definition. If a variable is not bound, we say that it is
      _free_. The set of expressions for which a binding defines a
      name is called the _scope_ of that name. In a procedure
      definition, the bound variables declared as the formal
      parameters of the procedure have the body of the procedure as
      their scope.

      In the definition of good-enough? above, guess and x are bound
      variables but <, -, abs, and square are free. The meaning of
      good-enough? should be independent of the names we choose for
      guess and x so long as they are distinct and different from <,
      -, abs, and square. (If we renamed guess to abs we would have
      introduced a bug by _capturing_ the variable abs. It would have
      changed from free to bound.) The meaning of good-enough? is not
      independent of the names of its free variables, however. it
      surely depends upon the fact (external to this definition) that
      the symbol abs names a procedure for computing the absolute
      value of a number. Goodn-enough? will compute a different
      function if we substitute cos for abs in its definition.

      
****** Internal definitions and block structure
       We have one kind of name isolation available to us so far: The
       formal parameters of a procedure are local to the body of the
       procedure. The square-root program illustrates another way in
       which we would like to control the use of names. The existing
       program consists of separate procedures:

       #+BEGIN_SRC scheme
       (define (sqrt x)
         (sqrt-iter 1.0 x))
       (define (sqrt-iter guess x)
         (if (good-enough? guess x)
             guess
             (sqrt-iter (improve guess x) x)))
       (define (good-enough? guess x)
         (< (abs (- (square guess) x)) .001))
       (define (improve guess x)
         (average guess (/ x guess)))  
       #+END_SRC

       The problem with this program is that the only procedure that
       is important to users of sqrt is sqrt. The other procedures
       (sqrt-iter, good-enough?, and improve) only clutter up their
       minds. They may not define any other procedure called
       good-enough? as part of another program to work together with
       the square-root program, because sqrt needs it. The problem is
       especially severe in the construction of large systems by many
       separate programmers. For example, in the construction of a
       large library of numerical procedures, many numerical functions
       are computed as successive approximations and thus might have
       procedures named good-enough? and improve as auxiliary
       procedures. We would like to localize the subprocedures, hiding
       them inside sqrt so that sqrt could coexist with other
       successive approximations, each having its own private
       good-enough? procedure. To make this possible, we allow a
       procedure to have internal definitions that are local to that
       procedure. For example, in the square-root problem we can write

       #+BEGIN_SRC scheme
       (define (sqrt x)
         (define (good-enough? guess x)
           (< (abs (- (square guess) x)) 0.001))
         (define (improve guess x)
           (average guess (/ x guess)))
         (define (sqrt-iter guess x)
           (if (good-enough? guess x)
               guess
               (sqrt-iter (improve guess x) x)))
        (sqrt-iter 1.0 x))
       #+END_SRC

       Such nesting of definitions, called _block structure_, is
       basically the right solution to the simplest name-packaging
       problem. But there is a better idea lurking here. In addition
       to internalizing the definitiosn of the auxiliary procedures,
       we can simplify them. Since x is bound in the definition of
       sqrt, the procedures good-enough?, improve, and sqrt-iter,
       which are defined internally to sqrt, are in the scope of
       x. Thus it is not necessary to pass x explicitly to each of
       these procedures. Instead, we allow x to be a free variable in
       the internal definitions, as shown below. Then x gets its value
       from the argument with which the enclosing procedure sqrt is
       called. This discipline is called _lexical scoping_.

       #+BEGIN_SRC scheme
       (define (sqrt x)
         (define (good-enough? guess)
           (< (abs (- (square guess) x)) 0.001))
         (define (improve guess)
           (average guess (/ x guess)))
         (define (sqrt-iter guess)
           (if (good-enough? guess)
               guess
               (sqrt-iter (improve guess))))
         (sqrt-iter 1.0))
       #+END_SRC

       We will use block structure extensively to help us break up
       large programs into tractable pieces. The idea of block
       structure originated with programming language Algol 60. It
       appears in most advanced programming languages and is an
       important tool for helping to organize the construction of
       large programs.

** 1.2 Procedures and the Processes They Generate
   We have now considered the elements of programming: We have used
   primitive arithmetic operations, we have combined these operations,
   and we have abstracted these composite operations by defining them
   as compound procedures. But that is not enough to enable us to say
   that we know hwo to program. Our situation is analogous to that of
   someone who has learned the rules for how the pieces move in chess
   but knows nothing of typical openings, tactics, or strategy. Like
   the novice chess player, we don't yet know the common patterns of
   usage in the domain. We lack the knowledge of which moves are worth
   making (which procedures are worth defining). We lack the
   experience to predict the consequences of making a move (executing
   a procedure). 

   The ability to visualize the consequences of the actions under
   consideration is crucial to becoming an expert programmer, just as
   it is in any synthetic, creative activity. In becoming an expert
   photographer, for example, one must learn how to look at a scene
   and know how dark each region will appear on a print for each
   possible choice of exposure and development conditions. Only then
   can one reason backward, planning framing, lighting, exposure, and
   development to taken by a process and where we control the process
   by means of a program. To become experts, we must learn to
   visualize the processes generated by various types of
   procedures. Only after we have developed such a skill can we learn
   to reliably construct programs that exhibit the desired behavior.

   A procedure is a pattern for the _local evolution_ of a
   computational process. It specifies how each stage of the process
   is built upon the previous stage. We would like to be able to make
   statements about the overall, or _global_, behavior of a process
   whose local evolution has been specified by a procedure. This is
   very difficult to do in general, but we can at least try to
   describe some typical patterns of process evolution.

   In this section, we will examine some common "shapes" for the
   processes generated by simple procedures. We will also investigate
   the rates at which these processes consume the important
   computation resources of time and space. The procedures we will
   consider are very simple. Their role is like that played by test
   patterns in photography: as oversimplified prototypical patterns,
   rather than practical examples in their own right.

*** 1.2.1. Linear recursion and Iteration
    We begin by considering the factorial function, defined by

    n! = n * (n - 1) * (n - 2) ... 3 * 2 * 1

    There are many ways to compute factorials. One way is to make use
    of the observation that n! is equal to n times (n - 1)! for any
    positive integer n:

    n! = n * [(n - 1) * (n - 2) ... 3 * 2 * 1] = n * (n - 1)!

    Thus, we can compute n! by computing (n - 1)! and multiplying the
    result by n. If we add the stipulation that 1! is equal to 1, this
    observation translates directly into a procedure:

    #+BEGIN_SRC scheme
    (define (factorial n)
      (if (= n 1)
          1
          (* n (factorial (- n 1)))))
    #+END_SRC

    We can use the substitution model of section 1.1.5 to watch this
    procedure in action computing 6!, as shown in figure 1.3

    #+BEGIN_SRC scheme
    (factorial 6)
    (* 6 (factorial 5))
    (* 6 (* 5 (factorial 4)))
    (* 6 (* 5 (* 4 (factorial 3))))
    ...
    (* 6 (* 4 (* 3 (* 2 1))))
    (* 6 (* 4 (* 3 2)))
    ...
    720
    #+END_SRC
    
    Now let's take a different perspective on computing factorials. We
    could describe the rule for computing n! by specifying that we
    first multiply 1 by 2, then multiply the result by 3, then by 4,
    and so on until we reach n. More formally, we maintain a running
    product, together with a counter that counts from 1 up to n. We
    can describe the computation by saying that the counter and the
    product simultaneously change from one step to the next according
    to the rule

    product = counter * product
    counter = counter + 1

    and stipulating that n! is the value of the product when the
    counter exceeds n.

    Once again, we can recast our description as a procedure for
    computing factorials:

    #+BEGIN_SRC scheme
    (define (factorial n)
      (fact-iter 1 1 n))
    (define (fact-iter product counter max-count)
      (if (> counter max-count)
          product
          (fact-iter (* counter product)
                     (+ counter 1)
                     max-count)))
    #+END_SRC

    As before, we can use the substitution model to visualize the
    process of computing 6!, as shown in figure 1.4.

    #+BEGIN_SRC scheme
    (factorial 6)
    (fact-iter 1 1 6)
    (fact-iter 1 2 6)
    (fact-iter 2 3 6)
    (fact-iter 6 4 6)
    (fact-iter 24 5 6)
    (fact-iter 120 6 6)
    (fact-iter 720 7 6)
    #+END_SRC

    Once again, we can recast our description as a procedure for
    computing factorials:

    #+BEGIN_SRC
    (define (factorial n)
      (fact-iter 1 1 n))
    (define (fact-iter product counter max-count)
      (if (> counter max-count)
          product
          (fact-iter (* counter product)
                     (+ counter 1)
                     max-count)))
    #+END_SRC

    Compare the two processes. From one point of view, they hardly
    seem different at all. Both compute the same mathematical function
    on the same domain, and each requires a number of steps
    proportional to n to compute n!. Indeed, both processes even carry
    out the same sequence of multiplications, obtaining the same
    sequence of partial products. On the other hand, when we consider
    the "shapes" of the two processes, we find that they evolve quite
    differently.

    Consider the first process. The substitution model reveals a shape
    of expansion followed by contraction, indicated by the arrow in
    figure 1.3. The expansion occurs as the process builds up a chain
    of _deferred operations_ (in this case, a chain of
    multiplications). The contraction occurs as the operations are
    actually performed. This type of process, characterized by a chain
    of deferred operations, is called a _recursive process_. Carrying
    out this process requires that the interpreter keep track of the
    operations to be performed later on. In the computation of n!, the
    length of the chain of deferred multiplications, and hence the
    amount of information needed to keep track of it, grows linearly
    with _n_ (is proportional to _n_), just like the number of
    steps. Such a process is called a _linear recursive process_.

    By contrast, the second process does not grow or shrink. At each
    step, all we need to keep track of, for any _n_, are the current
    values of the variables product, counter, and max-count. We call
    this an _iterative process_. In general, an iterative process is
    one whose state can be summarized by a fixed number of _state
    variables_, together with a fixed rule that describes how the
    state variables should be updated as the process moves from state
    to state and an (optional) end test that specifies conditions
    under which the process should terminate. In computing n!, the
    number of steps required grows linearly with _n_. Such a process
    is called a _linear iterative process_.

    The contrast between the two processes can be seen in another
    way. In the iterative case, the program variables provide a
    complete description of the state of the process at any point. If
    we stopped the computation between steps, all we would need to do
    to resume the computation is to supply the interpreter with the
    values of the three program variables. Not so with the recursive
    process. In this case there is some additional "hidden"
    information, maintained by the interpreter and not contained in
    the program variables, which indicates "where the process is" in
    negotiating the chain of deferred operations. The longer the
    chain, the more information must be maintained.

    In constrasting iteration and recursion, we must be careful not to
    confuse the notion of a recursive _process_ with the notion of a
    recursive _procedure_. When we describe a procedure as recursive,
    we are referring to the syntactic fact that the procedure
    definition refers (either directly or indirectly) to the procedure
    itself. But when we describe a process as following a pattern that
    is, say, linearly recursive, we are speaking about how the process
    evolves, not about the syntax of how a procedure is written. It
    may seem disturbing that we refer to a recursive procedure such as
    fact-iter as generating an iterative process. However, the process
    really is terative: Its state is captured completely by its three
    state variables, and an interpreter need keep track of only three
    variables in order to execute the process.

    One reason that the distinction between processs and procedure may
    be confusing is that most implementations of common languages
    (including Ada, Pascal, and C) are designed in such a way that the
    interpretation of any recursive procedure consumes an amount of
    memory that grows with the number of procedure calls, even when
    the process described is, in principle, iterative. as a
    consequence, these languages can describe iterative processes only
    by resorting to special-purpose "looping constructs" such as do,
    repeat, until, for and while. The implementation of Scheme we
    shall consider in chapter 5 does not share this defect. It will
    execute an iterative process in constant space, even if the
    iterative process is described by a recursive procedure. An
    implementation with this property is called _tail-recursive_. With
    a tail recursive implementation, iteration can be expressed using
    the ordinary procedure call mechanism, so that special iteration
    constructs are useful only as syntactic sugar.

    - Exercise 1.9. Each of the following two procedures defines a
      method for adding two positive integers in terms of the
      procedures inc, which increments its argument by 1, and dec,
      which decrements its argument by 1.

      #+BEGIN_SRC scheme
      (define (+ a b)
        (if (= a 0)
            b
            (inc (+ (dec a) b))))
      (define (+ a b)
        (if (= a 0)
            b
            (+ (dec a) (inc b))))
      #+END_SRC

      Using the substitution model, illustrate the process generated
      by each procedure in evaluating (+ 4 5). Are these processes
      iterative or recursive?

      #+BEGIN_SRC scheme
      (+ 4 5)
      (if (= 4 0) 5 (inc (+ (dec 4) 5)))
      (inc (+ 3 5))
      (inc (if (= 3 0) 5 (inc (+ (dec 3) 5))))
      (inc (inc (+ 2 5)))
      (inc (inc (inc (+ 1 5))))
      (inc (inc (inc (inc (+ 0 5)))))
      (inc (inc (inc (inc 5))))
      (inc (inc (inc 6)))
      (inc (inc 7))
      (inc 8)
      9

      (+ 4 5)
      (if (= 4 0) 5 (+ (dec 4) (inc 5)))
      (if false 5 (+ (dec 4) (inc 5)))
      (+ (dec 4) (inc 5))
      (+ 3 6)
      (if (= 3 0) 6 (+ (dec 3) (inc 6)))
      (if false 6 (+ (dec 3) (inc 6)))
      (+ (dec 3) (inc 6))
      (+ 2 7)
      (+ 1 8)
      (+ 0 9)
      9
      #+END_SRC

      The first process is linearly recursive, the second on e is
      iterative.

    - Exercise 1.10. The following procedure computes a mathematical
      function called Ackerman's function.

      #+BEGIN_SRC scheme
      (define (A x y)
        (cond ((= y 0) 0)
              ((= x 0) (* 2 y))
              ((= y 1) 2)
              (else (A (- x 1)
                       (A x (- y 1))))))
      #+END_SRC

      What are the values of the following expressions?

      #+BEGIN_SRC scheme
      (A 1 10)
      (A 2 4)
      (A 3 3)
      #+END_SRC

      #+BEGIN_SRC scheme
      (A 1 10)
      (cond ((= 10 0) 0)
            ((= 1 0) (* 2 10))
            ((= 10 1) 2)
            (else (A (- 1 1)
                     (A 1 (- 10 1)))))
      (cond (false 0)
            ((= 1 0) (* 2 10))
            ((= 10 1) 2)
            (else (A (- 1 1)
                     (A 1 (- 10 1)))))
      (cond ((= 1 0) (* 2 10))
            ((= 10 1) 2)
            (else (A (- 1 1)
                     (A 1 (- 10 1)))))
      (cond (false (* 2 10))
            ((= 10 1) 2)
            (else (A (- 1 1)
                     (A 1 (- 10 1)))))
      (cond ((= 10 1) 2)
            (else (A (- 1 1)
                     (A 1 (- 10 1)))))
      (cond (false 2)
            (else (A (- 1 1)
                     (A 1 (- 10 1)))))
      (cond (else (A (- 1 1)
                     (A 1 (- 10 1)))))       
      (A (- 1 1)
         (A 1 (- 10 1)))
      (A (- 1 1) (A 1 9))
      (A (- 1 1) 
         (cond ((= 9 0) 0)
               ((= 1 0) (* 2 9))
               ((= 9 1) 2)
               (else (A (- 1 1)
                        (A 1 (- 9 1))))))
      (A (- 1 1) 
         (A (- 1 1)
            (A 1 (- 9 1))))
      (A (- 1 1) 
         (A (- 1 1)
            (A 1 8)))
      (A (- 1 1) 
         (A (- 1 1)
            (cond ((= 8 0) 0)
                  ((= 1 0) (* 2 8))
                  ((= 8 1) 2)
                  (else (A (- 1 1)
                           (A 1 (- 8 1)))))))
      (A (- 1 1) 
         (A (- 1 1)
            (A (- 1 1)
               (A 1 (- 8 1)))))
      (A (- 1 1) 
         (A (- 1 1)
            (A (- 1 1)
               (A 1 7))))
      (A (- 1 1) 
         (A (- 1 1)
            (A (- 1 1)
               (A 1 7))))
      (A (- 1 1) 
         (A (- 1 1)
            (A (- 1 1)
               (A (- 1 1)
                  (A 1 6)))))
      (A (- 1 1) 
         (A (- 1 1)
            (A (- 1 1)
               (A (- 1 1)
                  (A (- 1 1)
                     (A 1 5))))))
      (A (- 1 1) 
         (A (- 1 1)
            (A (- 1 1)
               (A (- 1 1)
                  (A (- 1 1)
                     (A (- 1 1)
                        (A 1 4)))))))
      (A (- 1 1) 
         (A (- 1 1)
            (A (- 1 1)
               (A (- 1 1)
                  (A (- 1 1)
                     (A (- 1 1)
                        (A (- 1 1)
                           (A 1 3))))))))
      (A (- 1 1) 
         (A (- 1 1)
            (A (- 1 1)
               (A (- 1 1)
                  (A (- 1 1)
                     (A (- 1 1)
                        (A (- 1 1)
                           (A (- 1 1)
                              (A 1 2)))))))))
      (A (- 1 1) 
         (A (- 1 1)
            (A (- 1 1)
               (A (- 1 1)
                  (A (- 1 1)
                     (A (- 1 1)
                        (A (- 1 1)
                           (A (- 1 1)
                              (A (- 1 1)
                                 (A 1 1))))))))))
      (A (- 1 1) 
         (A (- 1 1)
            (A (- 1 1)
               (A (- 1 1)
                  (A (- 1 1)
                     (A (- 1 1)
                        (A (- 1 1)
                           (A (- 1 1)
                              (A (- 1 1)
                                 2)))))))))
      (A (- 1 1) 
         (A (- 1 1)
            (A (- 1 1)
               (A (- 1 1)
                  (A (- 1 1)
                     (A (- 1 1)
                        (A (- 1 1)
                           (A (- 1 1)
                              4))))))))
      (A (- 1 1) 
         (A (- 1 1)
            (A (- 1 1)
               (A (- 1 1)
                  (A (- 1 1)
                     (A (- 1 1)
                        (A (- 1 1)
                           8)))))))
      ...
      1024
      #+END_SRC

      (A 0 n) == 2*n
      (A 1 n) == 2^n
      (A 2 n) == 2^2^n (?) 2^2^4 == 65536
      
      #+BEGIN_SRC scheme
      (A 2 4)
      (cond ((= 4 0) 0)
            ((= 2 0) (* 2 4))
            ((= 4 1) 2)
            (else (A (- 2 1)
                     (A 2 (- 4 1)))))
      (A (- 2 1) 
         (A 2 (- 4 1)))
      (A 1 (A 2 3))
      (A 1
         (A 1
            (A 1 2)))
      (A 1
         (A 1
            (A 1
               (A 1 1))))
      (A 1
         (A 1
            (A 1 2)))
      (A 1
         (A 1
            (A 0 (A 1 1))))
      (A 1
         (A 1
            (A 0 2)))
      (A 1
         (A 1 4))
      (A 1 16)
      65536
      #+END_SRC

      #+BEGIN_SRC scheme
      (A 3 3)
      (A 2 (A 3 2))
      (A 2 (A 2 (A 3 1)))
      (A 2 (A 2 2))
      (A 2 (A 1 (A 2 1)))
      (A 2 (A 1 2))
      (A 2 (A 0 (A 1 1)))
      (A 2 (A 0 2))
      (A 2 4)
      (A 1 (A 2 3))
      (A 1 (A 1 (A 2 2)))
      (A 1 (A 1 (A 1 (A 2 1))))
      (A 1 (A 1 (A 1 2)))
      (A 1 (A 1 (A 0 (A 1 1))))
      (A 1 (A 1 (A 0 2)))
      (A 1 (A 1 4))
      (A 1 16)
      65536
      #+END_SRC

      Consider the following procedures, where A is the procedure
      defined above:

      #+BEGIN_SRC scheme
      (define (f n) (A 0 n))
      (define (g n) (A 1 n))
      (define (h n) (A 2 n))
      (define (k n) (* 5 n n))
      #+END_SRC
      
      Give concise mathematical definitions for the functions computed
      by the procedures f, g, and h for positive integer values of
      n. For example, (k n) computes 5n^2.

      f = 2 * n
      g = 2^n
      h = 2^2^2...(n times)

*** 1.2.2 Tree Recursion

    Another common pattern in computation is called _tree
    recursion_. As an example, consider computing the sequence of
    Fibonacci numbers, in which each number is the sum of the
    preceding two:

    0,1,1,2,3,5,8,13,21,...

    In general, the Fibonacci numbers can be defined by the rule:

    #+BEGIN_EXAMPLE
             | 0 if n = 0 
    Fib(n)=  | 1 if n = 1 
             | Fib(n-1) + Fib(n-2) otherwise           
    #+END_EXAMPLE
    
    We can immediately translate this definition into a recursive
    procedure for computing Fibonacci numbers:
    
    #+BEGIN_SRC scheme
    (define (fib n)
      (cond ((= n 0) 0)
            ((= n 1) 1)
            (else (+ (fib (- n 1))
                     (fib (- n 2))))))
    #+END_SRC

    Consider the pattern of this computation. To compute (fib 5), we
    compute (fib 4) and (fib 3). To compute (fib 4), we compute
    (fib 3) and (fib 2). In general, the evolved process looks like a
    tree. Notice that the branches split into two at each level
    (except at the botton); this reflects the fact that the fib
    procedure calls itself twice each time it is invoked.

    This procedure is instructive as a prototypical tree recursion,
    but it is a terrible way to compute Fibonacci numbers because it
    does so much redundant computation. Notice in figure 1.5 that the
    entire computation of (fib 3) -- almost half the work -- is
    duplicated. In fact, it is not hard to show that the number of
    times the procedure will compute (fib 1) or (fib 0) (the number of
    leaves in the above tree, in general) is precisely Fib(n+1). To
    get an idea of how bad this is, one can show that the value of
    Fib(n) grows exponentially with n. More precisely (see exercise
    1.13), Fib(n) is the closest integer to phi^n/sqrt(5), where

    phi = (1 + sqrt(5))/2 ~= 1.6180

    is the _golden ratio_, which satisfies the equation

    phi^2 = phi + 1

    Thus, the process uses a number of steps that grows exponentially
    with the input. On the other hand, the space required grows only
    linearly with the input, because we need keep track only of which
    nodes are above us in the tree at any point in the computation. In
    general, the number of steps required by a tree-recursive process
    will be proportional to the number of nodes in the tree, while the
    space required will be proportional to the maximum depth of the
    tree.

    We can also formulate an iterative process for computing the
    Fibonacci numbers. The idea is to use a pair of integers _a_ and
    _b_, initialized to _Fib(1)_ = 1 and _Fib(0) = 0_, and to
    repeatedly reply the simultaneous transformations

    a <- a + b
    b <- a

    It is not hard to show that, after applying this transformation
    _n_ times, _a_ and _b_ will be equal, respectively, to _Fib(n+1)_
    and _Fib(n)_. Thus, we can compute Fibonacci numbers iteratively
    using the procedure

    #+BEGIN_SRC scheme
    (define (fib n)
      (fib-iter 1 0 n))
    (define (fib-iter a b count)
      (if (= count 0)
          b
          (fib-iter (+ a b) a (- count 1))))
    #+END_SRC

    This second method for computing _Fib(n)_ is a linear
    iteration. The difference in number of steps required by the two
    methods -- one linear in _n_, one growing as fast as _Fib(n)_
    itself -- is enormous, even for small inputs.
    
    One should not conclude from this that tree-recursive processes
    are useless. When we consider processes that operate on
    hierarchically structured data rather than numbers, we will find
    that tree recursion is a natural and powerful tool. But even in
    numerical operations, tree-recursive processes can be useful in
    helping us to understand and design programs. For instance,
    although the first fib procedure is much less efficient than the
    second one, it is more straightforward, being little more than a
    translation into Lisp of the definition of the Fibonacci
    sequence. To formulate the iterative algorithm required noticing
    that the computation could be recast as an iteration with three
    state variables.

**** Example: Counting change.

     It takes only a bit of cleverness to come up with the iterative
     Fibonacci algorithm. In contrast, consider the following problem:
     How many different ways can we make change of $1.00, given
     half-dollars, quarters, dimes, nickels, and pennies? More
     generally, can we write a procedure to compute the number of ways
     to change any given amount of money?

     This problem has a simple solution as a recursive
     procedure. Suppose we think of the types of coins available as
     arranged in some order. Then the following relation holds:

     The number of ways to change amount _a_ nusing _n_ kinds of coins
     equals

     - The number of wasys to change amount _a_ using all but the
       first kind of coin, plus
     - The number of ways to change amount a - d using all n kinds of
       coins, where d is the denomination of the first kind of coin.

       
     To see why this is true, observe that the ways to make change
     can be divided into two groups: those that do not use any of
     the first kind of coin, and those that do. Therefore, the total
     number of ways to make change for some amount is equal to the
     number of ways to make change for the amount without using the
     first kind of coin, plus the number of ways to make change
     assuming that we do use the first kind of coin, plus the number
     of ways to make change assuming that we do use the first kind of
     coin. But the latter number is equal to the number of ways to
     make change for the amount that remains after using a coin of the
     first kind.

     Thus, we can recursively reduce the problem of changing a given
     amount to the problem of changing smaller amounts using fewer
     kinds of coins. Consider this reduction rule carefully, and
     convince yourself that we can use it to describe an algorithm if
     we specify the following degenerate cases:

     - If _a_ is exactly 0, we should count that as 1 way to make change.
     - If _a_ is less than 0, we should count that as 0 ways to make change.
     - If _n_ is 0, we should count that as 0 ways to make change.

     We can easily translate this description into a recursive
     procedure:

     #+BEGIN_SRC scheme
     (define (count-change amount)
       (cc amount 5))
     (define (cc amount kinds-of-coins)
       (cond ((= amount 0) 1)
             ((or (< amount 0) (= kinds-of-coins 0)) 0)
             (else (+ (cc amount
                          (- kinds-of-coins 1))
                      (cc (- amount
                             (first-denomination kinds-of-coins))
                          kinds-of-coins)))))
     (define (first-denomination kinds-of-coins)
       (cond ((= kinds-of-coins 1) 1)
             ((= kinds-of-coins 2) 5) 
             ((= kinds-of-coins 3) 10)
             ((= kinds-of-coins 4) 25)
             ((= kinds-of-coins 5) 50)))
     #+END_SRC

     (The first-denominator procedure takes as input the number of
     kinds of coins available and returns the denomination of the
     first kind. Here we are thinking of the coins as arranged in
     order from largest to smallest, but any order would do as well.)
     We can now answer our original question about changing a dollar:

     #+BEGIN_SRC scheme
     (count-change 100)
     292
     #+END_SRC
     
     count-change generates a tree-recursive process with redundancies
     similar to those in our first implementation of fib. (It will
     take quite a while for that 292 to be computed.) ON the other
     hand, it is not obvious how to design a better algorithm for
     computing the result, and we leave this problem as a
     challenge. The observation that a tree-recursive process may be
     highly inefficient but often easy to specify and understand has
     led people to propose that one could get the best of both worlds
     by designing a "smart compiler" that could transform
     tree-recursive procedures into more efficient procedures that
     compute the same result.

     - Exercise 1.11. A function _f_ is defined by the rule that
       _f(n) = n_ if _n < 3_ and _f(n) = f(n - 1) + 2 * f(n - 2) + 3 *
       f(n - 3)_ if _n>=3_. Write a procedure that computes _f_ by
       means of a recursive process. Write a procedure that computes
       _f_ by means of an iterative process.

       Recursive:
       #+BEGIN_SRC scheme
       (define (f-rec n)
         (cond
           ((< n 3) n)
           (else (+ (f-rec (- n 1)) (* 2 (f-rec (- n 2))) (* 3 (f-rec (- n 3)))))))
       #+END_SRC

       #+BEGIN_SRC scheme
       (define (f n i acc1 acc2 acc3)
         (cond 
            ((= i n) (+ acc1 (* 2 acc2) (* 3 acc3)))
            ((< n 3) n)                                ; if n is less than 3 just return n.
            ((< i 3) (f n 3 2 1 0))                    ; otherwise, start at 3 for i.
            (else (f n (+ i 1) (+ acc1 (* 2 acc2) (* 3 acc3)) acc1 acc2)))) ; iterate.
       #+END_SRC

     - Exercise 1.12. The following pattern of numbers is called
       _Pascal's triangle_.

       #+BEGIN_SRC scheme
              1
             1 1
            1 2 1
           1 3 3 1
          1 4 6 4 1
            ...
       #+END_SRC

       The numbers at the edge of the triangle are all 1, and each
       number inside the triangle is the sum of the two numbers above
       it. Write a procedure that computes the elements of Pascal's
       triangle by means of a recursive process.

       1 1 1 1 2 1 1 3 3 1

       row/column -- 

       #+BEGIN_SRC scheme
       (define (pascal i j)
         (cond
           ((or (= i 1) (= j 1) (= j i)) 1)
           (else
            (+ (pascal (- i 1) (- j 1))
               (pascal (- i 1) j)))))
       #+END_SRC

     - Exercise 1.13. Prove that Fib(n) is the closest integer to
       phi^n/sqrt(5), where phi = (1 + sqrt(5)) / 2. Let omega=(1 -
       sqrt(5))/2. Use induction and the definition of the Fibonacci
       numbers to prove that Fib(n) = (phi^n - omega^n)/sqrt(5)
    
       fib(n) = 1 1 2 3 5 8 13 ...
       assume fib(n) also = (phi^n - omega^n)/sqrt(5)

       n = 1 checks.
       n = 2 checks.

       assume that Fib'(n) works.
       Fib'(n+1) = 
       #+BEGIN_EXAMPLE
       (phi^(n - 1) - omega(n-1))/sqrt(5) 
       + 
       (phi^(n) - omega(n))/sqrt(5)

       = 

       (phi^(n - 1) + phi^n - omega^(n) - omega^(n-1)) / sqrt(5)

       =

       (phi^n * (phi^-1 + 1) - omega^n*(omega^-1 + 1)) / sqrt(5)

       = 
       
       
       #+END_EXAMPLE
       
       and

       #+BEGIN_EXAMPLE
       (phi^(n + 1) - omega(n+1)) / sqrt(5)
       #+END_EXAMPLE
      
       ... Basically you do a lot of algebra.

*** 1.2.3. Orders of Growth
    The previous examples illustrate that processes can differ
    considerably in the rates at which they consume computational
    resources. One convenient way to describe this difference is to
    use the notion of _order of growth_ to obtain a gross measure of
    the resources required by a process as the inputs become larger.

    Let _n_ be a parameter that measures the size of the problem, and
    let R(n) be the amount of resources the process requires for a
    problem of size _n_. In our previous examples we took _n_ to be
    the number for which a given function is to be computed, but there
    are other possibilities. For instance, if our goal is to compute
    an approximation to the square root of a number, we might take _n_
    to be the number of digits accuracy required. For matrix
    multiplication we might take _n_ to be the number of rows in the
    matrices. In general there are a number of properties of the
    problem with respect to which it will be desirable to analyze a
    given process. Similarly, R(n) might measure the number of
    internal storage registers used, the number of elementary machine
    operations performed, and so on. In computers that do only a fixed
    number of operations at a time, the time required will be
    proportional to the number of elementary machine operations
    performed.

    We say that R(n) has order of growth O(f(n)) (O is theta), written
    R(n) = O(f(n)) (pronounced "theta of n of f(n)"), if there are
    positive constants k_1 and k_2 independent ofn such that

    k_1*f(n) <= R(n) <= k_2*f(n)

    for any sufficiently large value of _n_. (In other words, for
    large _n_, the value R(n) is sandwiched between k_1*f(n) and
    k_2*f(n).)

    For instance, with linear recursive process for computing
    factorial described in section 1.2.1 the number of steps grows
    proportionally to the input _n_. Thus the steps required for this
    process grows as O(n). We also saw that the space require grows
    as O(n). For iterative factorial, the number of steps is still
    O(n) but the space required is O(1) -- that is, constant. The
    tree-recursive Fibonacci computation requries O(phi^n) steps and
    space O(n), where phi is the golden ratio.

    Orders of growth provide only a crude description of the behavior
    of a process. For example, a process requiring n^2 steps and a
    process requiring 1000n^2 steps and a process requiring 3*n^2 +
    10*n + 17 steps all have O(n^2) order of growth. On the other
    hand, order of growth provides a useful indication of how we may
    expect the behavior of the process to change as we change the size
    of the problem. For a O(n) (linear) process, doubling the size
    will roughly double the amount of resources used. For an
    exponential process, each increment in problem size will multiply
    the resource utilization by a constant factor. In the remainder of
    section 1.2 we will examine two algorithms whose order of growth
    is logarithmic, so that soubling the problem size increases the
    resource requirement by a constant amount.

    - Exercise 1.14 Draw the tree illustrating the process generated
      by the count-change procedure of section 1.2.2 in making change
      for 11 cents. What are the orders of growth of the space and
      number of steps used by this process as the amount to be changed
      increases?

      #+BEGIN_SRC scheme
      (define (count-change amount)
        (cc amount 5))
      (define (cc amount kinds-of-coins)
        (cond ((= amount 0) 1)
              ((or (< amount 0) (= kinds-of-coins 0)) 0)
              (else (+ (cc amount
                           (- kinds-of-coins 1))
                       (cc (- amount
                              (first-denomination kinds-of-coins))
                           kinds-of-coins)))))
      (define (first-denomination kinds-of-coins)
        (cond ((= kinds-of-coins 1) 1)
              ((= kinds-of-coins 2) 5)
              ((= kinds-of-coins 3) 10)
              ((= kinds-of-coins 4) 25)
              ((= kinds-of-coins 5) 50)))
      #+END_SRC 
      
      When we add amounts, especially for large n (large amount to be
      changed) for each coin type smaller than the
      largest, we end up with at least one branch of execution that
      results in 2 * n number of calls. This is for each coin type we
      have, so 5 * 2 * n. However, when it's a large amount, for each
      time we subtract a larger (not size 1) coin from the amount, we
      again end up with another branch with at the left edge roughly n
      calls. Because there are five coins, we can say that for large
      numbers most layers of the tree will have five branches and that
      the tree will have a depth of roughly n. Therefore, the order of
      growth in terms at least of time of computation/steps is 5^n. We
      can generalize m to be the number of coins and get O(m^n), it
      will never exceed this and will never be very far behind it
      either. Since there are deferred computations, the space order
      of growth isn't constant. Each function call results in two more
      function calls. The total number of function calls is O(m^n).
      But this is inaccurate, 5^n is a massive number for even more
      than five or six levels of recursion (n) and doesn't map at all
      to the actual number of operations.

      The space order of growth is O(n), the stack can only get n deep
      at a time because we finish one function call before going on to
      the next.

      The function itself will always have either two branches or
      none. We count every function call as a unit of work. 

      For every unit of currency, we divide n by the size of unit of
      currency and end up with something roughly O(n) in size. 

      So for two kinds of currency we get

      cc(n, 1) + cc(n-5, 2)

      which is an O(n) call and n/5 O(n) calls or basically n^2.

      So for very large n we get

      cc(n, 1) + cc(n-5, 2) + cc(n-10, 3) + cc(n-25, 4) + cc(n-50, 5)
      
      Each reduces to so many O(n) calls. 

      O(n) + (n/5 O(n) calls) + (n/10 O(n^2) calls, each decays into
      cc(n-5, 2) calls) + (n/25 O(n^3) calls) + (n/50 O(n^4) calls)

      So it's n^m, where m is the number of kinds of currency.

    - Exercise 1.15. The sine of an angle (specified in radians) can
      be computed by making use of the approximation sin(x) =~ x if x
      is sufficiently small, and the trigonometric identity

      sin(x) = 3 * sin(x / 3) - 4 * sin^3(x / 3)

      to reduce the size of the argument of sin. (For purposes of this
      exercise an angle is considered "sufficiently small" if its
      magnitude is not greater than 0.1 radians.) These ideas are
      incorporated in the following procedures:

      #+BEGIN_SRC scheme
      (define (cube x) (* x x x))
      (define (p x) (- (* 3 x) (* 4 (cube x))))
      (define (sine angle)
        (if (not (> (abs angle) 0.1))
            angle
            (p (sine (/ angle 3.0)))))
      #+END_SRC

      a. How many times is the procedure p applied when (sine 12.15)
      is evaluated?

      We can see that p is a deferred call and sine is deferred until
      the angle gets sufficiently small, so all we have to do is solve
      this equation:

      12.15*(1/3^n) < 0.1
      
      1/3^n > 0.1 / 12.15
      log1/3(1/3^n) > log1/3(0.1/12.15)
      n > log1/3(0.1/12.5)
      n > 4.36
      So 5 calls are needed. In general the larger the n, the more
      calls required.
      
      We created a function for the number of calls already:

      n = ceil(log(0.1/12.5)/log(1/3))
      
      Since 12.5 is n:

      ceil(log(0.1/n)/log(1/3))
      
      We get rid of ceil and the log(1/3) which are constant factors:

      log(0.1/n)

      We use a logarithmic identity to reduce this:

      log(0.1) - log(n)

      and remove more constant factors:

      log(n). 

      When we removed log(1/3) we removed a negation that always made
      the result positive for integer n.

      The space asymptotics are the same since we have a single
      deferred call on the stack, so it grows as the number of
      function calls grows.

*** 1.2.4 Exponentiation

    Consider the problem of computing the exponential of a given
    number. We would like a procedure that takes a arguments a base b
    and a positive integer exponent n and compute b^n. One way to do
    this is with the recursive definition

    b^n = b * b^n - 1
    b^0 = 1

    which translates readily into the procedure

    #+BEGIN_SRC scheme
    (define (expt b n)
      (if (= n 0)
          1
          (* b (expt b (- n 1)))))
    #+END_SRC

    This is a linear recursive process, which requires O(n) steps and
    O(n) space. Just as with factorial, we can readily formulate an
    equivalent linear iteration:

    #+BEGIN_SRC scheme
    (define (expt b n)
      (expt-iter b n 1))
    (define (expt-iter b counter product)
      (if (= counter 0)
          product
          (expt-iter b (- counter 1) (* b product))))
    #+END_SRC

    This version requires O(n) steps and O(1) space.

    We can compute exponentials in fewer steps by using successive
    squaring. For instance, rather than computing b^8 as

    b * (b *(b *(b *(b *(b *(b *(b)))))))

    we can compute it using three multiplications:

    b^2 = b * b
    b^4 = b^2 * b^2
    b^8 = b^4 * b^4

    This method works fine for exponents that are powers of 2. We can
    also take advantage of successive squaring in computing
    exponentials in general if we use the rule

    b^n = (b^n/2)^2         if n is even
    b^n = (b * b^(n - 1))   if n is odd

    We can express this method as a procedure:

    #+BEGIN_SRC scheme
    (define (fast-expt b n)
      (cond ((= n 0) 1)
            ((even? n) (square (fast-expt b (/ n 2))))
            (else (* b (fast-expt b (- n 1))))))
    #+END_SRC

    where the predicate to test whether an integer is even is defined
    in terms of the primitive procedure remainder by

    #+BEGIN_SRC scheme
    (define (even? n)
      (= (remainder n 2)) 0)
    #+END_SRC

    The process evolved by fast-expt grows logarithmically with n in
    both space and number of steps. To see this, observe that
    computing b^2n using fast-expt requires only one more
    multiplication than computing b^n. The size of the exponent we can
    compute therefore doubles (approximately) with every new
    multiplication we are allowed. Thus, the number of multiplications
    required for an exponent of n grows about as fast as the logarithm
    of n to the base 2. The process has O(log n) growth.

    The difference between O(log n) growth and O(n) growth becomes
    striking as n becomes large. For example, fast-expt for n = 1000
    requires only 14 multiplications. It is also possible to use the
    idea of successive squaring to devise an iterative algorithm that
    computes exponentials with a logarithmic number of steps (see
    exercise 1.16), although, as is often the case with iterative
    algorithms, this is not written down so straightforwardly as the
    recursive algorithm.

    - Exercise 1.16. Design a procedure that evolves an iterative
      exponentiation process that uses successive squaring and uses a
      logarithmic number of steps, as does fast-expt. (Hint: Using the
      observation that (b^(n/2)^2) = (b^2)^n/2, keep, along with the
      exponent n and the base b, an additional state variable a, and
      define the state transformation in such a way that the product a
      b^n is unchanged from state to state. At the beginning of the
      process a is taken to be 1, and the answer is given by the value
      of a at the end of the process. In general, the technique of
      defining an _invariant quantity_ that remains unchanged from
      state to state is a powerful way to think about the design of
      iterative algorithms). 

      #+BEGIN_SRC scheme
      (define (fast-expt b n)
        (fast-expt-iter b n 1))
      (define (fast-expt-iter b n a)
        (cond
          ((= n 0) a)
          ((even? n) (fast-expt-iter (* b b) (/ n 2) a))
          ((odd? n) (fast-expt-iter b (- n 1) (* b a)))))
      #+END_SRC

    - Exercise 1.17 The exponentiation algorithms in this section are
      based on performing exponentiation by means of repeated
      multiplication. In a similar way, one can perform integer
      multiplication by means of repeated addition. The following
      multiplication procedure (in which it is assumed that our
      language can only add, not multiply) is analogous to the expt
      procedure:

      #+BEGIN_SRC scheme
      (define (* a b)
        (if (= b 0)
            0
            (+ a (* a (- b 1)))))
      #+END_SRC

      This algorithm takes a number of steps that is linear in b. Now
      suppose we include, together with addition, operations double,
      which doubles an integer, and halve, which divides an (even)
      integer by 2. 

      #+BEGIN_SRC scheme
      (define (double a) (+ a a))
      (define (halve a) (/ a 2))
      (define (mul a b)
        (cond
          ((= b 0) 0)
          ((= a 0) 0)
          ((= a 1) b)
          ((= b 1) a)
          ((even? a) (mul (halve a) (double b)))
          ((odd? a) (+ b (mul (- a 1) b)))))
      #+END_SRC

    - Exercise 1.18 Using the results of exercises 1.16 and 1.17
      devise a procedure that generates an iterative process for
      multiplying two integers in terms of adding, doubling and
      halving and uses a logarithmic number of steps.

      #+BEGIN_SRC scheme
      (define (mul a b) (mul-iter a b 0))
      (define (mul-iter a b acc)
        (cond
          ((or (= a 1) (= b 0) (= a 0)) (+ acc b))
          ((even? a) (mul-iter (halve a)  (double b) acc))
          ((odd? a) (mul-iter (- a 1) b (+ acc b)))))
      #+END_SRC

    - Exercise 1.19 There is a clever algorithm for computing the
      Fibonacci numbers in a logarithmic number of steps. Recall the
      transformation of the state variables a and b in the fib-iter
      process of section 1.2.2: a <- a + b and b <- a. Call this
      transformation T, and observe that applying T over and over
      again n times, starting with 1 and 0, produces the pair
      Fib(n + 1) and Fib(n). In other words, the Fibonacci numbers are
      produced by applying T^n, the nth power of the transformation T,
      starting with the pair (1, 0). Now consider T to be the special
      case of p = 0 and q = 1 in a family of transformations T_pq,
      where T_pq transforms the pair (a,b) according to a <- bq + aq +
      ap and b <- bp + aq. Show that if we apply such a transformation
      T_pq twice, the effect is the same as using a single
      transformation T_p'q' of the same form, and compute p' and q' in
      terms of p and q. This gives us an explicit way to square these
      transformations, and thus we can compute T^n using successive
      squaring, as in the fast-expt procedure. put this all together
      to complete the following procedure, which runs in a logarithmic
      number of steps:
      
      #+BEGIN_SRC scheme
      (define (fib n)
        (fib-iter 1 0 0 1 n))
      (define (fib-iter a b p q count)
        (cond ((= count 0) b)
              ((even? count)
               (fib-iter a b 
                             (+ (* p p) (* q q))
                             (+ (* 2 p q) (* q q))
                             (/ count 2)))
              (else (fib-iter (+ (* b q) (* a q) (* a p))
                              (+ (* b p) (* a q))
                              p
                              q
                              (- count 1)))))
      #+END_SRC

*** 1.2.5 Greatest Common Divisors

    The greatest common divisor (GCD) of two integers _a_ and _b_ is
    defined to be the largest integer that divides both a and b with
    no remainder. For example, the GCD of 16 and 28 is 4. In chapter
    2, when we investigate how to implement rational-number
    arithmetic, we will need to be able to compute GCDs in order to
    reduce rational numbers to lowest terms. (To reduce a rational
    number to lowest terms, we must divide both the numerator and the
    denominator by their GCD. For example, 16/28 reduces to 4/7). One
    way to find the GCD of two integers is to factor them and search
    for common factors, but there is a famous algorithm that is much
    more efficient.

    The idea of the algorithm is based on the observation that if r is
    the remainder when a is divided by b, then the common divisors of
    a and b are precisely the same as the common divisors of b and
    r. Thus, we can use the equation

    GCD(a, b) = GCD(b, r)

    to successively reduce the problem of computing a GCD to the
    problem of computing the GCD of smaller and smaller pairs of
    integers. For example,

    #+BEGIN_EXAMPLE
    GCD(206, 40) = GCD(40, 6)
                 = GCD(6, 4)
                 = GCD(4, 2)
                 = GCD(2, 0)
                 = 2
    #+END_EXAMPLE
    
    reduces GCD(206, 50) to GCD(2, 0) which is 2. It is possible to
    show that starting with any two positive integers and performing
    repeated reductions will always eventually produce a pair where
    the second number is 0. Then the GCD is the other number in the
    pair. THis method for computing the GCD is known as Euclid's
    Algorithm.

    It is easy to express Euclid's Algorithm as a procedure:

    #+BEGIN_SRC scheme
    (define (gcd a b)
      (if (= b 0)
          a
          (gcd b (remainder a b))))
    #+END_SRC

    This generates an iterative process, whose number of steps grows
    as the logarithm of the numbers involved.

    The fact that the number of steps required by Euclid's Algorithm
    has logarithmic growth bears an interesting relation to the
    Fibonacci numbers:

    Lame's Theorem: If Euclid's Algorithm requires k steps to compute
    the GCD of some pair, then the smaller number in the pair must be
    greater than or equal to the _k_th fibonacci number.

    We can use this theorem to get an order-of-growth estimate for
    Euclid's Algorithm. Let n be the smaller of the two inputs to the
    procedure. If the process takes k steps, then we must have n >=
    Fib(k) ~~ phi^k/sqrt(5). Therefore the number of steps grows as
    the logarithm (base phi) of n. Hence the order of growth is O(log
    n).

    - Exercise 1.20. The process that a procedure generates is of
      course dependent on the rules used by the interpreter. As an
      example, consider the iterative gcd procedure given
      above. Suppose we were to interpret this procedure using
      normal-order evaluation, as discussed in section 1.1.5. (The
      normal-order evaluation rule for if is described in exercise
      1.5) Using the substitution method (for normal order),
      illustrate the process generated in evaluatin (gcd 206 40) and
      indicate the remainder operations that are actually
      performed. How many remainder operations are actually performed
      in the normal-order evaluation of (gcd 206 40)? In the
      applicative-order evaluation?

      normal order:
      #+BEGIN_SRC scheme
      (define (gcd a b)
        (if (= b 0)
            a
            (gcd b (remainder a b))))

      (gcd 206 40)
      (if (= 40 0)
          206
          (gcd 40 (remainder 206 40)))
      (gcd 40 (remainder 206 40))
      (if (= (remainder 206 40) 0) 206 (gcd (remainder 206 40) (remainder 40 (remainder 206 40)))) ; 1 eval
      (gcd (remainder 206 40) (remainder 40 (remainder 206 40)))
      (if (= (remainder 40 (remainder 206 40)) 0)                                                  ; 2 eval -- false
          (remainder 206 40)
          (gcd (remainder 40 (remainder 206 40)) (remainder (remainder 206 40) (remainder 40 (remainder 206 40)))))
      (gcd (remainder 40 (remainder 206 40)) (remainder (remainder 206 40) (remainder 40 (remainder 206 40))))
      (if (= (remainder (remainder 206 40) (remainder 40 (remainder 206 40))) 0)                   ; 4 eval -- false
          (remainder 40 (remainder 206 40))
          (gcd (remainder (remainder 206 40) (remainder 40 (remainder 206 40)))
               (remainder (remainder 40 (remainder 206 40)) (remainder (remainder 206 40) (remainder 40 (remainder 206 40))))))
     (gcd (remainder (remainder 206 40) (remainder 40 (remainder 206 40)))
          (remainder (remainder 40 (remainder 206 40)) (remainder (remainder 206 40) (remainder 40 (remainder 206 40)))))
      (if (= (remainder (remainder 40 (remainder 206 40)) (remainder (remainder 206 40) (remainder 40 (remainder 206 40)))) 0) ; 7 eval -- true
          (remainder (remainder 206 40) (remainder 40 (remainder 206 40)))
          (gcd ... ...))
      (remainder (remainder 206 40) (remainder 40 (remainder 206 40))) ; 4 eval
      2
      ;; total 18 evals of remainder
      #+END_SRC

      applicative order:
      #+BEGIN_SRC scheme
      (define (gcd a b)
        (if (= b 0)
            a
            (gcd b (remainder a b))))
      (gcd 206 40)
      (if (= 40 0)
            206
            (gcd 40 (remainder 206 40)))
      (gcd 40 (remainder 206 40))           ; eval - 1
      (gcd 40 6)
      (if (= 6 0)
          40
          (gcd 6 (remainder 40 6)))
      (gcd 6 (remainder 40 6))              ; eval - 1
      (gcd 6 4)
      (if (= 4 0)
          6
          (gcd 4 (remainder 6 4)))
      (gcd 4 (remainder 6 4))              ; eval - 1
      (gcd 4 2)
      (if (= 2 0)
          4
          (gcd 2 (remainder 4 2)))
      (gcd 2 (remainder 4 2))              ; eval - 1
      (gcd 2 0)
      (if (= 0 0)
          2
          (gcd 0 (remainder 2 0)))
      2
      #+END_SRC
      
      4 evals of remainder.

*** 1.2.6 Example: Testing for Primality

    This section describes two methods for checking the primality of
    an integer n, one with the order of growth O(sqrt(n)) and a
    "probabilistic" algorithm with order of growth O(log n).  The
    exercises at the end of this section suggest programming projects
    based on these algorithms.

    Searching for divisors

    Since ancient times, mathematicians have been fascinated by
    problems concerning prime numbers, and many people have worked on
    the problem of determining ways to test if numbers are prime. One
    way to test if a number is prime is to find the number's
    divisors. The following program finds the smallest integral
    divisor (greater than 1) of a given number n. It does this in a
    straightforward way, by testing for divisibility by successive
    integers starting with 2.

    #+BEGIN_SRC scheme
    (define (smallest-divisor n)
      (find-divisor n 2))
    (define (find-divisor n test-divisor)
      (cond ((> (square test-divisor) n) n)
            ((divides? test-divisor n) test-divisor)
            (else (find-divisor n (+ test-divisor 1)))))
    (define (divides? a b)
      (= (remainder b a) 0))
    #+END_SRC

    We can test whether a number is prime as follows: n is prime if
    and only if n is its own smallest divisor.

    #+BEGIN_SRC scheme
    (define (prime? n)
      (= n (smallest-divisor n)))
    #+END_SRC

    The end test for find-divisor is based on the fact that if n is
    not prime it must have a divisor less than or equal to
    sqrt(n). This means that the algorithm need only test divisors
    between 1 and sqrt(n). Consequently, the number of steps required
    to identify n as prime will have order of growth O(sqrt(n)).

    The Fermat test

    The O(log n) primality test is based on a result from number
    theory known as Fermat's Little Theorem.

    Fermat's Little Theorem: If n is a prime number and a is any
    positive integer less than n, then a raised to th enth power is
    congruent to a modulo n.

    (Two numbers are said to be _congruent modulo n_ if they both have
    the same remainder when divided by n. The remainder of a number
    _a_ when divided by _n_ is also referred to as the _remainder of a
    modulo n_ or simply _a modulo n_.)

    If _n_ is not prime, then, in general, most of the numbers a < n
    will not satisfy the above relation. This leads to the following
    algorithm for testing primality: Given a number _n_, pick a
    random number a < n and compute the remainder of a^n modulo n. If
    the result is not equal to _a_, then _n_ is certainly not
    prime. If it is _a_, then chances are good that _n_ is prime. Now
    pick another random number _a_ and test it with the same
    method. If it also satisfies the equation, then we can be even
    more confident that _n_ is prime. By trying more and more values
    of _a_, we can increase our confidence in the result. This
    algorithm is known as the Fermat test.

    To implement the Fermat test, we need a procedure that computes
    the exponential of a number modulo another number:

    #+BEGIN_SRC scheme
    (define (expmod base exp m)
      (cond ((= exp 0) 1)
            ((even? exp)
             (remainder (square expmod base (/ exp 2) m) m))
            (else
             (remainder (* base (expmod base (- exp 1) m))
                        m))))
    #+END_SRC

    This is very similar to the fast-expt procedure of section
    1.2.4. It uses successive squaring, so that the number of steps
    grows logarithmically with the exponent.

    The Fermat test is preformed by choosing a random number a between
    1 and n - 1 inclusive and checking whether the remainder modulo n
    of the nth power of a is equal to a. The random number a is chosen
    using the procedure random, which we assume is included as a
    primitive in Scheme. random returns a nonnegative integer less
    than its integer input. Hence, to obtain a random number between 1
    and n - 1, we call random with an input of n - 1 and add 1 to the
    result:

    #+BEGIN_SRC scheme
    (define (fermat-test n)
      (define (try-it a)
        (= (expmod a n n) a))
      (try-it (+ 1 (random (- n 1)))))
    #+END_SRC

    The following procedure runs the test a given number of times, as
    specified by a parameter. Its value is true if the test succeeds
    every time, and false otherwise.

    #+BEGIN_SRC scheme
    (define (fast-prime? n times)
      (cond ((= times 0) true)
            ((fermat-test n) (fast-prime? n (- times 1)))
            (else false)))
    #+END_SRC

    Probabilistic methods

    The Fermat test differs in character from most familiar
    algorithms, in which one computes an answer that is guaranteed to
    be correct. Here, the answer obtained is only probably
    correct. More precisely, if n ever fails the Fermat test, we can
    be certain that _n_ is not prime. But the fact that _n_ passes the
    test, while an extremely strong indication, is still not a
    guarantee that _n_ is prime. What we would like to say is that for
    any number _n_, if we perform the test enough times and find that
    _n_ always passes the test, then the probability of error in our
    primality test can be made as small as we like.

    Unfortunately, this assertion is not quite correct. There do
    exist numbers that fool the Fermat test: numbers _n_ that are not
    prime and yet have the property that a^n is congruent to a modulo
    n for all integers a < n. Such numbers are extremely rare, so the
    Fermat test is quite reliable in practice. There are variations of
    the Fermat test that cannot be fooled. In these tests, as with the
    Fermat method, one tests the primality of an integer _n_ by
    choosing a random integer _a<n_ and checking some condition that
    depends upon n and a. (See exercise 1.28 for an example of such a
    test.) On the other hand, in contrast to the Fermat test, one can
    prove that, for any _n_, the condition does not hold for most of
    the integers a < n unless n is prime. Thus, if _n_ passes the test
    for some random choice of a, the chances are better than even that
    _n_ is prime. If _n_ passes the test for two random choices of a,
    the chances are better than 3 out of 4 that _n_ is prime. By
    running the test with more and more randomly chosen values of a we
    can make the probability of error as small as we like.

    The existence of tests for which one can prove that the chance of error
    becomes arbitrarily small has sparked interest in algorithms of
    this type, which have come to be known as _probabilistic
    algorithms_. There is a great deal of research activity in this
    area, and probabilistic algorithms have been fruitfully applied to
    many fields.

    - Exercise 1.21. Use the smallest-divisor procedure to find the
      smallest divisor of each of the following numbers: 199, 1999,
      1. 

      Answers: 199, 1999, 7.

    - Exercise 1.22. Most Lisp implementations include a primitive
      called runtime that returns an integer that specifies the amount
      of time the system has been running (measured, for example, in
      microseconds). The following timed-prime-test procedure, when
      called with an integer n, prints n and checks to see if n is
      prime. If n is prime, the procedure prints three asterisks
      followed by the amount of time used in performing the test.

      #+BEGIN_SRC scheme
      (define (timed-prime-test n)
        (newline)
        (display n)
        (start-prime-test n (runtime)))
      (define (start-prime-test n start-time)
        (if (prime? n)
            (report-prime (- (runtime) start-time))))
      (define (report-prime elapsed-time)
        (display " *** ")
        (display elapsed-time))
      #+END_SRC

      Using this procedure, write a procedure search-for-primes that
      checks the primality of consecutive odd integers in a specified
      range. Use your procedure to find the three smallest primes
      larger than 1000; larger than 10,000; larger than 100,000;
      larger than 1,000,000. Note the time needed to test each
      prime. Since the testing algorithm has order of growth
      O(sqrt(n)), you should expect that testing for primes around
      10,000 should take around sqrt(10) times as long as testing for
      primes around 1000. Do your timing data bear this out? How well
      do the data for 100,000 and 1,000,000 support the sqrt(n)
      prediction? Is your result compatible with the notion that
      programs on your machine run in time proportional to the number
      of steps required for the computation?

      #+BEGIN_SRC scheme
      (define (search-for-primes start-range end-range nprimes)
        (cond
          ((= nprimes 0) true)
          ((> start-range end-range) true)
          ((and (odd? start-range))
           (timed-prime-test start-range)
           (search-for-primes 
            (+ 2 start-range) 
            end-range 
            (if (prime? start-range) (- nprimes 1) nprimes)))
          ((even? start-range) (search-for-primes (+ 1 start-range) end-range nprimes))))
      #+END_SRC

    - Exercise 1.23. The smallest-divisor procedure shown at the start
      of this section does lots of needless testing: After it checks
      to see if the number is divisible by 2 there is no point in
      checking to see if it's divisible by any larger even
      numbers. This suggests that the values used for test-divisor
      should not be 2, 3, 4, 5, 6, ..., but rather 2, 3, 5, 7, 9,
      .... To implement this change, define a procedure next that
      returns 3 if its input is equal to 2 and otherwise returns its
      input plus 2. Modify the smallest-divisor procedure to use (next
      test-divisor) instead of (+ test-divisor 1). With
      timed-prime-test incporporating this modified version of
      smallest-divisor, run the test for each of the 12 primes found
      in exercise 1.22. Since this modification halves the number of
      test steps, you should expect it to run about twice as fast. Is
      this expectation confirmed? If not, what is the observed ratio
      of speeds of the two algorithms, and how do you explain the fact
      that it is different from 2?

      #+BEGIN_SRC scheme
      (define (next input)
        (if (= input 2) 3 (+ 2 input)))
      (define (smallest-divisor n)
        (smallest-divisor-iter n 2))
      (define (smallest-divisor-iter n divisor-test)
        (cond
          ((> (square divisor-test) n) n)
          ((divides? divisor-test n) divisor-test)
          (else (smallest-divisor-iter n (next divisor-test)))))
      #+END_SRC
      
      The ratio is ~1.65, close to 2 but not very
      close. This may be because although we are reducing the number
      of calls to smallest-divisor, we're also increasing the number
      of steps with an if statement. There used to be only a single
      addition there now there's a procedure call and a test before we
      get the next number.

    - 1.24. Modify the timed-prime-test procedure of exercise 1.22 to
      use fast-prime? (the Fermat method), and test each of the 12
      primes you found in that exercise. Since the Fermat test has
      O(log n) growth, how would you expect the time to test primes
      near 1,000,000 to compare with the time needed to test primes
      near 1000? Do your data bear this out? Can you explain any
      discrepancy you find?

      Since 1,000,000 is 1000 times 1000, you would expect that the
      test for primes near 1,000,000 to be log(1000) times as much or
      rougly 3 times as much.

      #+BEGIN_SRC scheme
      (define (timed-prime-test n)
        (newline)
        (display n)
        (start-prime-test n (runtime)))
      (define (start-prime-test n start-time)
        (if (fast-prime? n 3)
            (report-prime (- (runtime) start-time))))
      (define (report-prime elapsed-time)
        (display " *** ")
        (display elapsed-time))

      (define (search-for-primes start-range end-range nprimes)
        (cond
          ((= nprimes 0) true)
          ((> start-range end-range) true)
          ((and (odd? start-range))
           (timed-prime-test start-range)
           (search-for-primes 
            (+ 2 start-range) 
            end-range 
            (if (fast-prime? start-range 3) (- nprimes 1) nprimes)))
          ((even? start-range) (search-for-primes (+ 1 start-range) end-range nprimes))))
      #+END_SRC

      The time takesn is so small that even for really huge numbers
      the time doesn't register for my implementation of scheme.

    - Exercise 1.25 Alyssa P. Hacker complains that we went to a lot
      of extra work in writing expmod. After all, she says, since we
      already know how to compute exponentials, we could simply have
      written

      #+BEGIN_SRC scheme
      (define (expmod base exp m)
        (remainder (fast-expt base exp) m))
      #+END_SRC

      Is she correct? Would this procedure serve as well for our fast
      prime tester? Explain.
      
      It probably won't make a huge difference. If you put the
      remainder outside the fast-expt function you end up taking the
      remainder of an extremely large number which, depending on how
      the division algorithm works, could take longer than just taking
      a remainder every time we increase the power. However, if we
      assume that we only have to do a single subtraction per call of
      remainder in our original version of expmod we end up with
      log(exp) subtractions, whereas we actually have (base^exp)/m
      subtractions in the second case,  which is larger. Even if we
      have a constant multiplier per call to make it log(2*exp) the
      second outgrows the first.

    - Exercise 1.26. Louis Reasoner is having great difficulty doing
      exericse 1.24. His fast-prime? test seems to run more slowly
      than his prime? test. Louis calls his friend Eva Lu Ator over to
      help. When they examine Louis' code, they find that he has
      rewritten the expmod procedure to use an explicit multiplication
      rather than calling square:

      #+BEGIN_SRC scheme
      (define (expmod base exp m)
        (cond ((= exp 0) 1)
              ((even? exp)
               (remainder (* (expmod base (/ exp 2) m)
                             (expmod base (/ exp 2) m))))
              (else
               (remainder (* base (expmod base (- exp 1) m))
                          m))))
      #+END_SRC

      "I don't see what difference that could make" says Louis. "I
      do." says Eva. "By writing the procedure like that, you have
      transformed a O(log n) process into a O(n) process." Explain.

      Every time you hit the even? clause of the cond, you evaluate
      (expmod base (/ exp 2) m) twice. This causes you to have
      something resembling a binary tree with depth log(exp), every
      time you have an even number you spawn two calls. When you spawn
      y calls per function call you end up with a O(y^n) process where
      n is the depth of the function call tree. So originally the
      procedure took O(log exp) long, and now since you have a binary
      tree we end up with 2^(log exp), or just exp, so O(exp) or O(n),
      since the tree is log exp deep.

    - Exercise 1.27. Demonstrate that the Carmichael numbers listed in
      footnote 47 really do fool the Fermat test. That is, write a
      procedure that takes an integer _n_ and tests whether a^n is
      congruent to a modulo n for every a < n, and try your
      procedure on the given Carmichael numbers.

      561, 1105, 1729, 2465, 2821, 6601.

      #+BEGIN_SRC scheme
      (define (test-number n)
        (test-number-iter n 1))
      (define (test-number-iter n iter)
        (cond 
         ((= iter n) true)
         ((= (expmod iter n n) (remainder iter n)) 
          (test-number-iter n (+ 1 iter)))
         (else false)))
      #+END_SRC

      This procedure returns true for basic primes -- 3, 29, 7, 59,
      false for non-primes -- 14, 28, 57 -- and true for all the
      Carmichael numbers.

    - Exercise 1.28. One variant of the Fermat test that cannot be
      fooled is called the _Miller-Rabin test_. This starts from an
      alternate form of Fermat's Little Theorem, which states that if
      _n_ is a prime number and _a_ is any positive integer less than
      _n_, then _a_ raised to the (n-1)st power is congruent to 1
      modulo n. To test the primality of a number _n_ by the
      Miller-Rabin test, we pick a random number a<n and raise a to
      the (n-1)st power modulo n using the expmod procedure. However,
      whenever we perform the squaring step in expmod, we check to see
      if we have discovered a "nontrivial square root of 1 modulo n"
      that is, a number not equal to 1 or n - 1 whose square is equal
      to 1 modulo n. It is possible to prove that if such a nontrivial
      square root of 1 exists, then n is not prime. It is also
      possible to prove that if n is an odd number that is not prime
      then, for at least half the numbers a<n, computing a^n-1 in this
      way will reveal a nontrival square root of 1 modulo n. (This is
      why the Miller-Rabin test cannot be fooled.) Modify the expmod
      procedure to signal if it discovers a nontrivial square root of
      1 and use this to implement the Miller-Rabin test with a
      procedure analogous to fermat-test. Check your procedure by
      testing various known primes and non-primes. Hint: One
      convenient way to make expmod signal is to have it return 0.

      #+BEGIN_SRC scheme
      (define (miller-rabin-expmod base-orig exp-orig mod)
        (define (calc-expmod base exp)
         (cond
          ((= exp 0) 1)
          ((even? exp)
           (let ((squared (square base)))
            (cond
             ((and (= (remainder squared mod) 1)
                   (< base exp-orig)) 0)
             (else (remainder (calc-expmod squared (/ exp 2)) mod)))))
          ((odd? exp)
           (remainder (* base (calc-expmod base (- exp 1))) mod))))
        (calc-expmod base-orig exp-orig))
                   
      (define (miller-rabin-expmod base exp mod)
        (cond
          ((= exp 0) 1)
          ((even? exp)
           (let ((squared (square base)))
           (cond
            ((and (= (remainder squared mod) 1)
(not (= base exp))) 0)
            (else (remainder (miller-rabin-expmod squared (/ exp 2) mod) mod)))))
          ((odd? exp)
            (remainder (* base (miller-rabin-expmod base (- exp 1) mod)) mod))))
      (define (miller-rabin-test n test-n)
        (cond
         ((= test-n 0) true)
         (else (and (miller-rabin-test-one n)
                    (miller-rabin-test n (- test-n 1))))))
      (define (random-base n)
        (define random-nr (random n))
        (cond ((< random-nr 2) 2)
              ((> random-nr (- n 1)) (- n 1))
              (else random-nr)))
      (define (miller-rabin-test-one n)
        (= (miller-rabin-expmod (random-base n) (- n 1) n) 1))
      #+END_SRC


      
** 1.3 Formulating Abstractions with Higher-Order Procedures
   We have seen that procedures are, in effect, abstractions that
   describe compound operations on numbers independent of the
   particular numbers. For example, when we

   #+BEGIN_SRC scheme
   (define (cube x) (* x x x))
   #+END_SRC

   we are not talking about the cube of a particular number, but
   rather about a method for obtaining the cube of any number. Of
   course we could get along without ever defining this procedure by
   always writing expressions such as

   #+BEGIN_SRC scheme
   (* 3 3 3)
   (* x x x)
   (* y y y)
   #+END_SRC

   and never mentioning cube explicitly. This would place us at a
   serious disadvantage, forcing us to work always at the level of the
   particular operations that happen to be primitives in the language
   (multiplication, in this case) rather than in terms of higher-level
   operations. Our programs would be able to compute cubes but our
   language would lack the ability to express the concept of
   cubing. One of the things we should demand from a powerful
   programming language is the ability to build abstractions by
   assigning names to common patterns and then to work in terms of the
   abstractions directly. Procedures provide this ability. This is why
   all but the most primitive programming languages include mechanisms
   for defining procedures.

   Yet even in numerical processing we will be severely limited in our
   ability to create abstractions if we are restricted to procedures
   whose parameters must be numbers. Often the same programming
   pattern will be used with a number of different procedures. To
   express such patterns as concepts we will need to construct
   procedures that can accept procedures as arguments or return
   procedures as values. Procedures that manipulate procedures are
   called _higher-order procedures_. This section shows how
   higher-order procedures can serve as powerful abstraction
   mechanisms, vastly increasing the expressive power of our language.

*** 1.3.1 Procedures as Arguments

    Consider the following three procedures. The first computes the
    sum of the integers a through b:

    #+BEGIN_SRC scheme
    (define (sum-integers a b)
      (if (> a b)
          0
          (+ a (sum-integers (+ a 1) b))))
    #+END_SRC

    The second computes the sum of the cubes of the integers in the
    given range:

    #+BEGIN_SRC scheme
    (define (sum-cubes a b)
      (if (> a b)
          0 
          (+ (cube a) (sum-cubes (+ a 1) b))))
    #+END_SRC

    The third computes the sum of a sequence in terms of the series:

    1/(1 * 3) + 1/(5 * 7) + 1/(9 * 11) + ...

    which converges to pi/8 (very slowly):

    #+BEGIN_SRC scheme
    (define (pi-sum a b)
      (if (> a b)
          0
          (+ (/ 1.0 (* a (+ a 2))) (pi-sum (+ a 4) b))))
    #+END_SRC
    
    These three procedures clearly share a common underlying
    pattern. They are for the most part identical, differing only in
    the name of the procedure, the function of a used to compute the
    term to be added, and the function that provides the next value of
    a. We could generate each of the procedures by filling in slots in
    the same template:

    #+BEGIN_SRC scheme
    (define (<name> a b)
      (if (> a b)
          0
          (+ (<term> a)
             (<name> (<next> a) b))))
    #+END_SRC

    The presence of such a common pattern is strong evidence that
    there is a useful abstraction 
