* Chapter 1
** 1.1
*** 1.1.5 The Substitution Model for Procedure Application
    To evaluate a combination whose operator names a compound
    procedure, the interpreter follows much the same process as for
    combinations whose operators name primitive procedures, which we
    described in section 1.1.3. That is, the interpreter evaluates the
    elements of the combination and applies the procedure (which is
    the value of the operator of the combination) to the arguments
    (which are the values of the operands of the combination). 

    We can assume that the mechanism for applying primitive procedures
    to arguments is built into the interpreter. For compound
    procedures, the application process is as follows:

    - To apply a compound procedure to arguments, evaluate the body of
      the procedure with each formal parameter replaced by the
      corresponding argument.

    To illustrate the process, let's evaluate the combination

    #+BEGIN_EXAMPLE
    (f 5)
    #+END_EXAMPLE

    where f is the procedure defined in section 1.1.4. We begin by
    retrieving the body of f:

    #+BEGIN_EXAMPLE
    (sum-of-squares (+ a 1) (* a 2))
    #+END_EXAMPLE

    Then we replace the formal parameter a by the argument 5:

    #+BEGIN_EXAMPLE
    (sum-of-squares (+ 5 1) (* 5 2))
    #+END_EXAMPLE

    Thus the problem reduces to the evaluation of a combination with
    two operands and an operator sum-of-squares. Evaluating this
    combination involves three subproblems. We must evaluate the
    operator to get the procedure to be applied, and we must evaluate
    the operands to get the arguments. Now (+ 5 1) produces 6 and (*
    5 2) produces 10, so we must apply the sum-of-squares procedure to
    6 and 10. These values are substituted for the formal parameters x
    and y in the body of sum-of-squares, reducing the expression to

    #+BEGIN_EXAMPLE
    (+ (square 6) (square 10))
    #+END_EXAMPLE

    If we use the definition of square, this reduces to

    #+BEGIN_EXAMPLE
    (+ (* 6 6) (* 10 10))
    #+END_EXAMPLE

    which reduces by multiplication to

    #+BEGIN_EXAMPLE
    (+ 36 100)
    #+END_EXAMPLE

    and finally to

    136

    The process we have just described is called the _substitution
    model_ for procedure application. It can be taken as a model that
    deterimines the 'meaning' of procedure application, insofar as the
    procedures in this chapter are concerned. However, there are two
    points that should be stressed:

    - The purpose of the substitution is to help us think about
      procedure application, not to provide a description of how the
      interpreter really works. Typical interpreters do not evaluate
      procedure applications by manipulating the text of a procedure
      to substitute values for the formal parameters. In practice, the
      "substitution" is accomplished by using a local environment for
      the formal parameters. We will discuss this more fully in
      chapters 3 and 4 when we examine the implementation of an
      interpreter in detail.
    - Over the course of this book, we will present a sequence of
      increasingly elaborate models of how interpreters work,
      culminating with a complete implementation of an interpreter and
      compiler in chapter 5. The substitution model is only the first
      of these models -- a way to get started thinking formally about
      the evaluation process. In general when modeling phenomena in
      science and engineering, we begin with simplified, incomplete
      models. As we examine things in creater detail, these simple
      models become inadequate and must be replaced by more refined
      models. The substitution model is no exception. In particular,
      when we address in chapter 3 the use of procedures with "mutable
      data", we will see that the substitution model breaks down and
      must be replaced by a more complicated model of procedure
      application.

**** Applicative order versus normal order
     According to the description of evaluation given in section
     1.1.3, the interpreter first evaluates the operator and operands
     and then applies the resulting procedure to the resulting
     arguments. This is not the only way to perform evaluation. An
     alternative evaluation model would not evaluate the operands
     until their values were needed. Instead it would first substitute
     operand expressions for parameters until it obtained an
     expression involving only primitive operators, and would then
     perform the evaluation. If we used this method, the evaluation of

     #+BEGIN_EXAMPLE
     (f 5)
     #+END_EXAMPLE

     would then proceed according to the sequence of expansions 

     (sum-of-squares (+ 5 1) (* 5 2))
     (+  (square (+ 5 1))    (square (* 5 2)) )
     (+  (* (+ 5 1) (+ 5 1)) (* (* 5 2) (* 5 2)))
     
     followed by the reductions
     
     (+   (* 6 6)            (* 10 10))
     (+    36                100)
     136

     This gives the same answer as our previous evaluation model, but
     the process is different. In particular, the evaluations of (+
     5 1) and (* 5 2) are each performed twice here, corresponding to
     the reduction of the expression

     (* x x)

     with x replaced respectively by (+ 5 1) and (* 5 2).

     This alternative "fully expand and then reduce" evaluation method
     is known as _normal-order evaluation_, in contrast to the
     "evaluate the arguments and then apply" method that the
     interpreter actually uses, which is called _applicative-order
     evaluation_. It can be show that, for procedure applications that
     can be modeled using substitution (including all the procedures
     in the first two chapters of this book) and yield legitimate
     values, normal-order and applicative-order evaluation produce the
     same value. 

     Lisp uses applicative-order evaluation, partly because of the
     additional efficiency obtained from avoiding multiple evaluations
     of expressions such as those illustrated with (+ 5 1) and (* 5 2)
     above and, more significantly, because normal-order evaluation
     becomes much more complicated to deal with when we leave the
     realm of procedures that can be modeled by substitution. On the
     other hand, normal-order evaluation can be extremely valuable
     tool, and we will investigate some of its implications in chapter
     3 and 4.

*** 1.1.6 Conditional expressions and Predicates
    The expressive power of the class of procedures that we can define
    at this point is very limited, because we have no way to make
    tests and to perform different operations depending on the result
    of a test. For instance, we cannot define a procedure that
    computes the absolute value of a number by testing whether the
    number is positive, negative, or zero and taking different actions
    in the different cases according to the rule:

    #+BEGIN_EXAMPLE
           -
           | x if x > 0
    |x| =  | 0 if x = 0
           |-x if x < 0
           -
    #+END_EXAMPLE
    
    This construct is called a _case analysis_, and there is a special
    form in Lisp for notating such a case analysis. It is called cond
    (which stands for conditional), and it is used as follows:
    
    #+BEGIN_EXAMPLE
    (define (abs x)
      (cond  ((> x 0) x)
             ((= x 0) 0)
             ((< x 0) (- x))))
    #+END_EXAMPLE

    The general form of a conditional expression is

    (cond (<p1> <e1>)
          (<p2> <e2>)
          .
          .
          .
          (<pn> <en>))

    consisting of the symbol cond followed by parenthesized pairs of
    expressions (<p> <e>) called _clauses_. The first expression in
    each pair is a _predicate_ -- that is, an expression whose value
    is interpreted as either true or false.

    Conditional expressions are evaluated as follows. The predicate
    <p1> is evaluated first. If its value is false, then <p2> is
    evaluated. If <p2>'s value is also false, then <p3> is
    evaluated. This process continues until a predicate is found whose
    value is true, in which case the interpreter returns the value of
    the corresponding _consequent expression_ <e> of the clause as the
    value of the conditional expression. If none of the <p>'s is found
    to be true, the value of the cond is undefined.

    The word _predicate_ is used for procedures that return true or
    false, as well as for expressions that evaluate to true or
    false. The absolute-value procedure abs makes use of the primitive
    predicates >, <, and =. These take two numbers as arguments and
    test whether the first number is, respectively, greater than, less
    than, or equal to the second number, returning true or false
    accordingly.

    Another way to write the absolute-value procedure is

    #+BEGIN_EXAMPLE
    (define (abs x)
      (if (< x 0)
          (- x)
          x)
    #+END_EXAMPLE

    


    

    
     
*** 1.1.7 Example: Square roots by Newton's Method
    Procedures, as introduced above, are much like ordinary
    mathematical functions. They specify a value that is determined by
    one or more parameters. But there is an important difference
    between mathematical functions and computer procedures. Procedures
    must be effective.
    
    As a case in point, consider the problem of computing square
    roots. We can define the square-root function as

    #+BEGIN_EXAMPLE
    âˆšx = the y such that y >= 0 and y^2 = x
    #+END_EXAMPLE

    This describes a perfectly legitmate mathematical function. We
    could use it to recognize whether one number is the square root of
    another, or to derive facts about square roots in general. On the
    other hand, the definition does not describe a procedure. Indeed,
    it tells us almost nothing about _how_ to actually find the square
    root of a given number. It will not help matters to rephrase this
    definition in pseudo lisp:

    #+BEGIN_SRC scheme
    (define (sqrt x)
      (the y (and (>= y 0)
                  (= (square y) x))))
    #+END_SRC

    This only begs the question.

    The contrast between function and procedure is a reflection of the
    general distinction between describing properties of things and
    describing how to do things, or, as it is sometimes referred to,
    the distinction between declarative knowledge and imperative
    knowledge. In mathematics we are usually concerned with
    declarative (what is) descriptions, whereas in computer science we
    are usually concerned with imperative (how to) descriptions.

    How does one compute square roots? The most common way is to use
    Newton's method of successive approximations, which says that
    whenever we have a guess _y_ for the value of the square root of a
    number _x_, we can perform a simple manimpulation to get a better
    guess (one closer to the actual square root) by averaging y with
    x, _x/y_. For example, we can compute the square root of 2 as
    follows. Suppose our initial guess is 1:

    Guess      Quotient                      Average
    1          (2/1) = 2                     ((2 + 1)/2) = 1.5
    1.5        (2/1.5) = 1.333               ((1.333 + 1.5)/2) = 1.4167
    1.4167     (2/1.4167) = 1.4118           ((1.4167 + 1.4118)/2) = 1.4142
    1.4142     ...                           ...

    Continuing this process, we obtain better and better
    approximations to the square root.

    Now let's formalize the process in terms of procedures. We start
    with a value for the radicand (the number whose square root we are
    trying to compute) and a value for the guess. If the guess is good
    enough for our purposes, we are done; if not, we must repeat the
    process with an improved guess. We write this basic strategy as a
    procedure:

    #+BEGIN_SRC scheme
    (define (sqrt-iter guess x)
      (if (good-enough? guess x)
          guess
          (squrt-iter (improve guess x)
                      x)))
    #+END_SRC

    A guess is improved by averaging it with the quotient of the
    radicand and the old guess:

    #+BEGIN_SRC scheme
    (define (improve guess x)
      (average guess (/ x guess)))
    #+END_SRC

    where

    #+BEGIN_SRC scheme
    (define (average x y)
      (/ (+ x y) 2))
    #+END_SRC

    We also have to say what we mean by "good enough." The following
    will do for illustration, but it is not really a very good
    test. The idea is to improve the answer until it is close enough
    so that its square differs from the radicand by less than a
    predetermined tolerance (here 0.001):

    #+BEGIN_SRC scheme 
    (define (good-enough? guess x)
      (< abs (- (square guess) x) 0.001))
    #+END_SRC

    Finally, we need a way to get started. For instance, we can always
    guess that the square root of any number is 1:

    #+BEGIN_SRC scheme
    (define (sqrt x)
      (sqrt-iter 1.0 x))
    #+END_SRC

    If we type these definitions to the interpreter we can use sqrt
    just as we can use any procedure:

    #+BEGIN_SRC scheme
    (sqrt 9)
    3.00009
    (sqrt (+ 100 37))
    11.7046
    #+END_SRC

    The sqrt program also illustrates that the simple procedural
    language we have introduced so far is sufficient for writing any
    purely numerical program that one we could write in, say, C or
    Pascal. This might seem surprising, since we have not included in
    our language any iterative (looping) constructs that direct the
    computer to do something over and over again. Sqrt-iter, on the
    other hand, demonstrates, how iteration can be accomplished using
    no special construct other than the ordinary ability to call a
    procedure.

    - Exercise 1.6 Alyssa P. Hacker doesn't see why if needs to be
      provided as a special form. "Why can't I just define it as an
      ordinary procedure in terms of cond?" she asks. Alyssa's friend
      Eva Lu Ator claims this can indeed be done, and she defines a
      new version of if:

      #+BEGIN_SRC scheme
      (define (new-if predicate then-clause else-clause)
        (cond (predicate then-clause)
              (else else-clause)))
      #+END_SRC

      Eva demonstrates the program for Alyssa:

      #+BEGIN_SRC scheme
      (new-if (= 2 3) 0 5)
      5
      (new-if (= 1 1) 0 5)
      0 
      #+END_SRC

      Delighted, Alyssa uses new-if to rewrite the square-root
      program:

      #+BEGIN_SRC scheme
      (define (sqrt-iter guess x)
        (new-if (good-enough? guess x)
                guess
                (sqrt-iter (improve guess x)
                           x)))
      #+END_SRC

      What happens when Alyssa attempts to use this to compute square
      roots? Explain.

      What happens is that evaluation never stops. Because we are
      using applicative-order evaluation, that is, evaluating all
      arguments before passing them to the function, we recursively
      evaluate sqrt-iter and it never terminates because we always
      evaluate it before going into the new-if function.

    - Exercise 1.7 The good-enough? test used in computing square
      roots will not be very effective for finding the square roots of
      very small numbers. Also, in real computers, arithmetic
      operations are almost always performed with limited
      precision. This makes our test inadequate for very large
      numbers. Explain these statements, with examples showing how the
      test fails for small and large numbers. An alternative strategy
      for implementing good-enough? is to watch how guess changes from
      one iteration to the next and to stop when the change is a very
      small fraction of the guess. Design a square-root procedure that
      uses this kind of end test. Does this work better for small and
      large numbers?

      For the case of a very small number, because we have defined
      good-enough? with a number that isn't, really, all that terribly
      small, if we try to find the square root of a number even
      smaller, we will get an incorrect result.

      #+BEGIN_EXAMPLE
      > (sqrt 1e-6)
      3.126e-2
      > (* (sqrt 1e-6) (sqrt 1e-6))
      9.7722e-4
      #+END_EXAMPLE

      The answer 9.7722e-4 is way off. We can divide the answer by the
      radicand and get 977.22, which is very far from being 1, which
      it should be.

      #+BEGIN_EXAMPLE
      > (/ 9.7722e-4 1e-6)
      977.22
      #+END_EXAMPLE

      The problem is that if we subtract the square of a small but
      still wildly off guess from 1e-6, the answer will be less than
      .001. 

      For extremely large numbers there is a different problem. On
      this machine typing

      #+BEGIN_EXAMPLE
      > (sqrt 1e13)
      #+END_EXAMPLE
      
      never ends and results in an infinite loop. The problem is
      actually that our value is too small in this case rather than
      too large, basically. That is, when we are representing
      extremely large numbers we can only have so much precision in
      the 'small end' of the number, especially the fractional
      part. So, you can get as close as you're going to get to the
      approximation, but still be more than .001 away from the actual
      number.

      So if we run the improve procedure for 1e13 manually, and
      continue using the results (e.g. the improved guess) and feed
      them back into improve, eventually we get exactly the same
      number out of improve over and over again. This is

      3162277.6601683795 --

      #+BEGIN_EXAMPLE
      > (improve 3162277.6601683795 1e13)
      3162277.6601683795
      #+END_EXAMPLE

      But if we square that number:

      #+BEGIN_EXAMPLE
      > (square 3162277.6601683795)
      10000000000000.002
      #+END_EXAMPLE
      
      SInce the threshold is .001, the value of the good-enough?
      procedure is false, subtracting the two results in exactly .001
      which is not less than .001. Since improve will return the exact
      same result again, we will continue to evaluate the sqrt-iter
      procedure indefinitely.

      If we dive in deeper, we can look at what the improve procedure
      does:

      #+BEGIN_EXAMPLE
      > (average 3162277.6601683795 (/ 1e13 3162277.6601683795))
       3162277.6601683795
      #+END_EXAMPLE

      This must be the fault, it should be converging.

      (/ 1e13 3162277.6601683795)

      3162277.660168379

      The average of 3162277.660168379 and 3162277.6601683795 we can
      tell by manual inspection should not be the value
      3162277.6601683795, which is the exact same as the first
      parameter. Averaging two unequal numbers should never be exactly
      equal to one of the numbers. This is therefore an incorrect
      result from our procedure. We can break it down further by
      looking at the value of average:

      #+BEGIN_EXAMPLE
      > (+ 3162277.660168379 3162277.6601683795)
      6324555.320336759
      #+END_EXAMPLE

      If we check this by hand-evaluation we realize this isn't the
      correct value. When we add by hand we get:

      6324555.3203367585

      The computer, however, rounds this up by .5, since presumably it
      can't represent that many fractional points as well as a larger
      number, introducing the error! The rounded up result happens to
      be exactly the same as the result of doubling the first
      parameter, 3162277.660168379. So, we will continually get the
      same answer from improve, which will always be just a little
      off. So there needs to be a way to have a 'relative'
      measurement. We could do this by dividing the square of the
      guess with the number itself, which should give us a relative
      value.

      #+BEGIN_SRC scheme
      (define (good-enough? guess x)
        (< (abs (- 1 (/ x (square guess)))) .001))
      #+END_SRC

      running the same procedures that before were inaccurate or bad
      results in actual termination of algorithms at the expense (on
      the large number side) of less accuracy.  I decreased the
      threshold to .000001 right away.

      #+BEGIN_SRC scheme
      1 ]=> (square (sqrt 1e-16))
      ;Value: 1.0000000018865009e-16
      1 ]=> (square (sqrt 1e13))
      ;Value: 10000000024299.582
      #+END_SRC

      If we decrease threshold to 1e-12:

      #+BEGIN_SRC scheme
      1 ]=>  (square (sqrt 1e13))
      ;Value: 10000000000000.002
      1 ]=> (square (sqrt 1e-16))
      ;Value: 1.0000000000000001e-16
      #+END_SRC

      This is very good.

    - Exercise 1.8 Newton's method for cube roots is based on the fact
      that if _y_ is an approximation to the cube root of _x_, then a
      better approximation is given by the value:

      (x / y^2 + 2y) / 3

      Use this formula to implement a cube-root procedure analogous to
      the square-root procedure. (In section 1.3.4 we will see how to
      implement Newton's method in general as an abstraction of these
      square-root and cube-root procedures).
      
      #+BEGIN_SRC scheme
      (define THRESHOLD 1e-10)
      (define (cube-root-iter guess x)
        (if (good-enough-cube-root? guess x)
             guess
            (cube-root-iter (improve-cube-root-guess guess x) x)))
      (define (good-enough-cube-root? guess x)
        (< (abs (- 1 (/ x (* guess guess guess)))) THRESHOLD))
      (define (improve-cube-root-guess guess x)
        (/ (+ (/ x (square guess)) (* 2 guess)) 3))
      #+END_SRC
    
      #+BEGIN_SRC
      1 ]=> (cube-root-iter 1.0 27)
      
      ;Value: 3.0000000000000977
      
      1 ]=> (cube-root-iter 1.0 27.0)
      
      ;Value: 3.0000000000000977
      
      1 ]=> (* 4 4 4) 
      
      ;Value: 64
      
      1 ]=> (cube-root-iter 1.0 64)
      
      ;Value: 4.000000000076121
      
      1 ]=> (cube-root-iter 1.0 1e15)
      
      ;Value: 100000.0000002152
      
      1 ]=> (*  100000.0000002152  100000.0000002152  100000.0000002152)
      
      ;Value: 1000000000006455.9
      
      1 ]=>
      #+END_SRC

*** 1.1.8 Procedures as Black-Box Abstractions
    Sqrt is our first example of a process defined by a set of
    mutually defined procedures. Notice that the definition of
    sqrt-iter is _recursive_; that is, the procedure is defined in
    terms of itself. The idea of being able to define a procedure in
    terms of itself may be disturbing; it may seem unclear how such a
    "circular" definition could make sense at all, much less specify a
    well-defined process to be carried out by a computer. This will be
    addressed more carefully in section 1.2. But first let's consider
    some other important points illustrated by the sqrt example.

    Observe that the problem of computing square roots breaks up
    naturally into a number of subproblems: how to tell whether a
    guess is good enough, how to improve a guess, and so on. Each of
    these tasks is accomplished by a separate procedure. The entire
    sqrt program can be viewed as a cluster of procedures (shown in
    figure 1.2) that mirrors the decomposition of the problem into
    subproblems.

    The importance of this decomposition strategy is not simply that
    one is dividing the program into parts. After all, we could take
    any large program and divide it into parts -- the first ten lines,
    the next ten lines, the next ten lines, and so on. Rather, it is
    crucial that each procedure accomplishes an identifiable task that
    can be used as a module in defining other procedures. For
    example, when we define the good-enough? procedure in terms of
    square, we are able to regard the square procedure as a "black
    box". We are not at that moment concerned with _how_ the procedure
    computes its result, only with the fact that it computes the
    square. The details of how the square is computed can be
    suppressed, to be considered at a later time. Indeed, as far as
    the good-enough? procedure is concerned, square is not quite a
    procedure but rather an abstraction of a procedure, a so-called
    _procedural abstraction_. At this level of abstraction, any
    procedure that computes the square is equally good.

    Thus, considering only the values they return, the following two
    procedures for squaring a number should be indistinguishable. Each
    takes a numerical argument and produces the square of that number
    as the value:

    #+BEGIN_SRC scheme
    (define (square x) (* x x))
    (define (square x)
      (exp (double (log x))))
    (define (double x) (+ x x))
    #+END_SRC

    So a procedure definition should be able to suppress detail. The
    users of the procedure may not have written the procedure
    themselves, but may have obtained it from another programmer as a
    black box. A user should not need to know how the procedure is
    implemented in order to use it.

***** Local names
      One detail of a procedure's implementation that should not
      matter to a user of the procedure is the implementer's choice of
      names for the procedure's formal parameters. Thus, the following
      procedures should not be distinguishable:

      #+BEGIN_SRC scheme
      (define (square x) (* x x))
      (define (square y) (* y y))
      #+END_SRC

      This principle -- that the meaning of a procedure should be
      independent of the parameter names used by its author -- seems
      on the surface to be self-evident, but its consequences are
      profound. The simplest consequence is that the parameter names
      of a procedure must be local to the body of the procedure. For
      example, we used square in the definition of good-enough? in our
      square-root procedure:

      #+BEGIN_SRC scheme
      (define (good-enough? guess x)
        (< (abs (- (square guess) x)) .001))
      #+END_SRC

      The intention of the author of good-enough? is to determine if
      the square of the first argument is within a given tolerance of
      the second argument. We see that the author of good-enough? used
      the name guess to refer to the first argument and x to refer to
      the second argument. The argument of square is guess. If the
      author of square used x (as above) to refer to that argument, we
      see that the x in good-enough? must be a different x than the
      one in square. Running the procedure square must not affect the
      value of x that is used by good-enough?, because that value of x
      may be needed by good-enough? after square is done computing.

      If the parameters were not local to the bodies of their
      respective procedures, then the parameter x in square could be
      confused with the parameter x in good-enough?, and the behavior
      of good-enough? would depend upon which version of square we
      used. Thus, square would not be the black box we desired.

      A formal parameter of a procedure has a very special role in the
      procedure definition, in that it doesn't matter what name the
      formal parameter has. Such a name is called a _bound variable_,
      and we say that the procedure definition _binds_ its formal
      parameters. The meaning of a procedure definition is unchanged
      if a bound variable is consistently renamed throughout the
      definition. If a variable is not bound, we say that it is
      _free_. The set of expressions for which a binding defines a
      name is called the _scope_ of that name. In a procedure
      definition, the bound variables declared as the formal
      parameters of the procedure have the body of the procedure as
      their scope.

      In the definition of good-enough? above, guess and x are bound
      variables but <, -, abs, and square are free. The meaning of
      good-enough? should be independent of the names we choose for
      guess and x so long as they are distinct and different from <,
      -, abs, and square. (If we renamed guess to abs we would have
      introduced a bug by _capturing_ the variable abs. It would have
      changed from free to bound.) The meaning of good-enough? is not
      independent of the names of its free variables, however. it
      surely depends upon the fact (external to this definition) that
      the symbol abs names a procedure for computing the absolute
      value of a number. Goodn-enough? will compute a different
      function if we substitute cos for abs in its definition.

      
****** Internal definitions and block structure
       We have one kind of name isolation available to us so far: The
       formal parameters of a procedure are local to the body of the
       procedure. The square-root program illustrates another way in
       which we would like to control the use of names. The existing
       program consists of separate procedures:

       #+BEGIN_SRC scheme
       (define (sqrt x)
         (sqrt-iter 1.0 x))
       (define (sqrt-iter guess x)
         (if (good-enough? guess x)
             guess
             (sqrt-iter (improve guess x) x)))
       (define (good-enough? guess x)
         (< (abs (- (square guess) x)) .001))
       (define (improve guess x)
         (average guess (/ x guess)))  
       #+END_SRC

       The problem with this program is that the only procedure that
       is important to users of sqrt is sqrt. The other procedures
       (sqrt-iter, good-enough?, and improve) only clutter up their
       minds. They may not define any other procedure called
       good-enough? as part of another program to work together with
       the square-root program, because sqrt needs it. The problem is
       especially severe in the construction of large systems by many
       separate programmers. For example, in the construction of a
       large library of numerical procedures, many numerical functions
       are computed as successive approximations and thus might have
       procedures named good-enough? and improve as auxiliary
       procedures. We would like to localize the subprocedures, hiding
       them inside sqrt so that sqrt could coexist with other
       successive approximations, each having its own private
       good-enough? procedure. To make this possible, we allow a
       procedure to have internal definitions that are local to that
       procedure. For example, in the square-root problem we can write

       #+BEGIN_SRC scheme
       (define (sqrt x)
         (define (good-enough? guess x)
           (< (abs (- (square guess) x)) 0.001))
         (define (improve guess x)
           (average guess (/ x guess)))
         (define (sqrt-iter guess x)
           (if (good-enough? guess x)
               guess
               (sqrt-iter (improve guess x) x)))
        (sqrt-iter 1.0 x))
       #+END_SRC

       Such nesting of definitions, called _block structure_, is
       basically the right solution to the simplest name-packaging
       problem. But there is a better idea lurking here. In addition
       to internalizing the definitiosn of the auxiliary procedures,
       we can simplify them. Since x is bound in the definition of
       sqrt, the procedures good-enough?, improve, and sqrt-iter,
       which are defined internally to sqrt, are in the scope of
       x. Thus it is not necessary to pass x explicitly to each of
       these procedures. Instead, we allow x to be a free variable in
       the internal definitions, as shown below. Then x gets its value
       from the argument with which the enclosing procedure sqrt is
       called. This discipline is called _lexical scoping_.

       #+BEGIN_SRC scheme
       (define (sqrt x)
         (define (good-enough? guess)
           (< (abs (- (square guess) x)) 0.001))
         (define (improve guess)
           (average guess (/ x guess)))
         (define (sqrt-iter guess)
           (if (good-enough? guess)
               guess
               (sqrt-iter (improve guess))))
         (sqrt-iter 1.0))
       #+END_SRC

       We will use block structure extensively to help us break up
       large programs into tractable pieces. The idea of block
       structure originated with programming language Algol 60. It
       appears in most advanced programming languages and is an
       important tool for helping to organize the construction of
       large programs.

** 1.2 Procedures and the Processes They Generate
   We have now considered the elements of programming: We have used
   primitive arithmetic operations, we have combined these operations,
   and we have abstracted these composite operations by defining them
   as compound procedures. But that is not enough to enable us to say
   that we know hwo to program. Our situation is analogous to that of
   someone who has learned the rules for how the pieces move in chess
   but knows nothing of typical openings, tactics, or strategy. Like
   the novice chess player, we don't yet know the common patterns of
   usage in the domain. We lack the knowledge of which moves are worth
   making (which procedures are worth defining). We lack the
   experience to predict the consequences of making a move (executing
   a procedure). 

   The ability to visualize the consequences of the actions under
   consideration is crucial to becoming an expert programmer, just as
   it is in any synthetic, creative activity. In becoming an expert
   photographer, for example, one must learn how to look at a scene
   and know how dark each region will appear on a print for each
   possible choice of exposure and development conditions. Only then
   can one reason backward, planning framing, lighting, exposure, and
   development to taken by a process and where we control the process
   by means of a program. To become experts, we must learn to
   visualize the processes generated by various types of
   procedures. Only after we have developed such a skill can we learn
   to reliably construct programs that exhibit the desired behavior.

   A procedure is a pattern for the _local evolution_ of a
   computational process. It specifies how each stage of the process
   is built upon the previous stage. We would like to be able to make
   statements about the overall, or _global_, behavior of a process
   whose local evolution has been specified by a procedure. This is
   very difficult to do in general, but we can at least try to
   describe some typical patterns of process evolution.

   In this section, we will examine some common "shapes" for the
   processes generated by simple procedures. We will also investigate
   the rates at which these processes consume the important
   computation resources of time and space. The procedures we will
   consider are very simple. Their role is like that played by test
   patterns in photography: as oversimplified prototypical patterns,
   rather than practical examples in their own right.

*** 1.2.1. Linear recursion and Iteration
    We begin by considering the factorial function, defined by

    n! = n * (n - 1) * (n - 2) ... 3 * 2 * 1

    There are many ways to compute factorials. One way is to make use
    of the observation that n! is equal to n times (n - 1)! for any
    positive integer n:

    n! = n * [(n - 1) * (n - 2) ... 3 * 2 * 1] = n * (n - 1)!

    Thus, we can compute n! by computing (n - 1)! and multiplying the
    result by n. If we add the stipulation that 1! is equal to 1, this
    observation translates directly into a procedure:

    #+BEGIN_SRC scheme
    (define (factorial n)
      (if (= n 1)
          1
          (* n (factorial (- n 1)))))
    #+END_SRC

    We can use the substitution model of section 1.1.5 to watch this
    procedure in action computing 6!, as shown in figure 1.3

    #+BEGIN_SRC scheme
    (factorial 6)
    (* 6 (factorial 5))
    (* 6 (* 5 (factorial 4)))
    (* 6 (* 5 (* 4 (factorial 3))))
    ...
    (* 6 (* 4 (* 3 (* 2 1))))
    (* 6 (* 4 (* 3 2)))
    ...
    720
    #+END_SRC
    
    Now let's take a different perspective on computing factorials. We
    could describe the rule for computing n! by specifying that we
    first multiply 1 by 2, then multiply the result by 3, then by 4,
    and so on until we reach n. More formally, we maintain a running
    product, together with a counter that counts from 1 up to n. We
    can describe the computation by saying that the counter and the
    product simultaneously change from one step to the next according
    to the rule

    product = counter * product
    counter = counter + 1

    and stipulating that n! is the value of the product when the
    counter exceeds n.

    Once again, we can recast our description as a procedure for
    computing factorials:

    #+BEGIN_SRC scheme
    (define (factorial n)
      (fact-iter 1 1 n))
    (define (fact-iter product counter max-count)
      (if (> counter max-count)
          product
          (fact-iter (* counter product)
                     (+ counter 1)
                     max-count)))
    #+END_SRC

    As before, we can use the substitution model to visualize the
    process of computing 6!, as shown in figure 1.4.

    #+BEGIN_SRC scheme
    (factorial 6)
    (fact-iter 1 1 6)
    (fact-iter 1 2 6)
    (fact-iter 2 3 6)
    (fact-iter 6 4 6)
    (fact-iter 24 5 6)
    (fact-iter 120 6 6)
    (fact-iter 720 7 6)
    #+END_SRC

    Once again, we can recast our description as a procedure for
    computing factorials:

    #+BEGIN_SRC
    (define (factorial n)
      (fact-iter 1 1 n))
    (define (fact-iter product counter max-count)
      (if (> counter max-count)
          product
          (fact-iter (* counter product)
                     (+ counter 1)
                     max-count)))
    #+END_SRC

    Compare the two processes. From one point of view, they hardly
    seem different at all. Both compute the same mathematical function
    on the same domain, and each requires a number of steps
    proportional to n to compute n!. Indeed, both processes even carry
    out the same sequence of multiplications, obtaining the same
    sequence of partial products. On the other hand, when we consider
    the "shapes" of the two processes, we find that they evolve quite
    differently.

    Consider the first process. The substitution model reveals a shape
    of expansion followed by contraction, indicated by the arrow in
    figure 1.3. The expansion occurs as the process builds up a chain
    of _deferred operations_ (in this case, a chain of
    multiplications). The contraction occurs as the operations are
    actually performed. This type of process, characterized by a chain
    of deferred operations, is called a _recursive process_. Carrying
    out this process requires that the interpreter keep track of the
    operations to be performed later on. In the computation of n!, the
    length of the chain of deferred multiplications, and hence the
    amount of information needed to keep track of it, grows linearly
    with _n_ (is proportional to _n_), just like the number of
    steps. Such a process is called a _linear recursive process_.

    By contrast, the second process does not grow or shrink. At each
    step, all we need to keep track of, for any _n_, are the current
    values of the variables product, counter, and max-count. We call
    this an _iterative process_. In general, an iterative process is
    one whose state can be summarized by a fixed number of _state
    variables_, together with a fixed rule that describes how the
    state variables should be updated as the process moves from state
    to state and an (optional) end test that specifies conditions
    under which the process should terminate. In computing n!, the
    number of steps required grows linearly with _n_. Such a process
    is called a _linear iterative process_.

    The contrast between the two processes can be seen in another
    way. In the iterative case, the program variables provide a
    complete description of the state of the process at any point. If
    we stopped the computation between steps, all we would need to do
    to resume the computation is to supply the interpreter with the
    values of the three program variables. Not so with the recursive
    process. In this case there is some additional "hidden"
    information, maintained by the interpreter and not contained in
    the program variables, which indicates "where the process is" in
    negotiating the chain of deferred operations. The longer the
    chain, the more information must be maintained.

    In constrasting iteration and recursion, we must be careful not to
    confuse the notion of a recursive _process_ with the notion of a
    recursive _procedure_. When we describe a procedure as recursive,
    we are referring to the syntactic fact that the procedure
    definition refers (either directly or indirectly) to the procedure
    itself. But when we describe a process as following a pattern that
    is, say, linearly recursive, we are speaking about how the process
    evolves, not about the syntax of how a procedure is written. It
    may seem disturbing that we refer to a recursive procedure such as
    fact-iter as generating an iterative process. However, the process
    really is terative: Its state is captured completely by its three
    state variables, and an interpreter need keep track of only three
    variables in order to execute the process.

    One reason that the distinction between processs and procedure may
    be confusing is that most implementations of common languages
    (including Ada, Pascal, and C) are designed in such a way that the
    interpretation of any recursive procedure consumes an amount of
    memory that grows with the number of procedure calls, even when
    the process described is, in principle, iterative. as a
    consequence, these languages can describe iterative processes only
    by resorting to special-purpose "looping constructs" such as do,
    repeat, until, for and while. The implementation of Scheme we
    shall consider in chapter 5 does not share this defect. It will
    execute an iterative process in constant space, even if the
    iterative process is described by a recursive procedure. An
    implementation with this property is called _tail-recursive_. With
    a tail recursive implementation, iteration can be expressed using
    the ordinary procedure call mechanism, so that special iteration
    constructs are useful only as syntactic sugar.

    - Exercise 1.9. Each of the following two procedures defines a
      method for adding two positive integers in terms of the
      procedures inc, which increments its argument by 1, and dec,
      which decrements its argument by 1.

      #+BEGIN_SRC scheme
      (define (+ a b)
        (if (= a 0)
            b
            (inc (+ (dec a) b))))
      (define (+ a b)
        (if (= a 0)
            b
            (+ (dec a) (inc b))))
      #+END_SRC

      Using the substitution model, illustrate the process generated
      by each procedure in evaluating (+ 4 5). Are these processes
      iterative or recursive?

      #+BEGIN_SRC scheme
      (+ 4 5)
      (if (= 4 0) 5 (inc (+ (dec 4) 5)))
      (inc (+ 3 5))
      (inc (if (= 3 0) 5 (inc (+ (dec 3) 5))))
      (inc (inc (+ 2 5)))
      (inc (inc (inc (+ 1 5))))
      (inc (inc (inc (inc (+ 0 5)))))
      (inc (inc (inc (inc 5))))
      (inc (inc (inc 6)))
      (inc (inc 7))
      (inc 8)
      9

      (+ 4 5)
      (if (= 4 0) 5 (+ (dec 4) (inc 5)))
      (if false 5 (+ (dec 4) (inc 5)))
      (+ (dec 4) (inc 5))
      (+ 3 6)
      (if (= 3 0) 6 (+ (dec 3) (inc 6)))
      (if false 6 (+ (dec 3) (inc 6)))
      (+ (dec 3) (inc 6))
      (+ 2 7)
      (+ 1 8)
      (+ 0 9)
      9
      #+END_SRC

      The first process is linearly recursive, the second on e is
      iterative.

    - Exercise 1.10. The following procedure computes a mathematical
      function called Ackerman's function.

      #+BEGIN_SRC scheme
      (define (A x y)
        (cond ((= y 0) 0)
              ((= x 0) (* 2 y))
              ((= y 1) 2)
              (else (A (- x 1)
                       (A x (- y 1))))))
      #+END_SRC

      What are the values of the following expressions?

      #+BEGIN_SRC scheme
      (A 1 10)
      (A 2 4)
      (A 3 3)
      #+END_SRC

      #+BEGIN_SRC scheme
      (A 1 10)
      (cond ((= 10 0) 0)
            ((= 1 0) (* 2 10))
            ((= 10 1) 2)
            (else (A (- 1 1)
                     (A 1 (- 10 1)))))
      (cond (false 0)
            ((= 1 0) (* 2 10))
            ((= 10 1) 2)
            (else (A (- 1 1)
                     (A 1 (- 10 1)))))
      (cond ((= 1 0) (* 2 10))
            ((= 10 1) 2)
            (else (A (- 1 1)
                     (A 1 (- 10 1)))))
      (cond (false (* 2 10))
            ((= 10 1) 2)
            (else (A (- 1 1)
                     (A 1 (- 10 1)))))
      (cond ((= 10 1) 2)
            (else (A (- 1 1)
                     (A 1 (- 10 1)))))
      (cond (false 2)
            (else (A (- 1 1)
                     (A 1 (- 10 1)))))
      (cond (else (A (- 1 1)
                     (A 1 (- 10 1)))))       
      (A (- 1 1)
         (A 1 (- 10 1)))
      (A (- 1 1) (A 1 9))
      (A (- 1 1) 
         (cond ((= 9 0) 0)
               ((= 1 0) (* 2 9))
               ((= 9 1) 2)
               (else (A (- 1 1)
                        (A 1 (- 9 1))))))
      (A (- 1 1) 
         (A (- 1 1)
            (A 1 (- 9 1))))
      (A (- 1 1) 
         (A (- 1 1)
            (A 1 8)))
      (A (- 1 1) 
         (A (- 1 1)
            (cond ((= 8 0) 0)
                  ((= 1 0) (* 2 8))
                  ((= 8 1) 2)
                  (else (A (- 1 1)
                           (A 1 (- 8 1)))))))
      (A (- 1 1) 
         (A (- 1 1)
            (A (- 1 1)
               (A 1 (- 8 1)))))
      (A (- 1 1) 
         (A (- 1 1)
            (A (- 1 1)
               (A 1 7))))
      (A (- 1 1) 
         (A (- 1 1)
            (A (- 1 1)
               (A 1 7))))
      (A (- 1 1) 
         (A (- 1 1)
            (A (- 1 1)
               (A (- 1 1)
                  (A 1 6)))))
      (A (- 1 1) 
         (A (- 1 1)
            (A (- 1 1)
               (A (- 1 1)
                  (A (- 1 1)
                     (A 1 5))))))
      (A (- 1 1) 
         (A (- 1 1)
            (A (- 1 1)
               (A (- 1 1)
                  (A (- 1 1)
                     (A (- 1 1)
                        (A 1 4)))))))
      (A (- 1 1) 
         (A (- 1 1)
            (A (- 1 1)
               (A (- 1 1)
                  (A (- 1 1)
                     (A (- 1 1)
                        (A (- 1 1)
                           (A 1 3))))))))
      (A (- 1 1) 
         (A (- 1 1)
            (A (- 1 1)
               (A (- 1 1)
                  (A (- 1 1)
                     (A (- 1 1)
                        (A (- 1 1)
                           (A (- 1 1)
                              (A 1 2)))))))))
      (A (- 1 1) 
         (A (- 1 1)
            (A (- 1 1)
               (A (- 1 1)
                  (A (- 1 1)
                     (A (- 1 1)
                        (A (- 1 1)
                           (A (- 1 1)
                              (A (- 1 1)
                                 (A 1 1))))))))))
      (A (- 1 1) 
         (A (- 1 1)
            (A (- 1 1)
               (A (- 1 1)
                  (A (- 1 1)
                     (A (- 1 1)
                        (A (- 1 1)
                           (A (- 1 1)
                              (A (- 1 1)
                                 2)))))))))
      (A (- 1 1) 
         (A (- 1 1)
            (A (- 1 1)
               (A (- 1 1)
                  (A (- 1 1)
                     (A (- 1 1)
                        (A (- 1 1)
                           (A (- 1 1)
                              4))))))))
      (A (- 1 1) 
         (A (- 1 1)
            (A (- 1 1)
               (A (- 1 1)
                  (A (- 1 1)
                     (A (- 1 1)
                        (A (- 1 1)
                           8)))))))
      ...
      1024
      #+END_SRC

      (A 0 n) == 2*n
      (A 1 n) == 2^n
      (A 2 n) == 2^2^n (?) 2^2^4 == 65536
      
      #+BEGIN_SRC scheme
      (A 2 4)
      (cond ((= 4 0) 0)
            ((= 2 0) (* 2 4))
            ((= 4 1) 2)
            (else (A (- 2 1)
                     (A 2 (- 4 1)))))
      (A (- 2 1) 
         (A 2 (- 4 1)))
      (A 1 (A 2 3))
      (A 1
         (A 1
            (A 1 2)))
      (A 1
         (A 1
            (A 1
               (A 1 1))))
      (A 1
         (A 1
            (A 1 2)))
      (A 1
         (A 1
            (A 0 (A 1 1))))
      (A 1
         (A 1
            (A 0 2)))
      (A 1
         (A 1 4))
      (A 1 16)
      65536
      #+END_SRC

      #+BEGIN_SRC scheme
      (A 3 3)
      (A 2 (A 3 2))
      (A 2 (A 2 (A 3 1)))
      (A 2 (A 2 2))
      (A 2 (A 1 (A 2 1)))
      (A 2 (A 1 2))
      (A 2 (A 0 (A 1 1)))
      (A 2 (A 0 2))
      (A 2 4)
      (A 1 (A 2 3))
      (A 1 (A 1 (A 2 2)))
      (A 1 (A 1 (A 1 (A 2 1))))
      (A 1 (A 1 (A 1 2)))
      (A 1 (A 1 (A 0 (A 1 1))))
      (A 1 (A 1 (A 0 2)))
      (A 1 (A 1 4))
      (A 1 16)
      65536
      #+END_SRC

      Consider the following procedures, where A is the procedure
      defined above:

      #+BEGIN_SRC scheme
      (define (f n) (A 0 n))
      (define (g n) (A 1 n))
      (define (h n) (A 2 n))
      (define (k n) (* 5 n n))
      #+END_SRC
      
      Give concise mathematical definitions for the functions computed
      by the procedures f, g, and h for positive integer values of
      n. For example, (k n) computes 5n^2.

      f = 2 * n
      g = 2^n
      h = 2^2^2...(n times)

*** 1.2.2 Tree Recursion

    Another common pattern in computation is called _tree
    recursion_. As an example, consider computing the sequence of
    Fibonacci numbers, in which each number is the sum of the
    preceding two:

    0,1,1,2,3,5,8,13,21,...

    In general, the Fibonacci numbers can be defined by the rule:

    #+BEGIN_EXAMPLE
             | 0 if n = 0 
    Fib(n)=  | 1 if n = 1 
             | Fib(n-1) + Fib(n-2) otherwise           
    #+END_EXAMPLE
    
    We can immediately translate this definition into a recursive
    procedure for computing Fibonacci numbers:
    
    #+BEGIN_SRC scheme
    (define (fib n)
      (cond ((= n 0) 0)
            ((= n 1) 1)
            (else (+ (fib (- n 1))
                     (fib (- n 2))))))
    #+END_SRC

    Consider the pattern of this computation. To compute (fib 5), we
    compute (fib 4) and (fib 3). To compute (fib 4), we compute
    (fib 3) and (fib 2). In general, the evolved process looks like a
    tree. Notice that the branches split into two at each level
    (except at the botton); this reflects the fact that the fib
    procedure calls itself twice each time it is invoked.

    This procedure is instructive as a prototypical tree recursion,
    but it is a terrible way to compute Fibonacci numbers because it
    does so much redundant computation. Notice in figure 1.5 that the
    entire computation of (fib 3) -- almost half the work -- is
    duplicated. In fact, it is not hard to show that the number of
    times the procedure will compute (fib 1) or (fib 0) (the number of
    leaves in the above tree, in general) is precisely Fib(n+1). To
    get an idea of how bad this is, one can show that the value of
    Fib(n) grows exponentially with n. More precisely (see exercise
    1.13), Fib(n) is the closest integer to phi^n/sqrt(5), where

    phi = (1 + sqrt(5))/2 ~= 1.6180

    is the _golden ratio_, which satisfies the equation

    phi^2 = phi + 1

    Thus, the process uses a number of steps that grows exponentially
    with the input. On the other hand, the space required grows only
    linearly with the input, because we need keep track only of which
    nodes are above us in the tree at any point in the computation. In
    general, the number of steps required by a tree-recursive process
    will be proportional to the number of nodes in the tree, while the
    space required will be proportional to the maximum depth of the
    tree.

    We can also formulate an iterative process for computing the
    Fibonacci numbers. The idea is to use a pair of integers _a_ and
    _b_, initialized to _Fib(1)_ = 1 and _Fib(0) = 0_, and to
    repeatedly reply the simultaneous transformations

    a <- a + b
    b <- a

    It is not hard to show that, after applying this transformation
    _n_ times, _a_ and _b_ will be equal, respectively, to _Fib(n+1)_
    and _Fib(n)_. Thus, we can compute Fibonacci numbers iteratively
    using the procedure

    #+BEGIN_SRC scheme
    (define (fib n)
      (fib-iter 1 0 n))
    (define (fib-iter a b count)
      (if (= count 0)
          b
          (fib-iter (+ a b) a (- count 1))))
    #+END_SRC

    This second method for computing _Fib(n)_ is a linear
    iteration. The difference in number of steps required by the two
    methods -- one linear in _n_, one growing as fast as _Fib(n)_
    itself -- is enormous, even for small inputs.
    
    One should not conclude from this that tree-recursive processes
    are useless. When we consider processes that operate on
    hierarchically structured data rather than numbers, we will find
    that tree recursion is a natural and powerful tool. But even in
    numerical operations, tree-recursive processes can be useful in
    helping us to understand and design programs. For instance,
    although the first fib procedure is much less efficient than the
    second one, it is more straightforward, being little more than a
    translation into Lisp of the definition of the Fibonacci
    sequence. To formulate the iterative algorithm required noticing
    that the computation could be recast as an iteration with three
    state variables.

**** Example: Counting change.

     It takes only a bit of cleverness to come up with the iterative
     Fibonacci algorithm. In contrast, consider the following problem:
     How many different ways can we make change of $1.00, given
     half-dollars, quarters, dimes, nickels, and pennies? More
     generally, can we write a procedure to compute the number of ways
     to change any given amount of money?

     This problem has a simple solution as a recursive
     procedure. Suppose we think of the types of coins available as
     arranged in some order. Then the following relation holds:

     The number of ways to change amount _a_ nusing _n_ kinds of coins
     equals

     - The number of wasys to change amount _a_ using all but the
       first kind of coin, plus
     - The number of ways to change amount a - d using all n kinds of
       coins, where d is the denomination of the first kind of coin.

       
     To see why this is true, observe that the ways to make change
     can be divided into two groups: those that do not use any of
     the first kind of coin, and those that do. Therefore, the total
     number of ways to make change for some amount is equal to the
     number of ways to make change for the amount without using the
     first kind of coin, plus the number of ways to make change
     assuming that we do use the first kind of coin, plus the number
     of ways to make change assuming that we do use the first kind of
     coin. But the latter number is equal to the number of ways to
     make change for the amount that remains after using a coin of the
     first kind.

     Thus, we can recursively reduce the problem of changing a given
     amount to the problem of changing smaller amounts using fewer
     kinds of coins. Consider this reduction rule carefully, and
     convince yourself that we can use it to describe an algorithm if
     we specify the following degenerate cases:

     - If _a_ is exactly 0, we should count that as 1 way to make change.
     - If _a_ is less than 0, we should count that as 0 ways to make change.
     - If _n_ is 0, we should count that as 0 ways to make change.

     We can easily translate this description into a recursive
     procedure:

     #+BEGIN_SRC scheme
     (define (count-change amount)
       (cc amount 5))
     (define (cc amount kinds-of-coins)
       (cond ((= amount 0) 1)
             ((or (< amount 0) (= kinds-of-coins 0)) 0)
             (else (+ (cc amount
                          (- kinds-of-coins 1))
                      (cc (- amount
                             (first-denomination kinds-of-coins))
                          kinds-of-coins)))))
     (define (first-denomination kinds-of-coins)
       (cond ((= kinds-of-coins 1) 1)
             ((= kinds-of-coins 2) 5) 
             ((= kinds-of-coins 3) 10)
             ((= kinds-of-coins 4) 25)
             ((= kinds-of-coins 5) 50)))
     #+END_SRC

     (The first-denominator procedure takes as input the number of
     kinds of coins available and returns the denomination of the
     first kind. Here we are thinking of the coins as arranged in
     order from largest to smallest, but any order would do as well.)
     We can now answer our original question about changing a dollar:

     #+BEGIN_SRC scheme
     (count-change 100)
     292
     #+END_SRC
     
     count-change generates a tree-recursive process with redundancies
     similar to those in our first implementation of fib. (It will
     take quite a while for that 292 to be computed.) ON the other
     hand, it is not obvious how to design a better algorithm for
     computing the result, and we leave this problem as a
     challenge. The observation that a tree-recursive process may be
     highly inefficient but often easy to specify and understand has
     led people to propose that one could get the best of both worlds
     by designing a "smart compiler" that could transform
     tree-recursive procedures into more efficient procedures that
     compute the same result.

     - Exercise 1.11. A function _f_ is defined by the rule that
       _f(n) = n_ if _n < 3_ and _f(n) = f(n - 1) + 2 * f(n - 2) + 3 *
       f(n - 3)_ if _n>=3_. Write a procedure that computes _f_ by
       means of a recursive process. Write a procedure that computes
       _f_ by means of an iterative process.

       Recursive:
       #+BEGIN_SRC scheme
       (define (f-rec n)
         (cond
           ((< n 3) n)
           (else (+ (f-rec (- n 1)) (* 2 (f-rec (- n 2))) (* 3 (f-rec (- n 3)))))))
       #+END_SRC

       #+BEGIN_SRC scheme
       (define (f n i acc1 acc2 acc3)
         (cond 
            ((= i n) (+ acc1 (* 2 acc2) (* 3 acc3)))
            ((< n 3) n)                                ; if n is less than 3 just return n.
            ((< i 3) (f n 3 2 1 0))                    ; otherwise, start at 3 for i.
            (else (f n (+ i 1) (+ acc1 (* 2 acc2) (* 3 acc3)) acc1 acc2)))) ; iterate.
       #+END_SRC

     - Exercise 1.12. The following pattern of numbers is called
       _Pascal's triangle_.

       #+BEGIN_SRC scheme
              1
             1 1
            1 2 1
           1 3 3 1
          1 4 6 4 1
            ...
       #+END_SRC

       The numbers at the edge of the triangle are all 1, and each
       number inside the triangle is the sum of the two numbers above
       it. Write a procedure that computes the elements of Pascal's
       triangle by means of a recursive process.

       1 1 1 1 2 1 1 3 3 1

       row/column -- 

       #+BEGIN_SRC scheme
       (define (pascal i j)
         (cond
           ((or (= i 1) (= j 1) (= j i)) 1)
           (else
            (+ (pascal (- i 1) (- j 1))
               (pascal (- i 1) j)))))
       #+END_SRC

     - Exercise 1.13. Prove that Fib(n) is the closest integer to
       phi^n/sqrt(5), where phi = (1 + sqrt(5)) / 2. Let omega=(1 -
       sqrt(5))/2. Use induction and the definition of the Fibonacci
       numbers to prove that Fib(n) = (phi^n - omega^n)/sqrt(5)
    
       fib(n) = 1 1 2 3 5 8 13 ...
       assume fib(n) also = (phi^n - omega^n)/sqrt(5)

       n = 1 checks.
       n = 2 checks.

       assume that Fib'(n) works.
       Fib'(n+1) = 
       #+BEGIN_EXAMPLE
       (phi^(n - 1) - omega(n-1))/sqrt(5) 
       + 
       (phi^(n) - omega(n))/sqrt(5)

       = 

       (phi^(n - 1) + phi^n - omega^(n) - omega^(n-1)) / sqrt(5)

       =

       (phi^n * (phi^-1 + 1) - omega^n*(omega^-1 + 1)) / sqrt(5)

       = 
       
       
       #+END_EXAMPLE
       
       and

       #+BEGIN_EXAMPLE
       (phi^(n + 1) - omega(n+1)) / sqrt(5)
       #+END_EXAMPLE
      
       ... Basically you do a lot of algebra.

*** 1.2.3. Orders of Growth
    The previous examples illustrate that processes can differ
    considerably in the rates at which they consume computational
    resources. One convenient way to describe this difference is to
    use the notion of _order of growth_ to obtain a gross measure of
    the resources required by a process as the inputs become larger.

    Let _n_ be a parameter that measures the size of the problem, and
    let R(n) be the amount of resources the process requires for a
    problem of size _n_. In our previous examples we took _n_ to be
    the number for which a given function is to be computed, but there
    are other possibilities. For instance, if our goal is to compute
    an approximation to the square root of a number, we might take _n_
    to be the number of digits accuracy required. For matrix
    multiplication we might take _n_ to be the number of rows in the
    matrices. In general there are a number of properties of the
    problem with respect to which it will be desirable to analyze a
    given process. Similarly, R(n) might measure the number of
    internal storage registers used, the number of elementary machine
    operations performed, and so on. In computers that do only a fixed
    number of operations at a time, the time required will be
    proportional to the number of elementary machine operations
    performed.

    We say that R(n) has order of growth O(f(n)) (O is theta), written
    R(n) = O(f(n)) (pronounced "theta of n of f(n)"), if there are
    positive constants k_1 and k_2 independent ofn such that

    k_1*f(n) <= R(n) <= k_2*f(n)

    for any sufficiently large value of _n_. (In other words, for
    large _n_, the value R(n) is sandwiched between k_1*f(n) and
    k_2*f(n).)

    For instance, with linear recursive process for computing
    factorial described in section 1.2.1 the number of steps grows
    proportionally to the input _n_. Thus the steps required for this
    process grows as O(n). We also saw that the space require grows
    as O(n). For iterative factorial, the number of steps is still
    O(n) but the space required is O(1) -- that is, constant. The
    tree-recursive Fibonacci computation requries O(phi^n) steps and
    space O(n), where phi is the golden ratio.

    Orders of growth provide only a crude description of the behavior
    of a process. For example, a process requiring n^2 steps and a
    process requiring 1000n^2 steps and a process requiring 3*n^2 +
    10*n + 17 steps all have O(n^2) order of growth. On the other
    hand, order of growth provides a useful indication of how we may
    expect the behavior of the process to change as we change the size
    of the problem. For a O(n) (linear) process, doubling the size
    will roughly double the amount of resources used. For an
    exponential process, each increment in problem size will multiply
    the resource utilization by a constant factor. In the remainder of
    section 1.2 we will examine two algorithms whose order of growth
    is logarithmic, so that soubling the problem size increases the
    resource requirement by a constant amount.

    - Exercise 1.14 Draw the tree illustrating the process generated
      by the count-change procedure of section 1.2.2 in making change
      for 11 cents. What are the orders of growth of the space and
      number of steps used by this process as the amount to be changed
      increases?

      #+BEGIN_SRC scheme
      (define (count-change amount)
        (cc amount 5))
      (define (cc amount kinds-of-coins)
        (cond ((= amount 0) 1)
              ((or (< amount 0) (= kinds-of-coins 0)) 0)
              (else (+ (cc amount
                           (- kinds-of-coins 1))
                       (cc (- amount
                              (first-denomination kinds-of-coins))
                           kinds-of-coins)))))
      (define (first-denomination kinds-of-coins)
        (cond ((= kinds-of-coins 1) 1)
              ((= kinds-of-coins 2) 5)
              ((= kinds-of-coins 3) 10)
              ((= kinds-of-coins 4) 25)
              ((= kinds-of-coins 5) 50)))
      #+END_SRC 
      
      When we add amounts, especially for large n (large amount to be
      changed) for each coin type smaller than the
      largest, we end up with at least one branch of execution that
      results in 2 * n number of calls. This is for each coin type we
      have, so 5 * 2 * n. However, when it's a large amount, for each
      time we subtract a larger (not size 1) coin from the amount, we
      again end up with another branch with at the left edge roughly n
      calls. Because there are five coins, we can say that for large
      numbers most layers of the tree will have five branches and that
      the tree will have a depth of roughly n. Therefore, the order of
      growth in terms at least of time of computation/steps is 5^n. We
      can generalize m to be the number of coins and get O(m^n), it
      will never exceed this and will never be very far behind it
      either. Since there are deferred computations, the space order
      of growth isn't constant. Each function call results in two more
      function calls. The total number of function calls is O(m^n).
      But this is inaccurate, 5^n is a massive number for even more
      than five or six levels of recursion (n) and doesn't map at all
      to the actual number of operations.

      The space order of growth is O(n), the stack can only get n deep
      at a time because we finish one function call before going on to
      the next.

      The function itself will always have either two branches or
      none. We count every function call as a unit of work. 

      For every unit of currency, we divide n by the size of unit of
      currency and end up with something roughly O(n) in size. 

      So for two kinds of currency we get

      cc(n, 1) + cc(n-5, 2)

      which is an O(n) call and n/5 O(n) calls or basically n^2.

      So for very large n we get

      cc(n, 1) + cc(n-5, 2) + cc(n-10, 3) + cc(n-25, 4) + cc(n-50, 5)
      
      Each reduces to so many O(n) calls. 

      O(n) + (n/5 O(n) calls) + (n/10 O(n^2) calls, each decays into
      cc(n-5, 2) calls) + (n/25 O(n^3) calls) + (n/50 O(n^4) calls)

      So it's n^m, where m is the number of kinds of currency.

    - Exercise 1.15. The sine of an angle (specified in radians) can
      be computed by making use of the approximation sin(x) =~ x if x
      is sufficiently small, and the trigonometric identity

      sin(x) = 3 * sin(x / 3) - 4 * sin^3(x / 3)

      to reduce the size of the argument of sin. (For purposes of this
      exercise an angle is considered "sufficiently small" if its
      magnitude is not greater than 0.1 radians.) These ideas are
      incorporated in the following procedures:

      #+BEGIN_SRC scheme
      (define (cube x) (* x x x))
      (define (p x) (- (* 3 x) (* 4 (cube x))))
      (define (sine angle)
        (if (not (> (abs angle) 0.1))
            angle
            (p (sine (/ angle 3.0)))))
      #+END_SRC

      a. How many times is the procedure p applied when (sine 12.15)
      is evaluated?

      We can see that p is a deferred call and sine is deferred until
      the angle gets sufficiently small, so all we have to do is solve
      this equation:

      12.15*(1/3^n) < 0.1
      
      1/3^n > 0.1 / 12.15
      log1/3(1/3^n) > log1/3(0.1/12.15)
      n > log1/3(0.1/12.5)
      n > 4.36
      So 5 calls are needed. In general the larger the n, the more
      calls required.
      
      We created a function for the number of calls already:

      n = ceil(log(0.1/12.5)/log(1/3))
      
      Since 12.5 is n:

      ceil(log(0.1/n)/log(1/3))
      
      We get rid of ceil and the log(1/3) which are constant factors:

      log(0.1/n)

      We use a logarithmic identity to reduce this:

      log(0.1) - log(n)

      and remove more constant factors:

      log(n). 

      When we removed log(1/3) we removed a negation that always made
      the result positive for integer n.

      The space asymptotics are the same since we have a single
      deferred call on the stack, so it grows as the number of
      function calls grows.

*** 1.2.4 Exponentiation

    Consider the problem of computing the exponential of a given
    number. We would like a procedure that takes a arguments a base b
    and a positive integer exponent n and compute b^n. One way to do
    this is with the recursive definition

    b^n = b * b^n - 1
    b^0 = 1

    which translates readily into the procedure

    #+BEGIN_SRC scheme
    (define (expt b n)
      (if (= n 0)
          1
          (* b (expt b (- n 1)))))
    #+END_SRC

    This is a linear recursive process, which requires O(n) steps and
    O(n) space. Just as with factorial, we can readily formulate an
    equivalent linear iteration:

    #+BEGIN_SRC scheme
    (define (expt b n)
      (expt-iter b n 1))
    (define (expt-iter b counter product)
      (if (= counter 0)
          product
          (expt-iter b (- counter 1) (* b product))))
    #+END_SRC

    This version requires O(n) steps and O(1) space.

    We can compute exponentials in fewer steps by using successive
    squaring. For instance, rather than computing b^8 as

    b * (b *(b *(b *(b *(b *(b *(b)))))))

    we can compute it using three multiplications:

    b^2 = b * b
    b^4 = b^2 * b^2
    b^8 = b^4 * b^4

    This method works fine for exponents that are powers of 2. We can
    also take advantage of successive squaring in computing
    exponentials in general if we use the rule

    b^n = (b^n/2)^2         if n is even
    b^n = (b * b^(n - 1))   if n is odd

    We can express this method as a procedure:

    #+BEGIN_SRC scheme
    (define (fast-expt b n)
      (cond ((= n 0) 1)
            ((even? n) (square (fast-expt b (/ n 2))))
            (else (* b (fast-expt b (- n 1))))))
    #+END_SRC

    where the predicate to test whether an integer is even is defined
    in terms of the primitive procedure remainder by

    #+BEGIN_SRC scheme
    (define (even? n)
      (= (remainder n 2)) 0)
    #+END_SRC

    The process evolved by fast-expt grows logarithmically with n in
    both space and number of steps. To see this, observe that
    computing b^2n using fast-expt requires only one more
    multiplication than computing b^n. The size of the exponent we can
    compute therefore doubles (approximately) with every new
    multiplication we are allowed. Thus, the number of multiplications
    required for an exponent of n grows about as fast as the logarithm
    of n to the base 2. The process has O(log n) growth.

    The difference between O(log n) growth and O(n) growth becomes
    striking as n becomes large. For example, fast-expt for n = 1000
    requires only 14 multiplications. It is also possible to use the
    idea of successive squaring to devise an iterative algorithm that
    computes exponentials with a logarithmic number of steps (see
    exercise 1.16), although, as is often the case with iterative
    algorithms, this is not written down so straightforwardly as the
    recursive algorithm.

    - Exercise 1.16. Design a procedure that evolves an iterative
      exponentiation process that uses successive squaring and uses a
      logarithmic number of steps, as does fast-expt. (Hint: Using the
      observation that (b^(n/2)^2) = (b^2)^n/2, keep, along with the
      exponent n and the base b, an additional state variable a, and
      define the state transformation in such a way that the product a
      b^n is unchanged from state to state. At the beginning of the
      process a is taken to be 1, and the answer is given by the value
      of a at the end of the process. In general, the technique of
      defining an _invariant quantity_ that remains unchanged from
      state to state is a powerful way to think about the design of
      iterative algorithms). 

      #+BEGIN_SRC scheme
      (define (fast-expt b n)
        (fast-expt-iter b n 1))
      (define (fast-expt-iter b n a)
        (cond
          ((= n 0) a)
          ((even? n) (fast-expt-iter (* b b) (/ n 2) a))
          ((odd? n) (fast-expt-iter b (- n 1) (* b a)))))
      #+END_SRC

    - Exercise 1.17 The exponentiation algorithms in this section are
      based on performing exponentiation by means of repeated
      multiplication. In a similar way, one can perform integer
      multiplication by means of repeated addition. The following
      multiplication procedure (in which it is assumed that our
      language can only add, not multiply) is analogous to the expt
      procedure:

      #+BEGIN_SRC scheme
      (define (* a b)
        (if (= b 0)
            0
            (+ a (* a (- b 1)))))
      #+END_SRC

      This algorithm takes a number of steps that is linear in b. Now
      suppose we include, together with addition, operations double,
      which doubles an integer, and halve, which divides an (even)
      integer by 2. 

      #+BEGIN_SRC scheme
      (define (double a) (+ a a))
      (define (halve a) (/ a 2))
      (define (mul a b)
        (cond
          ((= b 0) 0)
          ((= a 0) 0)
          ((= a 1) b)
          ((= b 1) a)
          ((even? a) (mul (halve a) (double b)))
          ((odd? a) (+ b (mul (- a 1) b)))))
      #+END_SRC

    - Exercise 1.18 Using the results of exercises 1.16 and 1.17
      devise a procedure that generates an iterative process for
      multiplying two integers in terms of adding, doubling and
      halving and uses a logarithmic number of steps.

      #+BEGIN_SRC scheme
      (define (mul a b) (mul-iter a b 0))
      (define (mul-iter a b acc)
        (cond
          ((or (= a 1) (= b 0) (= a 0)) (+ acc b))
          ((even? a) (mul-iter (halve a)  (double b) acc))
          ((odd? a) (mul-iter (- a 1) b (+ acc b)))))
      #+END_SRC

    - Exercise 1.19 There is a clever algorithm for computing the
      Fibonacci numbers in a logarithmic number of steps. Recall the
      transformation of the state variables a and b in the fib-iter
      process of section 1.2.2: a <- a + b and b <- a. Call this
      transformation T, and observe that applying T over and over
      again n times, starting with 1 and 0, produces the pair
      Fib(n + 1) and Fib(n). In other words, the Fibonacci numbers are
      produced by applying T^n, the nth power of the transformation T,
      starting with the pair (1, 0). Now consider T to be the special
      case of p = 0 and q = 1 in a family of transformations T_pq,
      where T_pq transforms the pair (a,b) according to a <- bq + aq +
      ap and b <- bp + aq. Show that if we apply such a transformation
      T_pq twice, the effect is the same as using a single
      transformation T_p'q' of the same form, and compute p' and q' in
      terms of p and q. This gives us an explicit way to square these
      transformations, and thus we can compute T^n using successive
      squaring, as in the fast-expt procedure. put this all together
      to complete the following procedure, which runs in a logarithmic
      number of steps:
      
      #+BEGIN_SRC scheme
      (define (fib n)
        (fib-iter 1 0 0 1 n))
      (define (fib-iter a b p q count)
        (cond ((= count 0) b)
              ((even? count)
               (fib-iter a b 
                             (+ (* p p) (* q q))
                             (+ (* 2 p q) (* q q))
                             (/ count 2)))
              (else (fib-iter (+ (* b q) (* a q) (* a p))
                              (+ (* b p) (* a q))
                              p
                              q
                              (- count 1)))))
      #+END_SRC

*** 1.2.5 Greatest Common Divisors

    The greatest common divisor (GCD) of two integers _a_ and _b_ is
    defined to be the largest integer that divides both a and b with
    no remainder. For example, the GCD of 16 and 28 is 4. In chapter
    2, when we investigate how to implement rational-number
    arithmetic, we will need to be able to compute GCDs in order to
    reduce rational numbers to lowest terms. (To reduce a rational
    number to lowest terms, we must divide both the numerator and the
    denominator by their GCD. For example, 16/28 reduces to 4/7). One
    way to find the GCD of two integers is to factor them and search
    for common factors, but there is a famous algorithm that is much
    more efficient.

    The idea of the algorithm is based on the observation that if r is
    the remainder when a is divided by b, then the common divisors of
    a and b are precisely the same as the common divisors of b and
    r. Thus, we can use the equation

    GCD(a, b) = GCD(b, r)

    to successively reduce the problem of computing a GCD to the
    problem of computing the GCD of smaller and smaller pairs of
    integers. For example,

    #+BEGIN_EXAMPLE
    GCD(206, 40) = GCD(40, 6)
                 = GCD(6, 4)
                 = GCD(4, 2)
                 = GCD(2, 0)
                 = 2
    #+END_EXAMPLE
    
    reduces GCD(206, 50) to GCD(2, 0) which is 2. It is possible to
    show that starting with any two positive integers and performing
    repeated reductions will always eventually produce a pair where
    the second number is 0. Then the GCD is the other number in the
    pair. THis method for computing the GCD is known as Euclid's
    Algorithm.

    It is easy to express Euclid's Algorithm as a procedure:

    #+BEGIN_SRC scheme
    (define (gcd a b)
      (if (= b 0)
          a
          (gcd b (remainder a b))))
    #+END_SRC

    This generates an iterative process, whose number of steps grows
    as the logarithm of the numbers involved.

    The fact that the number of steps required by Euclid's Algorithm
    has logarithmic growth bears an interesting relation to the
    Fibonacci numbers:

    Lame's Theorem: If Euclid's Algorithm requires k steps to compute
    the GCD of some pair, then the smaller number in the pair must be
    greater than or equal to the _k_th fibonacci number.

    We can use this theorem to get an order-of-growth estimate for
    Euclid's Algorithm. Let n be the smaller of the two inputs to the
    procedure. If the process takes k steps, then we must have n >=
    Fib(k) ~~ phi^k/sqrt(5). Therefore the number of steps grows as
    the logarithm (base phi) of n. Hence the order of growth is O(log
    n).

    - Exercise 1.20. The process that a procedure generates is of
      course dependent on the rules used by the interpreter. As an
      example, consider the iterative gcd procedure given
      above. Suppose we were to interpret this procedure using
      normal-order evaluation, as discussed in section 1.1.5. (The
      normal-order evaluation rule for if is described in exercise
      1.5) Using the substitution method (for normal order),
      illustrate the process generated in evaluatin (gcd 206 40) and
      indicate the remainder operations that are actually
      performed. How many remainder operations are actually performed
      in the normal-order evaluation of (gcd 206 40)? In the
      applicative-order evaluation?

      normal order:
      #+BEGIN_SRC scheme
      (define (gcd a b)
        (if (= b 0)
            a
            (gcd b (remainder a b))))

      (gcd 206 40)
      (if (= 40 0)
          206
          (gcd 40 (remainder 206 40)))
      (gcd 40 (remainder 206 40))
      (if (= (remainder 206 40) 0) 206 (gcd (remainder 206 40) (remainder 40 (remainder 206 40)))) ; 1 eval
      (gcd (remainder 206 40) (remainder 40 (remainder 206 40)))
      (if (= (remainder 40 (remainder 206 40)) 0)                                                  ; 2 eval -- false
          (remainder 206 40)
          (gcd (remainder 40 (remainder 206 40)) (remainder (remainder 206 40) (remainder 40 (remainder 206 40)))))
      (gcd (remainder 40 (remainder 206 40)) (remainder (remainder 206 40) (remainder 40 (remainder 206 40))))
      (if (= (remainder (remainder 206 40) (remainder 40 (remainder 206 40))) 0)                   ; 4 eval -- false
          (remainder 40 (remainder 206 40))
          (gcd (remainder (remainder 206 40) (remainder 40 (remainder 206 40)))
               (remainder (remainder 40 (remainder 206 40)) (remainder (remainder 206 40) (remainder 40 (remainder 206 40))))))
     (gcd (remainder (remainder 206 40) (remainder 40 (remainder 206 40)))
          (remainder (remainder 40 (remainder 206 40)) (remainder (remainder 206 40) (remainder 40 (remainder 206 40)))))
      (if (= (remainder (remainder 40 (remainder 206 40)) (remainder (remainder 206 40) (remainder 40 (remainder 206 40)))) 0) ; 7 eval -- true
          (remainder (remainder 206 40) (remainder 40 (remainder 206 40)))
          (gcd ... ...))
      (remainder (remainder 206 40) (remainder 40 (remainder 206 40))) ; 4 eval
      2
      ;; total 18 evals of remainder
      #+END_SRC

      applicative order:
      #+BEGIN_SRC scheme
      (define (gcd a b)
        (if (= b 0)
            a
            (gcd b (remainder a b))))
      (gcd 206 40)
      (if (= 40 0)
            206
            (gcd 40 (remainder 206 40)))
      (gcd 40 (remainder 206 40))           ; eval - 1
      (gcd 40 6)
      (if (= 6 0)
          40
          (gcd 6 (remainder 40 6)))
      (gcd 6 (remainder 40 6))              ; eval - 1
      (gcd 6 4)
      (if (= 4 0)
          6
          (gcd 4 (remainder 6 4)))
      (gcd 4 (remainder 6 4))              ; eval - 1
      (gcd 4 2)
      (if (= 2 0)
          4
          (gcd 2 (remainder 4 2)))
      (gcd 2 (remainder 4 2))              ; eval - 1
      (gcd 2 0)
      (if (= 0 0)
          2
          (gcd 0 (remainder 2 0)))
      2
      #+END_SRC
      
      4 evals of remainder.

*** 1.2.6 Example: Testing for Primality

    This section describes two methods for checking the primality of
    an integer n, one with the order of growth O(sqrt(n)) and a
    "probabilistic" algorithm with order of growth O(log n).  The
    exercises at the end of this section suggest programming projects
    based on these algorithms.

    Searching for divisors

    Since ancient times, mathematicians have been fascinated by
    problems concerning prime numbers, and many people have worked on
    the problem of determining ways to test if numbers are prime. One
    way to test if a number is prime is to find the number's
    divisors. The following program finds the smallest integral
    divisor (greater than 1) of a given number n. It does this in a
    straightforward way, by testing for divisibility by successive
    integers starting with 2.

    #+BEGIN_SRC scheme
    (define (smallest-divisor n)
      (find-divisor n 2))
    (define (find-divisor n test-divisor)
      (cond ((> (square test-divisor) n) n)
            ((divides? test-divisor n) test-divisor)
            (else (find-divisor n (+ test-divisor 1)))))
    (define (divides? a b)
      (= (remainder b a) 0))
    #+END_SRC

    We can test whether a number is prime as follows: n is prime if
    and only if n is its own smallest divisor.

    #+BEGIN_SRC scheme
    (define (prime? n)
      (= n (smallest-divisor n)))
    #+END_SRC

    The end test for find-divisor is based on the fact that if n is
    not prime it must have a divisor less than or equal to
    sqrt(n). This means that the algorithm need only test divisors
    between 1 and sqrt(n). Consequently, the number of steps required
    to identify n as prime will have order of growth O(sqrt(n)).

    The Fermat test

    The O(log n) primality test is based on a result from number
    theory known as Fermat's Little Theorem.

    Fermat's Little Theorem: If n is a prime number and a is any
    positive integer less than n, then a raised to th enth power is
    congruent to a modulo n.

    (Two numbers are said to be _congruent modulo n_ if they both have
    the same remainder when divided by n. The remainder of a number
    _a_ when divided by _n_ is also referred to as the _remainder of a
    modulo n_ or simply _a modulo n_.)

    If _n_ is not prime, then, in general, most of the numbers a < n
    will not satisfy the above relation. This leads to the following
    algorithm for testing primality: Given a number _n_, pick a
    random number a < n and compute the remainder of a^n modulo n. If
    the result is not equal to _a_, then _n_ is certainly not
    prime. If it is _a_, then chances are good that _n_ is prime. Now
    pick another random number _a_ and test it with the same
    method. If it also satisfies the equation, then we can be even
    more confident that _n_ is prime. By trying more and more values
    of _a_, we can increase our confidence in the result. This
    algorithm is known as the Fermat test.

    To implement the Fermat test, we need a procedure that computes
    the exponential of a number modulo another number:

    #+BEGIN_SRC scheme
    (define (expmod base exp m)
      (cond ((= exp 0) 1)
            ((even? exp)
             (remainder (square expmod base (/ exp 2) m) m))
            (else
             (remainder (* base (expmod base (- exp 1) m))
                        m))))
    #+END_SRC

    This is very similar to the fast-expt procedure of section
    1.2.4. It uses successive squaring, so that the number of steps
    grows logarithmically with the exponent.

    The Fermat test is preformed by choosing a random number a between
    1 and n - 1 inclusive and checking whether the remainder modulo n
    of the nth power of a is equal to a. The random number a is chosen
    using the procedure random, which we assume is included as a
    primitive in Scheme. random returns a nonnegative integer less
    than its integer input. Hence, to obtain a random number between 1
    and n - 1, we call random with an input of n - 1 and add 1 to the
    result:

    #+BEGIN_SRC scheme
    (define (fermat-test n)
      (define (try-it a)
        (= (expmod a n n) a))
      (try-it (+ 1 (random (- n 1)))))
    #+END_SRC

    The following procedure runs the test a given number of times, as
    specified by a parameter. Its value is true if the test succeeds
    every time, and false otherwise.

    #+BEGIN_SRC scheme
    (define (fast-prime? n times)
      (cond ((= times 0) true)
            ((fermat-test n) (fast-prime? n (- times 1)))
            (else false)))
    #+END_SRC

    Probabilistic methods

    The Fermat test differs in character from most familiar
    algorithms, in which one computes an answer that is guaranteed to
    be correct. Here, the answer obtained is only probably
    correct. More precisely, if n ever fails the Fermat test, we can
    be certain that _n_ is not prime. But the fact that _n_ passes the
    test, while an extremely strong indication, is still not a
    guarantee that _n_ is prime. What we would like to say is that for
    any number _n_, if we perform the test enough times and find that
    _n_ always passes the test, then the probability of error in our
    primality test can be made as small as we like.

    Unfortunately, this assertion is not quite correct. There do
    exist numbers that fool the Fermat test: numbers _n_ that are not
    prime and yet have the property that a^n is congruent to a modulo
    n for all integers a < n. Such numbers are extremely rare, so the
    Fermat test is quite reliable in practice. There are variations of
    the Fermat test that cannot be fooled. In these tests, as with the
    Fermat method, one tests the primality of an integer _n_ by
    choosing a random integer _a<n_ and checking some condition that
    depends upon n and a. (See exercise 1.28 for an example of such a
    test.) On the other hand, in contrast to the Fermat test, one can
    prove that, for any _n_, the condition does not hold for most of
    the integers a < n unless n is prime. Thus, if _n_ passes the test
    for some random choice of a, the chances are better than even that
    _n_ is prime. If _n_ passes the test for two random choices of a,
    the chances are better than 3 out of 4 that _n_ is prime. By
    running the test with more and more randomly chosen values of a we
    can make the probability of error as small as we like.

    The existence of tests for which one can prove that the chance of error
    becomes arbitrarily small has sparked interest in algorithms of
    this type, which have come to be known as _probabilistic
    algorithms_. There is a great deal of research activity in this
    area, and probabilistic algorithms have been fruitfully applied to
    many fields.

    - Exercise 1.21. Use the smallest-divisor procedure to find the
      smallest divisor of each of the following numbers: 199, 1999,
      1. 

      Answers: 199, 1999, 7.

    - Exercise 1.22. Most Lisp implementations include a primitive
      called runtime that returns an integer that specifies the amount
      of time the system has been running (measured, for example, in
      microseconds). The following timed-prime-test procedure, when
      called with an integer n, prints n and checks to see if n is
      prime. If n is prime, the procedure prints three asterisks
      followed by the amount of time used in performing the test.

      #+BEGIN_SRC scheme
      (define (timed-prime-test n)
        (newline)
        (display n)
        (start-prime-test n (runtime)))
      (define (start-prime-test n start-time)
        (if (prime? n)
            (report-prime (- (runtime) start-time))))
      (define (report-prime elapsed-time)
        (display " *** ")
        (display elapsed-time))
      #+END_SRC

      Using this procedure, write a procedure search-for-primes that
      checks the primality of consecutive odd integers in a specified
      range. Use your procedure to find the three smallest primes
      larger than 1000; larger than 10,000; larger than 100,000;
      larger than 1,000,000. Note the time needed to test each
      prime. Since the testing algorithm has order of growth
      O(sqrt(n)), you should expect that testing for primes around
      10,000 should take around sqrt(10) times as long as testing for
      primes around 1000. Do your timing data bear this out? How well
      do the data for 100,000 and 1,000,000 support the sqrt(n)
      prediction? Is your result compatible with the notion that
      programs on your machine run in time proportional to the number
      of steps required for the computation?

      #+BEGIN_SRC scheme
      (define (search-for-primes start-range end-range nprimes)
        (cond
          ((= nprimes 0) true)
          ((> start-range end-range) true)
          ((and (odd? start-range))
           (timed-prime-test start-range)
           (search-for-primes 
            (+ 2 start-range) 
            end-range 
            (if (prime? start-range) (- nprimes 1) nprimes)))
          ((even? start-range) (search-for-primes (+ 1 start-range) end-range nprimes))))
      #+END_SRC

    - Exercise 1.23. The smallest-divisor procedure shown at the start
      of this section does lots of needless testing: After it checks
      to see if the number is divisible by 2 there is no point in
      checking to see if it's divisible by any larger even
      numbers. This suggests that the values used for test-divisor
      should not be 2, 3, 4, 5, 6, ..., but rather 2, 3, 5, 7, 9,
      .... To implement this change, define a procedure next that
      returns 3 if its input is equal to 2 and otherwise returns its
      input plus 2. Modify the smallest-divisor procedure to use (next
      test-divisor) instead of (+ test-divisor 1). With
      timed-prime-test incporporating this modified version of
      smallest-divisor, run the test for each of the 12 primes found
      in exercise 1.22. Since this modification halves the number of
      test steps, you should expect it to run about twice as fast. Is
      this expectation confirmed? If not, what is the observed ratio
      of speeds of the two algorithms, and how do you explain the fact
      that it is different from 2?

      #+BEGIN_SRC scheme
      (define (next input)
        (if (= input 2) 3 (+ 2 input)))
      (define (smallest-divisor n)
        (smallest-divisor-iter n 2))
      (define (smallest-divisor-iter n divisor-test)
        (cond
          ((> (square divisor-test) n) n)
          ((divides? divisor-test n) divisor-test)
          (else (smallest-divisor-iter n (next divisor-test)))))
      #+END_SRC
      
      The ratio is ~1.65, close to 2 but not very
      close. This may be because although we are reducing the number
      of calls to smallest-divisor, we're also increasing the number
      of steps with an if statement. There used to be only a single
      addition there now there's a procedure call and a test before we
      get the next number.

    - 1.24. Modify the timed-prime-test procedure of exercise 1.22 to
      use fast-prime? (the Fermat method), and test each of the 12
      primes you found in that exercise. Since the Fermat test has
      O(log n) growth, how would you expect the time to test primes
      near 1,000,000 to compare with the time needed to test primes
      near 1000? Do your data bear this out? Can you explain any
      discrepancy you find?

      Since 1,000,000 is 1000 times 1000, you would expect that the
      test for primes near 1,000,000 to be log(1000) times as much or
      rougly 3 times as much.

      #+BEGIN_SRC scheme
      (define (timed-prime-test n)
        (newline)
        (display n)
        (start-prime-test n (runtime)))
      (define (start-prime-test n start-time)
        (if (fast-prime? n 3)
            (report-prime (- (runtime) start-time))))
      (define (report-prime elapsed-time)
        (display " *** ")
        (display elapsed-time))

      (define (search-for-primes start-range end-range nprimes)
        (cond
          ((= nprimes 0) true)
          ((> start-range end-range) true)
          ((and (odd? start-range))
           (timed-prime-test start-range)
           (search-for-primes 
            (+ 2 start-range) 
            end-range 
            (if (fast-prime? start-range 3) (- nprimes 1) nprimes)))
          ((even? start-range) (search-for-primes (+ 1 start-range) end-range nprimes))))
      #+END_SRC

      The time takesn is so small that even for really huge numbers
      the time doesn't register for my implementation of scheme.

    - Exercise 1.25 Alyssa P. Hacker complains that we went to a lot
      of extra work in writing expmod. After all, she says, since we
      already know how to compute exponentials, we could simply have
      written

      #+BEGIN_SRC scheme
      (define (expmod base exp m)
        (remainder (fast-expt base exp) m))
      #+END_SRC

      Is she correct? Would this procedure serve as well for our fast
      prime tester? Explain.
      
      It probably won't make a huge difference. If you put the
      remainder outside the fast-expt function you end up taking the
      remainder of an extremely large number which, depending on how
      the division algorithm works, could take longer than just taking
      a remainder every time we increase the power. However, if we
      assume that we only have to do a single subtraction per call of
      remainder in our original version of expmod we end up with
      log(exp) subtractions, whereas we actually have (base^exp)/m
      subtractions in the second case,  which is larger. Even if we
      have a constant multiplier per call to make it log(2*exp) the
      second outgrows the first.

    - Exercise 1.26. Louis Reasoner is having great difficulty doing
      exericse 1.24. His fast-prime? test seems to run more slowly
      than his prime? test. Louis calls his friend Eva Lu Ator over to
      help. When they examine Louis' code, they find that he has
      rewritten the expmod procedure to use an explicit multiplication
      rather than calling square:

      #+BEGIN_SRC scheme
      (define (expmod base exp m)
        (cond ((= exp 0) 1)
              ((even? exp)
               (remainder (* (expmod base (/ exp 2) m)
                             (expmod base (/ exp 2) m))))
              (else
               (remainder (* base (expmod base (- exp 1) m))
                          m))))
      #+END_SRC

      "I don't see what difference that could make" says Louis. "I
      do." says Eva. "By writing the procedure like that, you have
      transformed a O(log n) process into a O(n) process." Explain.

      Every time you hit the even? clause of the cond, you evaluate
      (expmod base (/ exp 2) m) twice. This causes you to have
      something resembling a binary tree with depth log(exp), every
      time you have an even number you spawn two calls. When you spawn
      y calls per function call you end up with a O(y^n) process where
      n is the depth of the function call tree. So originally the
      procedure took O(log exp) long, and now since you have a binary
      tree we end up with 2^(log exp), or just exp, so O(exp) or O(n),
      since the tree is log exp deep.

    - Exercise 1.27. Demonstrate that the Carmichael numbers listed in
      footnote 47 really do fool the Fermat test. That is, write a
      procedure that takes an integer _n_ and tests whether a^n is
      congruent to a modulo n for every a < n, and try your
      procedure on the given Carmichael numbers.

      561, 1105, 1729, 2465, 2821, 6601.

      #+BEGIN_SRC scheme
      (define (test-number n)
        (test-number-iter n 1))
      (define (test-number-iter n iter)
        (cond 
         ((= iter n) true)
         ((= (expmod iter n n) (remainder iter n)) 
          (test-number-iter n (+ 1 iter)))
         (else false)))
      #+END_SRC

      This procedure returns true for basic primes -- 3, 29, 7, 59,
      false for non-primes -- 14, 28, 57 -- and true for all the
      Carmichael numbers.

    - Exercise 1.28. One variant of the Fermat test that cannot be
      fooled is called the _Miller-Rabin test_. This starts from an
      alternate form of Fermat's Little Theorem, which states that if
      _n_ is a prime number and _a_ is any positive integer less than
      _n_, then _a_ raised to the (n-1)st power is congruent to 1
      modulo n. To test the primality of a number _n_ by the
      Miller-Rabin test, we pick a random number a<n and raise a to
      the (n-1)st power modulo n using the expmod procedure. However,
      whenever we perform the squaring step in expmod, we check to see
      if we have discovered a "nontrivial square root of 1 modulo n"
      that is, a number not equal to 1 or n - 1 whose square is equal
      to 1 modulo n. It is possible to prove that if such a nontrivial
      square root of 1 exists, then n is not prime. It is also
      possible to prove that if n is an odd number that is not prime
      then, for at least half the numbers a<n, computing a^n-1 in this
      way will reveal a nontrival square root of 1 modulo n. (This is
      why the Miller-Rabin test cannot be fooled.) Modify the expmod
      procedure to signal if it discovers a nontrivial square root of
      1 and use this to implement the Miller-Rabin test with a
      procedure analogous to fermat-test. Check your procedure by
      testing various known primes and non-primes. Hint: One
      convenient way to make expmod signal is to have it return 0.

      #+BEGIN_SRC scheme
      (define (miller-rabin-expmod base-orig exp-orig mod)
        (define (calc-expmod base exp)
         (cond
          ((= exp 0) 1)
          ((even? exp)
           (let ((squared (square base)))
            (cond
             ((and (= (remainder squared mod) 1)
                   (< base exp-orig)) 0)
             (else (remainder (calc-expmod squared (/ exp 2)) mod)))))
          ((odd? exp)
           (remainder (* base (calc-expmod base (- exp 1))) mod))))
        (calc-expmod base-orig exp-orig))
                   
      (define (miller-rabin-expmod base exp mod)
        (cond
          ((= exp 0) 1)
          ((even? exp)
           (let ((squared (square base)))
           (cond
            ((and (= (remainder squared mod) 1)
(not (= base exp))) 0)
            (else (remainder (miller-rabin-expmod squared (/ exp 2) mod) mod)))))
          ((odd? exp)
            (remainder (* base (miller-rabin-expmod base (- exp 1) mod)) mod))))
      (define (miller-rabin-test n test-n)
        (cond
         ((= test-n 0) true)
         (else (and (miller-rabin-test-one n)
                    (miller-rabin-test n (- test-n 1))))))
      (define (random-base n)
        (define random-nr (random n))
        (cond ((< random-nr 2) 2)
              ((> random-nr (- n 1)) (- n 1))
              (else random-nr)))
      (define (miller-rabin-test-one n)
        (= (miller-rabin-expmod (random-base n) (- n 1) n) 1))
      #+END_SRC


      
** 1.3 Formulating Abstractions with Higher-Order Procedures
   We have seen that procedures are, in effect, abstractions that
   describe compound operations on numbers independent of the
   particular numbers. For example, when we

   #+BEGIN_SRC scheme
   (define (cube x) (* x x x))
   #+END_SRC

   we are not talking about the cube of a particular number, but
   rather about a method for obtaining the cube of any number. Of
   course we could get along without ever defining this procedure by
   always writing expressions such as

   #+BEGIN_SRC scheme
   (* 3 3 3)
   (* x x x)
   (* y y y)
   #+END_SRC

   and never mentioning cube explicitly. This would place us at a
   serious disadvantage, forcing us to work always at the level of the
   particular operations that happen to be primitives in the language
   (multiplication, in this case) rather than in terms of higher-level
   operations. Our programs would be able to compute cubes but our
   language would lack the ability to express the concept of
   cubing. One of the things we should demand from a powerful
   programming language is the ability to build abstractions by
   assigning names to common patterns and then to work in terms of the
   abstractions directly. Procedures provide this ability. This is why
   all but the most primitive programming languages include mechanisms
   for defining procedures.

   Yet even in numerical processing we will be severely limited in our
   ability to create abstractions if we are restricted to procedures
   whose parameters must be numbers. Often the same programming
   pattern will be used with a number of different procedures. To
   express such patterns as concepts we will need to construct
   procedures that can accept procedures as arguments or return
   procedures as values. Procedures that manipulate procedures are
   called _higher-order procedures_. This section shows how
   higher-order procedures can serve as powerful abstraction
   mechanisms, vastly increasing the expressive power of our language.

*** 1.3.1 Procedures as Arguments

    Consider the following three procedures. The first computes the
    sum of the integers a through b:

    #+BEGIN_SRC scheme
    (define (sum-integers a b)
      (if (> a b)
          0
          (+ a (sum-integers (+ a 1) b))))
    #+END_SRC

    The second computes the sum of the cubes of the integers in the
    given range:

    #+BEGIN_SRC scheme
    (define (sum-cubes a b)
      (if (> a b)
          0 
          (+ (cube a) (sum-cubes (+ a 1) b))))
    #+END_SRC

    The third computes the sum of a sequence in terms of the series:

    1/(1 * 3) + 1/(5 * 7) + 1/(9 * 11) + ...

    which converges to pi/8 (very slowly):

    #+BEGIN_SRC scheme
    (define (pi-sum a b)
      (if (> a b)
          0
          (+ (/ 1.0 (* a (+ a 2))) (pi-sum (+ a 4) b))))
    #+END_SRC
    
    These three procedures clearly share a common underlying
    pattern. They are for the most part identical, differing only in
    the name of the procedure, the function of a used to compute the
    term to be added, and the function that provides the next value of
    a. We could generate each of the procedures by filling in slots in
    the same template:

    #+BEGIN_SRC scheme
    (define (<name> a b)
      (if (> a b)
          0
          (+ (<term> a)
             (<name> (<next> a) b))))
    #+END_SRC

    The presence of such a common pattern is strong evidence that
    there is a useful abstraction waiting to be brought to the
    surface. Indeed, mathematicians long ago identified the
    abstraction of _summation of a series_ and invented "sigma
    notation," for example:

    $$ \sum_{n=a}^{b} f(n) = f(a) + ... + f(b) $$


    to express this concept. The power of sigma notation is that it
    allows mathematicians to deal with the concept of summation itself
    rather than only with particular sums -- for example, to formulate
    general results about sums that are independent of the particular
    series being summed.

    Similarly, as program designers, we would like our language to be
    powerful enough so that we can write a procedure that expresses
    the concept of summation itself rather than only procedures that
    compute particular sums. We can do so readily in our procedural
    language by taking the common template shown above and
    transforming the "slots" into formal parameters:

    #+BEGIN_SRC scheme
    (define (sum term a next b)
      (if (> a b)
          0
          (+ (term a)
             (sum term (next a) next b))))
    #+END_SRC

    Notice that sum takes as its arguments the lower and upper bounds
    a and b together with the procedures term and next. We can use sum
    just as we would any procedure. For example, we can use it (along
    with a procedure inc that increments its argument by 1) to define
    sum-cubes:

    #+BEGIN_SRC scheme
    (define (inc n) (+ n 1))
    (define (sum-cubes a b)
      (sum cube a inc b))
    #+END_SRC

    Using this we can compute the sum of the cubes of the integers
    from 1 to 10:

    #+BEGIN_SRC scheme
    (sum-cubes 1 10)
    3025
    #+END_SRC

    With the aid of an identity procedure to compute the term we can
    define sum-integers in terms of sum:

    #+BEGIN_SRC scheme
    (define (identity x) x)
    (define (sum-integers a b)
      (sum identity a inc b))
    #+END_SRC

    We can also define pi-sum in the same way:

    #+BEGIN_SRC scheme
    (define (pi-sum a b)
      (define (pi-term x)
        (/ 1.0 (* x (+ x 2))))
      (define (pi-next x)
        (+ x 4))
      (sum pi-term a pi-next b))
    #+END_SRC

    Using these procedures, we can compute an approximation to \pi:

    (* 8 (pi-sum 1 1000))
    3.139

    Once we have sum we can use it as a building block in formulating
    further concepts. For instance the definite integral of a function
    f between the limits a and b can be approximated numerically using
    the formula:

    \int_{a}^{b} f(x) dx = [f(a + dx/2) + f(a + dx + dx/2) + ...] dx
    
    for small values of dx. We can express this directly as a
    procedure:

    #+BEGIN_SRC scheme
    (define (integral f a b dx)
      (define (add-dx x) (+ x dx))
      (* (sum f (+ a (/ dx 2.0)) add-dx b)
         dx))
    (integral cube 0 1 0.01)
    .24998...
    (integral cube 0 1 0.001)
    .249999...
    #+END_SRC

    The actual value is 1/4.

    - Exercise 1.29. Simpson's Rule is a more accurate method of
      numerical integration than the method illustrated above. Using
      Simpson's Rule the integral of a function f between a and b is
      approximated as

      \frac{h}{3}[y_{0} + 4y_{1} + 2y_{2} + 4y_{3} + 2y_{4} + ... + 2y_{n-2} + 4y_{n-1} + y_{n}]
    
      where h = (b - a)/n for some integer n and y_{k} = f(a +
      kh). (Increasing n increases the accuracy of the approximation.)
      Define a procedure that takes as arguments f, a, b, and n and
      returns the value of the integral, computed using Simpson's
      Rule. Use your procedure to integrate cube between 0 and 1 (with
      n = 100 and n = 1000) and compare the results to those of the
      integral procedure shown above.

      #+BEGIN_SRC scheme
      (define (simpsons-rule f a b n)
        (define h (/ (- b a) n))
        (define (simpsons-term x)
          (define fval (f (+ a (* x h))))
          (* (/ h 3)
             (cond
              ((or (= x 0) (= x n))
               fval)
              ((even? x) (* 2 fval))
              ((odd? x) (* 4 fval)))))
        (define (next x) (+ x 1))
        (sum simpsons-term 0 next n))
      #+END_SRC

      This procedure is more exact, in part because it uses fractions
      and not inexact numbers.

    - Exercise 1.30. The sum procedure above generates a linear
      recursion. The procedure can be rewritten so that the sum is
      performed iteratively. Show how to do this by filling in the
      missing expressions in the following definition:

      #+BEGIN_SRC scheme
      (define (sum term a next b)
        (define (iter a result)
          (if <??>
              <??>
              (iter <??> <??>)))
        (iter <??> <??>))
      #+END_SRC

      #+BEGIN_SRC scheme
      (define (sum term a next b)
        (define (iter a result)
          (if (> a b)
              result
              (iter (next a) (+ result (term a)))))
        (iter a 0))
      #+END_SRC

    - Exercise 1.31. The sum procedure is only the simplest of a vast
      number of similar abstractions that can be captured as
      higher-order procedures. Write an analogous procedure called
      product that returns the product of the values of a function at
      points over a given range. Show how to define factorial in terms
      of product. Also use product to compute approximations to \pi
      using the formula:

      \frac{\pi}{4} = \frac{2 * 4 * 4 * 4 *6 * 6 * 7 ...}{3 * 3 * 5 * 5 * 7 * 7 ...}

      #+BEGIN_SRC scheme
      (define (product term a next b)
        (if (> a b)
            1
            (* (term a) (product term (next a) next b))))

      (define (pi-over-four-product n)
        (define (pi-term a)
          (define num 
           (cond ((= 0 a) 2)
                 ((even? a) (+ 2. (* 2 (/ a 2))))
                 ((odd? a) (+ 2. (* 2 (/ (+ a 1) 2))))))
          (define denom
           (cond ((even? a) (+ 3. (* 2 (/ a 2))))
                 ((odd? a) (+ 3. (* 2 (/ (- a 1) 2))))))
          (/ num denom))
        (define (pi-next x) (+ x 1))
        (product pi-term 0 pi-next n))

      #+END_SRC

      If your product procedure generates a recursive process, write
      one that generates an iterative process. If it generates an
      iterative process, write one that generates a recursive process.

      #+BEGIN_SRC scheme
      (define (product term a next b)
        (define (iter a accum)
          (if (> a b) 
              accum
              (iter (next a) (* accum (term a)))))
        (iter a 1))
      #+END_SRC

    - Exercise 1.32. Show that sum and product (exercise 1.31) are
      both special cases of a still more general notion called
      accumulate that combines a collection of terms, using some
      general accumulation function:

      #+BEGIN_SRC scheme
      (accumulate combiner null-value term a next b)
      #+END_SRC

      #+BEGIN_SRC scheme
      (define (sum term a next b)
        (if (> a b)
            0 
            (+ (term a) (sum term (next a) next b))))
      (define (product term a next b)
        (if (> a b)
            1
            (* (term a) (product term (next a) next b))))
      (define (accumulate combiner null-value term a next b)
        (if (> a b)
            null-value
            (combiner 
             (term a) 
             (accumulate combiner null-value term (next a) next b))))
      #+END_SRC

      #+BEGIN_SRC scheme
      (define (sum term a next b)
        (accumulate + 0 term a next b))
      (define (product term a next b)
        (accumulate * 1 term a next b))
      #+END_SRC

      If your accumulate procedure generates a recursive process write
      one that generates an iterative process. If it generates an
      iterative process write one that generates a recursive process.

      #+BEGIN_SRC scheme
      (define (accumulate combiner null-value term a next b)
        (define (iter a accum)
          (if (> a b)
              accum
              (iter (next a) (combiner (term a) accum))))
        (iter a null-value))
      #+END_SRC

    - Exercise 1.33. You can obtain an even more general version of
      accumulate (exercise 1.32.) by introducing the notion of a
      _filter_ on the terms to be combined. That is, combine only
      those terms derived from values in the range that satisfy a
      specified condition. The resulting filtered-accumulate
      abstraction takes the same arguments as accumulate together with
      an additional predicate of one argument that specifies the
      filter. Write filtered-accumulate as a procedure. Show how to
      express the following using filtered-accumulate:

      a. the sum of squares of the prime numbers in the interval a to
      b (assuming you have a prime? predicate already written)

      b. the product of all the positive integers less than n that are
      relatively prime to n (i.e. all positive integers i < n such
      that GCD(i, n) = 1).

      #+BEGIN_SRC scheme
      (define (filtered-accumulate combine null-value term a next b should-use?)
        (define (iter a accum)
          (if (> a b) 
              accum
              (iter (next a) 
                    (if (should-use? a)
                        (combine (term a) accum)
                        accum))))
        (iter a null-value))
      #+END_SRC

      #+BEGIN_SRC scheme
      (define (identity x) x)
      (define (addone x) (+ x 1))
      (define (sum-of-squares-of-primes a b)
        (filtered-accumulate
         +
         0
         square
         a
         addone
         b
         prime?))

      (define (product-of-relative-primes n)
       (filtered-accumulate
        *
        1
        identity
        a
        addone
        b
        relatively-prime?))
      #+END_SRC


    
      
*** 1.3.2. Constructing Procedures using Lambda
    In using sum as in section 1.3.1, it seems terribly awkward to
    have to define trivial procedures such as pi-term and pi-next just
    so we can use them as arguments to our higher-order
    procedure. Rather than define pi-next and pi-term, it would be
    more convenient to have a way to directly specify "the procedure
    that returns its input incremented by 4" and "the procedure that
    returns the reciprocal of its input times its input plus 2." We
    can do this by introducing the special form lambda, which creates
    procedures. Using lambda we can describe what we want as

    #+BEGIN_SRC scheme
    (lambda (x) (+ x 4))
    #+END_SRC

    and

    #+BEGIN_SRC scheme
    (lambda (x) (/ 1.0 (* x (+ x 2))))
    #+END_SRC

    Then our pi-sum procedure can be expressed without defining any
    auxiliary procedures as

    #+BEGIN_SRC scheme
    (define (pi-sum a b)
      (sum (lambda (x) (/ 1.0 (* x (+ x 2))))
           a
           (lambda (x) (+ x 4))
           b))
    #+END_SRC

    Again using lambda, we can write the integral procedure without
    having to define the auxiliary procedure add-dx:

    #+BEGIN_SRC scheme
    (define (integral f a b dx)
      (* (sum f
              (+ a (/ dx 2.0))
              (lambda (x) (+ x dx))
              b)
         dx))
    #+END_SRC
    
    In general, lambda is used to create procedures in the same way as
    define except that no name is specified for the procedure:

    #+BEGIN_SRC scheme
    (lambda (<forma-parameters>) <body>)
    #+END_SRC

    The resulting procedure is just as much a procedure as one that is
    created using define. The only difference is that it has not been
    associated with any name in the environment. In fact,

    #+BEGIN_SRC scheme
    (define (plus4 x) (+ x 4))
    #+END_SRC
    
    is equivalent to

    #+BEGIN_SRC scheme
    (define plus4 (lambda (x) (+ x 4)))
    #+END_SRC

    We can read a lambda expression as follows:

    #+BEGIN_SRC scheme
    (lambda (x) (+ x 4))
    #+END_SRC

    Like any expression that has a procedure as its value, a lambda
    expression can be used as the operator in a combination such as

    #+BEGIN_SRC scheme
    ((lambda (x y z) (+ x y (square z))) 1 2 3)
    => 12
    #+END_SRC

    or, more generally in any context where we would normally use a
    procedure name.

**** Using let to create local variables

     Another use of lambda is in creating local variables. We often
     need local variables in our procedures other than those that have
     been bound as formal parameters. For example, suppose we wish to
     compute the function

     \begin{equation}
     f(x, y) = x(1 + xy)^{2} + y(1 - y) + (1 + x y)(1 - y)
     \end{equation}
     
     which we could also express as

     \begin{another_equation}
     a = 1 + xy
     b = 1 - y
     f(x, y) = xa^{2} + yb + ab
     \end{another_equation}

     In writing a procedure to compute f, we would like to include as
     local variables not only x and y but also the names of
     intermediate quantities like _a_ and _b_. One way to accomplish
     this is to use an auxiliary procedure to bind the local
     variables:

     #+BEGIN_SRC scheme
     (define (f x y)
       (define (f-helper a b)
         (+ (* x (square a))
            (* y b)
            (* a b)))
       (f-helper (+ 1 (* x y))
                 (- 1 y)))
     #+END_SRC

     Of course, we could use a lambda expression to specify an
     anonymous procedure for binding our local variables. The body of
     f then becomes a single call to that procedure:

     #+BEGIN_SRC scheme
     (define (f x y)
       ((lambda (a b)
          (+ (* x (square a))
             (* y b)
             (* a b)))
        (+ 1 (* x y))
        (- 1 y)))
     #+END_SRC

     This construct is so useful that there is a special form called
     let to make its use more convenient. Using let, the f procedure
     could be written as

     #+BEGIN_SRC scheme
     (define (f x y)
       (let ((a (+ 1 (* x y)))
             (b (- 1 y)))
        (+ (* x (square a))
           (* y b)
           (* a b))))
     #+END_SRC

     which can be thought of as saying

     let <var1> have the value <exp1> and
         <var2> have the value <exp2> and
         ...
	 <varn> have the value <expn>
     in <body>

     The first part of the let expression is a list of name-expression
     pairs. When the let is evaluated each name is associated with the
     value of the corresponding expression. The body of the let is
     evaluated with these names bound as local variables. The way this
     happens is that the let expression is interpreted as an alternate
     syntax for

     #+BEGIN_SRC scheme
     ((lambda (<var1> ... <varn>)
         <body>)
      <exp1>
      ...
      <expn>)
     #+END_SRC

     No new mechanism is required in the interpreter in order to
     provide local variables. A let expression is simply syntactic
     sugar for the underlying lambda application.

     We can see from this equivalence that the scope of a variable
     specified by a let expression is the body of the let. This
     implies that:

     - Let allows one to bind variables as locally as possible to
       where they are to be used. For example, if the value of x is 5,
       the value of the expression
       
       #+BEGIN_SRC scheme
       (+ (let ((x 3))
           (+ x (* x 10)))
          x)
       #+END_SRC

       is 38. Here, the x in the body of the let is 3, so the value of
       the let expression is 33. On the other hand, the x that is the
       second argument to the outermost + is still 5.

     - The variables' values are computed outside the let. This
       matters when the expressions that provide the values for the
       local variables depend on the variables having the same names
       as the local variables themselves. For example, if the value of
       x is 2, the expression

       #+BEGIN_SRC scheme
       (let ((x 3)
             (y (+ x 2)))
        (* x y))
       #+END_SRC

       will have the value 12 because, inside the body of the let, x
       will be 3 and y will be 4 (which is the outer x plus 2).

       
     Sometimes we can use internal definitions to get the same effect
     as with let. For example, we could have defined the procedure f
     above as

     #+BEGIN_SRC scheme
     (define (f x y)
       (define a (+ 1 (* x y)))
       (define b (- 1 y))
       (+ (* x (square a))
          (* y b)
          (* a b)))
     #+END_SRC

     We prefer, however, to use let in situations like this and to use
     the internal define only for internal procedures.

     - Exercise 1.34. Suppose we define the procedure

       #+BEGIN_SRC scheme
       (define (f g)
         (g 2))
       #+END_SRC

       Then we have

       #+BEGIN_SRC scheme
       (f square)
       4
       (f (lambda (z) (* z (+ z 1))))
       6
       #+END_SRC

       What happens if we (perversely) ask the interpreter to evaluate
       the combination (f f)? Explain.

       #+BEGIN_SRC scheme
       (f f)
       (f 2)
       (2 2) -> error
       #+END_SRC

       You get an error, it calls f with 2 which is not a function and
       then tries to apply '2' to '2' and 2 is not a function.

*** 1.3.3. Procedures as General Methods

    We introduced compound procedures in section 1.1.4. as a mechanism
    for abstracting patterns of numerical operations so as to make
    them independent of the particular numbers involved. With
    higher-order procedures, such as the integeral procedure of
    section 1.3.1, we began to see a more powerful kind of
    abstraction: procedures used to express general methods of
    computation, independent of the particular functions involved. In
    this section we discuss two more elaborate examples -- general
    methods for finding zeros and fixed points of functions -- and
    show how these methods can be expressed directly as procedures.

**** Finding roots of equations by the half-interval method

     The _half-interval method_ is a simple but powerful technique for
     finding roots of an equation f(x) = 0, where f is a continuous
     function. The idea is that, if we are given points a and b such
     that f(a) < 0 < f(b), then f must have at least one zero between
     a and b. To locate a zero, let x be the average and of a and b
     and compute f(x). If f(x) > 0, then f must have a zero between a
     and x. If f(x) < 0, then f must have a zero between x and
     b. Continuing in this way, we can identify smaller and smaller
     intervals on which f must have a zero. When we reach a point
     where the interval is small enough, the process stops. Since the
     interval of uncertainty is reduced by half at each step of the
     process, the number of steps required grows as O(log(L/T)), where
     L is the length of the original interval and T is the error
     tolerance (that is, the size of the interval we will consider
     "small enough"). Here is a procedure that implements this
     strategy:
     
     #+BEGIN_SRC scheme
     (define (search f neg-point pos-point)
       (let ((midpoint (average neg-point pos-point)))
        (if (close-enough? neg-point pos-point)
            midpoint
            (let ((test-value (f midpoint)))
             (cond ((positive? test-value)
                    (search f neg-point midpoint))
                   ((negative? test-value)
                    (search f midpoint pos-point))
                   (else midpoint))))))
     #+END_SRC

     We assume that we are initially given the function f together
     with points at which its values are negative and positive. We
     first compute the midpoint of the two given points. Next we check
     to see if the given interval is small enough, and if so we simply
     return the midpoint as our answer. Otherwise, we compute as a
     test value the value of f at the midpoint. If the test value is
     positive, then we continue the process with a new interval
     running from the original negative point to the midpoint. If the
     test value is negative, we continue with the interval from the
     midpoint to the positive point. Finally, there is the possibility
     that the test value is 0, in which case the midpoint is itself
     the root we are searching for.

     To test whether the endpoints are "close enough" we can use a
     procedure similar to the one used in section 1.1.7. For computing
     square roots:
     
     #+BEGIN_SRC scheme
     (define (close-enough? x y)
       (< (abs (- x y)) 0.001))
     #+END_SRC

     Search is awkward to use directly, because we can accidentally
     give it points at which f's values do not have the required sign,
     in which case we get the wrong answer. Instead we will use search
     via the following procedure, which checks to see which of the
     endpoints has a negative function value and which has a positive
     value, and calls the search procedure accordingly. If the
     function has the same sign on the two given points, the
     half-interval method cannot be used, in which case the procedure
     signals an error.

     #+BEGIN_SRC scheme
     (define (half-interval-method f a b)
       (let ((a-value (f a))
             (b-value (f b)))
        (cond ((and (negative? a-value) (positive? b-value))
               (search f a b))
              ((and (negative? b-value) (positive? a-value))
               (search f b a))
              (else
               (error "Values are not of opposite sign" a b)))))
     #+END_SRC 

     The following example uses the half-interval method to
     approximate \pi as the root between 2 and 4 of sin x = 0:

     
     #+BEGIN_SRC scheme
     (half-interval-method sin 2.0 4.0)
     3.1411
     #+END_SRC
       
     Here is another example, using the half-interval method to search
     for a root of the equation x^{3} - 2x - 3 = 0 between 1 and 2:

     #+BEGIN_SRC scheme
     1 (user) => (half-interval-method (lambda (x) (- (* x x x) (* 2 x) 3)) 1.0 2.0)
     ;Value: 1.89306640625
     #+END_SRC

**** Finding fixed points of functions

     A number x is called a fixed point of a function f if x satisfies
     the equation f(x) = x. For some functions f we can locate a fixed
     point by beginning with an initial guess and applying f
     repeatedly,

     f(x), f(f(x()), f(f(f(x))),

     until the value does not change very much. Using this idea, we
     can devise a procedure fixed-point that takes as inputs a
     function and an initial guess and produces an approximation to a
     fixed point of the function. We apply the function repeatedly
     until we find two successive values whose difference is less than
     some prescribed tolerance:

     #+BEGIN_SRC scheme
     (define tolerance 0.00001)
     (define (fixed-point f first-guess)
       (define (close-enough? v1 v2)
         (< (abs (- v1 v2)) tolerance))
       (define (try guess)
         (let ((next (f guess)))
           (if (close-enough? next guess)
               next
               (try next))))
        (try first-guess))
     #+END_SRC

     For example, we can use this method to approximate the fixed
     point of a cosine function, starting with 1 as an initial
     approximation:

     #+BEGIN_SRC scheme
     1 (user) => (fixed-point cos 1.0)
     ;Value: .7390822985224024
     #+END_SRC

     Similarly, we can find a solution to the equation y= sin y + cos
     y:

     #+BEGIN_SRC scheme
     1 (user) => (fixed-point (lambda (y) (+ (sin y) (cos y))) 1.0)
     ;Value: 1.2587315962971173
     #+END_SRC

     The fixed-point process is reminiscent of the process we used for
     finding square roots in section 1.1.7. Both are based on the idea
     of repeatedly improving a guess until the result satisfies some
     criterion. In fact, we can readily formulate the square-root
     computation as a fixed-point search. Computing the square root of
     some number x requires finding a y such that y^2 = x. Putting
     this equation into the equivalent form y = x/y, we recognize that
     we are looking for a fixed point of the function y -> x/y, and we
     can therefore try to compute square roots as
     
     #+BEGIN_SRC scheme
     (define (sqrt x)
       (fixed-point (lambda (y) (/ x y)) 1.0))
     #+END_SRC

     Unfortunately, this fixed-point search does not
     converge. Consider an initial guess y_{1}. The next guess is
     y_{2} = x / y_{1} and the next guess is y_{3} = x/y_{2} =
     x/(x/y_{1}) = y_{1}. This results in an infinite loop in which
     the two guesses y_{1} and y_{2} repeat over and over, oscillating
     about the answer.

     One way to control such oscillations is to prevent the guesses
     from changing so much. Since the answer is always between our
     guess y and x/y, we can make a new guess that is not as far from
     y as x/y by averaging y with x/y, so that the next guess after y
     is (1/2)(y + x/y) instead of x/y. The process of making such a
     sequence of guesses is simply the process of looking for a fixed
     point of y -> (1/2)(y + x/y):

     #+BEGIN_SRC scheme
     (define (sqrt x)
       (fixed-point (lambda (y) (average y (/ x y)))
                    1.0))
     #+END_SRC

     (Note that y = (1/2)(y + x/y) is a simple transformation of the
     equation y = x/y; to derive it, add y to both sides of the
     equation and divide by 2.)

     With this modification, the square-root procedure works. In fact,
     if we unravel the definitions, we can see that the sequence of
     approximations to the square root generated here is precisely the
     same as the one generated by our original square-root procedure
     in section 1.1.7. This approach of averaging successive
     approximations to a solution, a technique that we call _average
     damping_, often aids the convergence of fixed-point searches.

     - Exercise 1.35. Show that the golden ratio \phi (section 1.2.2)
       is a fixed point of the transformation x -> 1 + 1/x, and use
       this fact to compute \phi by means of the fixed-point procedure.

       One definition of the golden ratio is that

       \begin{phi_equ}
       \phi = 1 + \frac{1}_{\phi}
       \end{phi_eq}

       So it is a fixed point of the transformation x -> 1/x.

       #+BEGIN_SRC scheme
       1 (user) => (fixed-point (lambda (x) (+ 1 (/ 1 x))) 1.0)
			 )
        guess: 1.
        guess: 2.
        guess: 1.5
        guess: 1.6666666666666665
        guess: 1.6
        guess: 1.625
        guess: 1.6153846153846154
        guess: 1.619047619047619
        guess: 1.6176470588235294
        guess: 1.6181818181818182
        guess: 1.6179775280898876
        guess: 1.6180555555555556
        guess: 1.6180257510729614
        guess: 1.6180371352785146
       guess: 1.6180327868852458
       uess: 1.618034447821682
       guess: 1.618033813400125
       guess: 1.6180340557275543
       ;Value: 1.6180339631667064
       #+END_SRC

     - Exercise 1.36 Modify the fixed-point so that it prints the
       sequence of approximations it generates, using the newline and
       display primitives shown in exercise 1.22. Then find a solution
       to x^{x} = 1000 by finding a fixed point of x ->
       log(1000)/log(x). (Use Scheme's primitive log procedure which
       computes natural logarithms.) Compare the number of steps this
       takes with and without average damping. (Note that you cannot
       start fixed-point with a guess of 1, as this would cause
       division by log(1) = 0). 
  
       #+BEGIN_SRC scheme
       (define tolerance 0.00001)
       (define (fixed-point f first-guess)
         (define (close-enough? v1 v2)
           (< (abs (- v1 v2)) tolerance))
         (define (try guess)
           (display "guess: ")
           (display guess)
           (newline)
           (let ((next (f guess)))
            (if (close-enough? guess next) 
                next
                (try next))))
         (try first-guess))
       #+END_SRC

       No damping:

       #+BEGIN_SRC scheme
        (fixed-point (lambda (x) (/ (log 1000) (log x))) 1.1)
        guess: 1.1
        guess: 72.47657378429035
        guess: 1.6127318474109593
        guess: 14.45350138636525
        guess: 2.5862669415385087
        guess: 7.269672273367045
        guess: 3.4822383620848467
        guess: 5.536500810236703
        guess: 4.036406406288111
        guess: 4.95053682041456
        guess: 4.318707390180805
        guess: 4.721778787145103
        guess: 4.450341068884912
        guess: 4.626821434106115
        guess: 4.509360945293209
        guess: 4.586349500915509
        guess: 4.535372639594589
        guess: 4.568901484845316
        guess: 4.546751100777536
        guess: 4.561341971741742
        guess: 4.551712230641226
        guess: 4.558059671677587
        guess: 4.55387226495538
        guess: 4.556633177654167
        guess: 4.554812144696459
        guess: 4.556012967736543
        guess: 4.555220997683307
        guess: 4.555743265552239
        guess: 4.555398830243649
        guess: 4.555625974816275
        guess: 4.555476175432173
        guess: 4.555574964557791
        guess: 4.555509814636753
        guess: 4.555552779647764
        guess: 4.555524444961165
        guess: 4.555543131130589
        guess: 4.555530807938518
        ;Value: 4.55553893484850
       #+END_SRC

       38 guesses.

       #+BEGIN_SRC scheme
       (fixed-point (lambda (x) (/ (+ x (/ (log 1000) (log x))) 2)) 1.1)
       guess: 1.1
       guess: 36.78828689214517
       guess: 19.352175531882512
       guess: 10.84183367957568
       guess: 6.870048352141772
       guess: 5.227224961967156
       guess: 4.701960195159289
       guess: 4.582196773201124
       guess: 4.560134229703681
       guess: 4.5563204194309606
       guess: 4.555669361784037
       guess: 4.555558462975639
       guess: 4.55553957996306
       ;Value: 4.555536364911781
       #+END_SRC

       13 guesses, far fewer.

     - Exercise 1.37. An infinite _continued fraction_ is an
       expression of the form 

       \begin{fract}
       f = \frac{N_1}{D_1 + \frac{N_2}{D_2 + \frac{N_3}{D_3 + ...}}}
       \end{fract}

       As an example, one can show that the infinite continued
       fraction expressed with the N_i and D_i all equal to 1 produces
       1/\phi, where \phi is the golden ratio (described in section
       1.2.2). One way to approximate an infinite continued fraction
       is to truncate the expansion after a given number of
       terms. Such a truncation -- a so-called k-term finite continued
       fraction -- has the form

       \begin{fract}
       \frac{N_1}{D_1 + \frac{N_2}{+ N_K/D_K}}
       \end{fract}

       Suppose that n and d are procedures of one argument (the term
       index i) that return N_i and D_i of the terms of the continued
       fraction. Define a procedure cont-frac such that evaluating
       (cond-frac n d k) computes the value of the k-term finite
       continued fraction. Check your procedure by approximating
       1/\phi using

       #+BEGIN_SRC scheme
       (cont-frac (lambda (i) 1.0)
                  (lambda (i) 1.0)
                  k)
       #+END_SRC

       for successive values of k. How large must you make k in order
       to get an approximation that is accurate to 4 decimal places?

       b. If your cont-frac procedure generates a recursive process,
       write one that generates an iterative process. If it generates
       an iterative process, write one that generates a recursive
       process.

       Recursive:
       #+BEGIN_SRC scheme
       (define (cont-frac n-gen d-gen k)
         (if (= k 0) 
             (/ (n-gen 0) (d-gen 0))
             (/ (n-gen k) (+ (d-gen k) (cont-frac n-gen d-gen (- k 1))))))
       #+END_SRC

       Iterative:
       #+BEGIN_SRC scheme
       (define (cont-frac n-gen d-gen k)
         (define (cont-frac-iter k-iter acc)
           (if (= k-iter 0) 
               acc
               (cont-frac-iter 
                (- k-iter 1)
                (/ (n-gen k-iter) 
                   (+ (d-gen k-iter) acc)))))
          (cont-frac-iter k 0))
       #+END_SRC

     - Exercise 1.38 In 1737, the Swiss mathematician Leonhard Euler
       published a memoir _De Fractionibus Cantinuis_, which included
       a continued fraction expansion for e - 2, where e is the base
       of the natural logarithms. In this fraction, the N_i, are all
       1, and the D_i are successively 1, 2, 1, 1, 4, 1, 1, 6, 1, 1,
       8, .... Write a program that uses your cont-frac procedure from
       exercise 1.37 to approximate e, based on Euler's expansion.

       #+BEGIN_SRC scheme
       (+ 2 (cont-frac 
        (lambda (i) 1.0)
        (lambda (i) (cond
                     ((= i 1) 1)
                     ((= (remainder (- i 1) 3) 1) 
                      (+ 2 (* 2 (floor (/ i 3)))))
                     (else 1))) 100))
       #+END_SRC

     - Exercise 1.39. A continued fraction representation of the
       tangent function was published in 1770 by the German
       mathematician J.H. Lambert:

       \begin{tan}
       tan x = \frac{x}{1 - \frac{x^2}{3 - \frac{x^2}{5 - ...}}}
       \end{tan}

       where _x_ is in radians. Define a procedure (tan-cf x k) that
       computes an approximation to the tangent function based on
       Lambert's formula. K specifies the number of terms to compute,
       as in exercise 1.37.

       #+BEGIN_SRC scheme
       (define (tan-cf x k)
        (cont-frac (lambda (i) (if (= i 1) x (* -1 (square x))))
                  (lambda (i) (- (* 2 i) 1)) k))
       #+END_SRC

*** 1.3.4 Procedures as Returned Values

    The above examples demonstrate how the ability to pass procedures
    as arguments significantly enhances the expressive power of our
    programming language. We can achieve even more expressive power by
    creating procedures whose returned values are themselves
    procedures.

    We can illustrate this idea by looking again at the fixed-point
    example described at the end of section 1.3.3. We formulated a new
    version of the square-root procedure as a fixed-point search,
    starting with the observation that \sqrt{x} is a fixed-point of
    the function y => x/y. Then we used average damping to make the
    approximations converge. Average damping is a useful general
    technique in itself. Namely, given a function _f_, we consider the
    function whose value at x is equal to the average of x and f(x).

    We can express the idea of average damping by means of the
    following procedure:

    #+BEGIN_SRC scheme
    (define (average-damp f)
      (lambda (x) (average x (f x))))
    #+END_SRC

    Average-damp is a procedure that takes as its argument a procedure
    f and returns as its value a procedure (produced by the lambda)
    that, when applied to a number x, produces the average of x and (f
    x). For example, applying average-damp to the square procedure
    produces a procedure whose value at some number x is the average
    of x and x^2. Applying this resulting procedure to 10 returns the
    average of 10 and 100, or 55:

    #+BEGIN_SRC scheme
    ((average-damp square) 10)
    55
    #+END_SRC

    Using average-damp, we can reformulate the square-root procedure
    as follows:

    #+BEGIN_SRC scheme
    (define (sqrt x)
     (fixed-point (average-damp (lambda (y) (/ x y))) 1.0))
    #+END_SRC

    Notice how this formulation makes explicit three ideas in the
    method: fixed-point search, average damping, and the function y =>
    x/y. It is instructive to compare this formulation of the
    square-root method with the original version given in section
    1.1.7. Bear in mind that these procedures express the same
    process, and notice how much clearer the idea becomes when we
    express the process in terms of these abstractions. In general,
    there are many ways to formulate a process as a
    procedure. Experienced programmers know how to choose procedural
    formulations that are particularly perspicuous and where useful
    elements of the process are exposed as separate entities that can
    be reused in other applications. As a simple example of reuse,
    notice that the cube root of x is a fixed point of the function y
    -> x/y^2, so we can immediately generalize our square-root
    procedure to one that extracts cube roots:

    #+BEGIN_SRC scheme
    (define (cube-root x)
     (fixed-point (average-damp (lambda (y) (/ x (square y))))
                  1.0))
    #+END_SRC

**** Newton's method
     When we first introduced the square-root procedure, in section
     1.1.7, we mentioned that this was a special case of _Newton's
     method_. If x |=> g(x) is a differentiable function, then a
     solution of the equation g(x) = 0 is a fixed point of the
     function x |=> f(x) where

     \begin{newton}
     f(x) = x - \frac{g(x)}{Dg(x)}
     \end{newton}

     and Dg(x) is the derivative of g evaluated at x. Newton's method
     is the use of the fixed-point method we saw above to approximate
     a solution of the equation by finding a fixed point of the
     function f. For many functions g and for sufficiently good
     initial guesses for x, Newton's method converges very rapidly to
     a solution of g(x) = 0.

     In order to implement Newton's method as a procedure, we must
     first express the idea of a derivative. Note that "derivative,"
     like average damping, is something that transforms a function
     into another function. For instance, the derivative of the
     function x |=> x^3 is the function x |=> 3x^2. In general if g is
     a function and dx is a small number then the derivative Dg of g
     is the function whose value at any number x is given (in the
     limit of small dx) by

     \begin{deriv}
     Dg(x) = \frac{g(x + dx) - g(x)}{dx}
     \end{deriv}
     
     Thus, we can express the idea of a derivative (taking dx to be,
     say, 0.000001) as the procedure

     #+BEGIN_SRC scheme
     (define (deriv g)
      (lambda (x)
       (/ (- (g (+ x dx)) (g x))
          dx)))
     #+END_SRC
     
     along with the definition

     #+BEGIN_SRC scheme
     (define dx 0.000001)
     #+END_SRC

     Like average-damp, deriv is a procedure that takes a procedure as
     argument and returns a procedure as value. For example, to
     approximate the derivative of x |=> x^3 at 5 (whose exact value
     is 75) we can evaluate

     #+BEGIN_SRC scheme
     (define (cube x) (* x x x))
     ((deriv cube) 5)
     75.00014999
     #+END_SRC

     With the aid of deriv, we can express Newton's method as a
     fixed-point process:

     #+BEGIN_SRC scheme
     (define (newton-transform g)
      (lambda (x)
       (- x (/ (g x) ((deriv g) x)))))
     (define (newtons-method g guess)
      (fixed-point (newton-transform g) guess))
     #+END_SRC
     
     The newton-transform procedure expresses the formula at the
     beginning of this section and newtons-method is readily defined
     in terms of this. It takes as arguments a procedure that computes
     the function for which we want to find a zero, together with an
     initial guess. For instance, to find the square root of x, we can
     use Newton's method to find a zero of the function y |=> y^2 - x
     starting with an initial guess of 1. This provides yet another
     form of the square-root procedure:

     #+BEGIN_SRC scheme
     (define (sqrt x)
      (newtons-method (lambda (y) (- (square y) x)) 1.0))
     #+END_SRC

**** Abstractions and first-class procedures

     We've seen two ways to express the square-root computation as an
     instance of a more general method, once as a fixed-point search
     and once using Newton's method. Since Newton's method was itself
     expressed as a fixed-point process, we actually saw two ways to
     compute square roots as fixed points. Each method begins with a
     function and finds a fixed point of some transformation of the
     function. We can express this general idea itself as a procedure:
     
     #+BEGIN_SRC scheme
     (define (fixed-point-of-transform g transform guess)
      (fixed-point (transform g) guess))
     #+END_SRC

     This very general procedure takes as its arguments a procedure g
     that computes some function, a procedure that transforms g, and
     an initial guess. The returned result is a fixed point of the
     transformed function.

     Using this abstraction, we can recast the first square-root
     computation from this section (where we look for a fixed point of
     the average-damped version of y => x/y) as an instance of this
     general method:

     #+BEGIN_SRC scheme
     (define (sqrt x)
      (fixed-point-of-transform (lambda (y) (/ x y))
                                average-damp
                                1.0))
     #+END_SRC
     
     Similarly we can express the second square-root computation from
     this section (an instance of Newton's method that finds a fixed
     point of the Newton transform of y => y^2 - x) as

     #+BEGIN_SRC scheme
     (define (sqrt x)
      (fixed-point-of-transform (lambda (y) (- (square y) x))
                                newton-transform
                                1.0))
     #+END_SRC

     We began section 1.3 with the observation that compound
     procedures are a crucial abstraction mechanism because they
     permit us to express general methods of computing as explicit
     elements in our programming language. Now we've seen how
     high-order procedures permit us to manipulate these general
     methods to create further abstractions.

     As programmers, we should be alert to opportunities to identify
     the underlying abstractions in our programs and to build upon
     them and generalize them to create more powerful
     abstractions. This is not to say that one should always write
     programs in the most abstract way possible; expert programmers
     know how to choose the level of abstraction appropriate to their
     task. But it is important to be able to think in terms of these
     abstractions so that we can be ready to apply them in new
     contexts. The significance of higher-order procedures is that
     they enable us to represent these abstractions explicitly as
     elements in our programming language, so that they can be handled
     just like other computational elements.

     In general, programming languages impose restrictions on the ways
     in which computational elements can be manipulated. Elements with
     the fewest restrictions are said to have _first-class_
     status. Some of the "rights and privileges" of first-class
     elements are:

     - They may be named by variables
     - They may be passed as arguments to procedures
     - They may be returned as the results of procedures
     - They may be included in data structures.

     Lisp, unlike other common programming languages, awards full
     first-class status. This poses challenges for efficient
     implementation, but the resulting gain in expressive power is
     enormous.

     - Exercise 1.40. Define a procedure cubic that can be used
       together with the newtons-method procedure in expressions of
       the form

       #+BEGIN_SRC scheme
       (newtons-method (cubic a b c) 1)
       #+END_SRC

       to approximate zeros of the cubic x^3 + ax^2 + bx + c.

       #+BEGIN_SRC scheme
       (define (cubic a b c)
        (lambda (x) (+ (* x x x) (* a x x) (* b x) c)))
       #+END_SRC

     - Exercise 1.41. Define a procedure double that takes a procedure
       of one argument as argument and returns a procedure that
       applies the original procedure twice. For example, if inc is a
       procedure that adds 1 to its argument, then (double inc) should
       be a procedure that adds 2. What value is returned by

       #+BEGIN_SRC scheme
       (((double (double double)) inc) 5)
       #+END_SRC

       #+BEGIN_SRC scheme
       (define (double f) (lambda (x) (f (f x))))
       #+END_SRC
       
       The inner function is inc, which is passed to the return value
       of (double (double double)). 

       What happens is you apply a function that quadruples the number
       of function calls to itself which results in f getting called
       sixteen times.

     - Exercise 1.42. Let f and g be two one-argument functions. The
       _composition f_ after _g_ is defined to be the function x ->
       f(g(x)). Define a procedure compose that implements
       composition. For example, if inc is a procedure that adds 1 to
       its argument

       #+BEGIN_SRC scheme
       ((compose square inc) 6)
       49
       #+END_SRC

       #+BEGIN_SRC scheme
       (define (compose f g)
        (lambda (x) (f (g x))))
       #+END_SRC

     - Exercise 1.43. If _f_ is a numerical function and _n_ is a
       positive integer, then we can form the _n_th repeated
       application of f, which is defined to be the function whose
       value at x is f(f(...(f(x))...)). For example, if f is the
       function x -> x + 1, then the nth repeated application of f is
       the function x -> x + n. If f is the operation of squaring a
       number, then the nth repeated application of f is the function
       that raises its argument to the 2^nth power. Write a procedure
       that takes as inputs a procedure that computes f and a positive
       integer n and returns the procedure that computes the nth
       repeated application of f. Your procedure should be able to be
       used as follows:

       #+BEGIN_SRC scheme
       ((repeated square 2) 5)
       625
       #+END_SRC

       #+BEGIN_SRC scheme
       (define (repeated f n)
        (define (repeated-iter f n x)
         (cond
          ((= n 1) (f x))
          (else (f (repeated-iter f (- n 1) x)))))
        (lambda (x)
         (repeated-iter f n x)))
    
       (define (repeated2 f n)
        (cond
         ((= n 1) f)
         (else (compose f (repeated2 f (- n 1))))))
       #+END_SRC

     - Exercise 1.44. The idea of a smoothing function is an important
       concept in signal processing. If f is a function and dx is some
       small number, then the smoothed version of f is the function
       whose value at a point x is the average of f(x - dx), f(x) and
       f(x + dx). Write a procedure smooth that takes as input a
       procedure that computes f and returns a procedure that computes
       the smoothed f. It i sometimes valuable to repeatedly smooth a
       function (that is, smooth the smoothed function and so on) to
       obtain the n-fold smoothed function. Show how to generate an
       n-fold smoothed function of any given function using smooth and
       repeated from exercise 1.43.
       
       #+BEGIN_SRC scheme
       (define (average3 a b c) (/ (+ a b c) 3))
       (define (smooth f)
        (lambda (x)
         (average (f (- x dx)) (f x) (f (+ x dx)))))
       #+END_SRC

       #+BEGIN_SRC scheme
       (define (cube x) (* x x x))
       (define repeated-average-of-cube (repeated (smooth cube) 5))
       #+END_SRC

     - Exercise 1.45 We saw in section 1.3.3 that attempting to
       compute square roots by naively finding a fixed point of y ->
       x/y does not converge, and that this can be fixed by average
       damping. THe same method works for finding cube roots as fixed
       points of the average-damped y -> x/y^2. Unfortunately, the
       process does not work for fourth roots -- a single average damp
       is not enough to make a fixed-point search for y -> x/y^3
       converge. On the other hand, if we average damp twice (i.e. use
       the average damp of the average damp of y -> x/^3) the
       fixed-point search does converge. Do some experiments to
       determine how many average damps are required to compute nth
       rotos as a fixed-point search based on the repeated average
       damping of y-> x/y^n-1. Use this to implement a simple
       procedure for computing nth roots using fixed-point,
       average-damp, and the repeated procedure of exercise
       1.43. Assume that any arithmetic operations you need are
       available as primitives.

       4 needs 2
       8 needs 3
       16 needs 4
       32 needs 5

       so log2 of n.

       #+BEGIN_SRC scheme
       (define (fixed-point f guess)
        (define tolerance 1e-10)
        (define (close-enough? a b)
         (< (abs (- a b)) tolerance))
        (define (fixed-point-iter guess)
         (let ((next (f guess)))
          (if (close-enough? guess next)
              guess
              (fixed-point-iter next))))
        (fixed-point-iter guess))

       (define (compose f g)
        (lambda (x) (f (g x))))
       
       (define (repeated f n)
        (if (= n 1)
            f
            (compose f (repeated f (- n 1)))))

       (define (average x y) (/ (+ x y) 2))
      
       (define (average-damp f)
        (lambda (x) (average x (f x))))

       (define (nth-root x n)
        (fixed-point
         ((repeated 
           average-damp
           (floor (/ (log n) (log 2))))
          (lambda (y) (/ x (expt y (- n 1)))))
         1.0))
       #+END_SRC

     - Exercise 1.46. Several of the numerical methods described in
       this chapter are instances of an extremely general
       computational strategy known as _iterative
       improvement_. Iterative improvement says that, to compute
       something, we start with an initial guess for the answer, test
       if the guess is good enough, and otherwise improve the guess
       and continue the process using the improved guess as the new
       guess. Write a procedure iterative-improve that takes two
       procedures as arguments: a method for telling whether a guess
       is good enough and a method for improving a
       guess. Iterative-improve should return as its value a procedure
       that takes a guess as argument and keeps improving the guess
       until it is good enough. Rewrite the sqrt procedure of section
       1.1.7 and the fixed-point procedure of section 1.3.3 in terms
       of iterative-improve.

       #+BEGIN_SRC scheme
       (define (iterative-improve good-enough? improve-guess)
        (define (iterator guess)
         (cond
          ((good-enough? guess) guess)
          (else (iterator (improve-guess guess)))))
        iterator)
       #+END_SRC

       #+BEGIN_SRC scheme
       (define (fixed-point f guess)
        (define tolerance 1e-10)
        ((iterative-improve 
          (lambda (guess) (< (abs (- guess (f guess))) tolerance))
          (lambda (guess) (f guess))) guess))
       #+END_SRC

       #+BEGIN_SRC scheme
       (define (sqrt x)
        (define tolerance 1e-10)
        ((iterative-improve
          (lambda (guess) (< (abs (- (* guess guess) x)) tolerance))
          (lambda (guess) (average guess (/ x guess))))
         1.0))
       #+END_SRC

       
